---
title: "ADM Standalone Model Report"
title-block-banner: true
author: "Pega data scientist tools"
date: today
subtitle: > 
  Details of one ADM model instance
execute:
  echo: false
format:
  html:
    code-fold: true
    embed-resources: true
    standalone: true
    code-tools: true
    toc: true
    toc-title: Table of Contents
    theme:
        light: flatly
    fontsize: small
jupyter: python3
---

```{python}
# | label: Imports
# | code-fold: true
# | code-summary: Python imports
# | output: false

import sys
import os.path
from pathlib import Path
import re
import subprocess
import datetime

from pdstools import datasets, ADMDatamart
from pdstools.utils.cdh_utils import zRatio, lift
import polars as pl
from IPython.display import display, Markdown
from tabulate import tabulate
from itables import show
import plotly.express as px

# Convenience wrapper functions


def quarto_print(text):
    display(Markdown(text))


def quarto_callout_info(info):
    quarto_print(
        """
::: {.callout-note}
%s
:::
"""
        % info
    )


def quarto_callout_important(info):
    quarto_print(
        """
::: {.callout-important}
%s
:::
"""
        % info
    )


```

```{python}
# | tags: [parameters]

# These parameters are overwritten when called externally

datafolder = ""
modelfilename = ""
predictorfilename = ""
modelid = ""

predictordetails_activeonly = True

title = "Demo Dataset"  # pass in customer name here
subtitle = ""  # typically used to pass in a date range or other qualification of the data source

kwargs = dict()


```

```{python}
# Predictor data for one model ID
if len(kwargs) > 0:
    # streamlit call
    modelid = kwargs["modelid"]
    datamart = ADMDatamart(
        **kwargs, include_cols="pyFeatureImportance", query=pl.col("ModelID") == modelid
    ).fillMissing()
    predictordetails_activeonly = kwargs["predictordetails_activeonly"]
elif len(datafolder) > 0 or len(modelfilename) > 0 or len(predictorfilename) > 0:
    # command line call
    datamart = ADMDatamart(
        datafolder,
        model_filename=modelfilename,
        predictor_filename=predictorfilename,
        extract_keys=True,
        include_cols="pyFeatureImportance",
        query=pl.col("ModelID") == modelid,
    ).fillMissing()
else:
    # fall back to sample data
    modelid = "bd70a915-697a-5d43-ab2c-53b0557c85a0"
    datamart = datasets.CDHSample(query=pl.col("ModelID") == modelid)

# TODO ensure this is only one predictor snapshot, just in case? although it would be
# neat to show predictor evolution, if available


def columnExists(df, col):
    return col in df.columns and df.schema[col] != pl.Null


# Simple way to make sure Treatment is set and not null
if columnExists(datamart.modelData, "Treatment"):
    datamart.modelData = datamart.modelData.with_columns(
        pl.col("Treatment").fill_null("-")
    )
else:
    datamart.modelData = datamart.modelData.with_columns(pl.lit("-").alias("Treatment"))

```

```{python}
channel_name = (
    datamart.last(strategy="lazy")
    .select(pl.format("{}/{}", "Direction", "Channel"))
    .unique()
    .collect()
    .item(0, 0)
)
model_name_in_context = (
    datamart.last(strategy="lazy")
    .select(pl.format("{}/{}/{}/{}", "Issue", "Group", "Name", "Treatment"))
    .unique()
    .collect()
    .item(0, 0)
)
model_name = (
    datamart.last(strategy="lazy")
    .select(pl.format("{}/{}", "Name", "Treatment"))
    .unique()
    .collect()
    .item(0, 0)
)

quarto_print(
    f"""
# {title}

{subtitle}

## {model_name_in_context}

| Channel | ID |
|:-----|------:|
| {channel_name} | {modelid} |

"""
)
```

```{python}
fig = datamart.plotScoreDistribution(modelids=[modelid])

# Customize some of the default styling of the plot

fig.update_layout(
    title=f"Classifier Score Distribution<br>{model_name}", xaxis_title=""
)
fig.data[0].opacity = 0.5
fig.data[1].line.color = "#EF8B08"
fig.data[1].line.width = 3
fig.data[1].marker.color = "black"

fig.show()
```

::: {.callout-tip}
The charts (built with [Plotly](https://plotly.com/python/)) have [user controls for panning,
zooming etc](https://plotly.com/chart-studio-help/zoom-pan-hover-controls/). These interactive plots do not render well in portals like Sharepoint
or Box. It is preferable to view them from a browser.
:::

## Model Performance

```{python}
classifier = (
    datamart.last("predictorData", strategy="lazy")
    .filter(pl.col("EntryType") == "Classifier")
    .sort("BinIndex")
)

auc_roc = round(classifier.select(pl.last("Performance")).collect().item(), 4)

quarto_print(
    f"""
The model performance is **{auc_roc}** measured as AUC-ROC. This number is calculated from the “active” bins of the Classifier.
"""
)

```

The classifier maps the model scores (average of the log odds of the active predictors) to a propensity value. The “active” bins are the ones that can be reached from the current binning of the active predictors.

See the [ADM Explained](https://pegasystems.github.io/pega-datascientist-tools/Python/articles/ADMExplained.html) article for more information on how ADM exactly works.

```{python}
# | output: asis

classifier_properties = classifier.select(
    pl.last("ResponseCount").alias("Responses"),
    pl.last("Positives"),
    pl.format(
        "{}%", (pl.last("Positives") * 100 / pl.last("ResponseCount")).round(3)
    ).alias("Base Propensity"),
    pl.format("{}", pl.last("Performance").round(4)).alias("Performance"),
)

show(classifier_properties.collect().to_pandas())
```


## Score Distribution

The Score Distribution shows the volume and average propensity in every bin of
the score ranges of the Classifier.

Propensity is defined as $\frac{positives}{positives+negatives}$ per bin.
The adjusted propensity that is returned is a small modification (*Laplace
smoothing*) to this and calculated as
$\frac{0.5+positives}{1+positives+negatives}$ so new models initially return a
propensity of 0.5. This helps to address the cold start when introducing new
actions.

::: {.callout-warning}
The Python version has no notion of unreachable bins (yet), when it has we should grey out the binning rows that fall outside of the active range
:::

```{python}
# | tbl-cap: Score Distribution

human_friendly_scoredistribution = (
    classifier.select(
        pl.col("BinIndex").alias("Index"),
        pl.col("BinSymbol").alias("Bin"),
        pl.col("BinResponseCount").alias("Responses"),
        pl.col("BinPositives").alias("Positives"),
        (100 * (pl.col("BinPositives").cumsum(reverse=True)) / pl.sum("BinPositives"))
        .round(2)
        .alias("Cum. Positives (%)"),
        (
            100
            * (pl.col("BinResponseCount").cumsum(reverse=True))
            / pl.sum("BinResponseCount")
        )
        .round(2)
        .alias("Cum. Total (%)"),
        (100 * pl.col("BinPropensity")).round(3).alias("Propensity (%)"),
        zRatio(pl.col("BinPositives"), pl.col("BinNegatives"))
        .round(3)
        .alias("Z Ratio"),
        (lift(pl.col("BinPositives"), pl.col("BinNegatives")) * 100)
        .round(2)
        .alias("Lift (%)"),
        (pl.col("BinAdjustedPropensity") * 100)
        .round(3)
        .alias("Adjusted Propensity (%)"),
    )
).collect()

# Convert to a list of lists, use tabulate to print to ascii art
# the markdown to make it a nice table
# Seems to look better than itables.show() but maybe I'm missing something
# Neither of the two are as good as kable / kableExtra, highlighting of specific
# cells, rows, columns is not easily done

Markdown(
    tabulate(
        [
            [f[i] for f in human_friendly_scoredistribution]
            for i in range(human_friendly_scoredistribution.shape[0])
        ],
        headers=human_friendly_scoredistribution.columns,
    )
)

```


## Cumulative Gains and Lift charts

Below are alternative ways to view the Classifier.

The Cumulative Gains chart shows the percentage of he overall cases in the "positive" category gained by targeting a percentage of the total number of cases. For example, this view shows how large a percentage of the total expected responders you target by targeting only the top decile.

The Lift chart is derived from this and shows the ratio of the cumulative gain and the targeted volume.

TODO: unfortunately the Python version has no notion of unreachable bins (yet) which can cause really strange high lift values

::: {layout-ncol=2}

```{python}
# TODO perhaps this should move into the pdstools plot functions "plotCumulativeGains"
# however it is so trivial, not really sure it should be. See also the other gains charts
# in the health check.
# TODO in HC there now is a beter way to plot gains, use that when it has progressed into PDS tools
fig = px.area(
    human_friendly_scoredistribution,
    x="Cum. Total (%)",
    y="Cum. Positives (%)",
    title="Cumulative Gains",
    template="pega",
)
fig.add_shape(type="line", line=dict(dash="dash"), x0=0, x1=100, y0=0, y1=100)
fig.update_yaxes(
    scaleanchor="x", scaleratio=1, constrain="domain", title="% of Positive Responders"
)
fig.update_xaxes(constrain="domain", title="% of Population")
fig.show()
```

```{python}
# TODO perhaps this should move into the pdstools plot functions "plotCumulativeLift"
fig = px.area(
    human_friendly_scoredistribution,
    x="Cum. Total (%)",
    y="Lift (%)",
    title="Lift",
    template="pega",
)
fig.update_yaxes(
    scaleanchor="x", scaleratio=0.01, constrain="domain", title="Propensity Lift"
)
fig.update_xaxes(constrain="domain", title="% of Population")
fig.show()
```

:::

# Trend charts

TODO see if we can put the various trend charts in tabs - although this seems to require go not express, then gain .data[0] probably gives the go objects

But maybe .data[0] gives the trace


```{python}
fig = datamart.plotOverTime("weighted_performance")

fig.update_layout(
    title="Model Performance Trend", yaxis_title="ROC-AUC", xaxis_title=""
)
fig.update_layout(showlegend=False)

fig.show()
```

```{python}
fig = datamart.plotOverTime("SuccessRate")

fig.update_layout(
    title="Success Rate Trend", yaxis_title="Success Rate", xaxis_title=""
)
fig.update_layout(showlegend=False)
fig.update_yaxes(rangemode="tozero")

fig.show()
```


# Performance by Predictor Category

Showing the performance across all predictors. The predictor categories default
to the text before the first dot. This can be customized when reading the data
for a particular customer.

```{python}
# | error: true

# TODO facets are meaningless but plotPredictorCategoryPerformance fails when
# not giving any

fig = datamart.plotPredictorCategoryPerformance(facets="Channel")

fig.update_layout(
    title="Predictor Performance per Category",
    yaxis_title="",
    showlegend=False,
    height=300,
    width=500,
)

fig.show()
```

# Predictor Overview

The predictors for this model are sorted by performance and grouped if they are
correlated (shown with an indentation and a lighter color).

The negatives and positives counts are usually the same across all the
predictors but will be different when predictors have been removed or added. IH
predictors often have slightly lower counts.

For Adaptive Gradient Boosting models ("AGB") the number of positives and
negatives is not available.

```{python}
# TODO predictor groups missing https://github.com/pegasystems/pega-datascientist-tools/issues/127
# once this is fixed we can include the group index and group
# the overview by correlation group, and list the correlated preds in the
# summaries

predictors_summary_info = (
    datamart.last("predictorData", strategy="lazy")
    .group_by("PredictorName")
    .agg(
        pl.last("EntryType").alias("Status"),
        pl.last("Type"),
        pl.last("Performance").alias("Univariate Performance"),
        pl.max("BinIndex").alias("Bins"),
        (
            pl.col("BinResponseCount").where(pl.col("BinType") == "MISSING").sum()
            * 100
            / pl.sum("BinResponseCount")
        ).alias("Missing %"),
        (
            pl.col("BinResponseCount").where(pl.col("BinType") == "RESIDUAL").sum()
            * 100
            / pl.sum("BinResponseCount")
        ).alias("Residual %"),
        pl.last("Positives"),
        pl.last("ResponseCount").alias("Responses"),
    )
    .rename({"PredictorName": "Predictor"})
    .sort("Univariate Performance", descending=True)
)


def shadeInactive(s):
    if s.Status == "Inactive":
        return ["color: grey"] * len(s)
    else:
        return [""] * len(s)


display(
    predictors_summary_info.filter(pl.col("Status") != "Classifier")
    .collect()
    .to_pandas()
    .style.set_caption("Predictor Overview")
    .hide(axis="index")
    .hide(["Positives", "Responses"], axis="columns")
    .format(precision=4, subset=["Univariate Performance"])
    .format(precision=2, subset=["Missing %", "Residual %"])
    .format(precision=0, subset=["Positives", "Responses"])
    .apply(shadeInactive, axis=1)
)
```

# Binning of the Predictors

The predictors are listed in the same order as in the summary above.

```{python}
quarto_print(
    f"""
Here we show **{'only the active' if predictordetails_activeonly else 'all'}**
predictors. This can be configured via a parameter to this report.
"""
)
```

```{python}
# | output: asis


def show_single_predictor(pred):
    quarto_print(f"## {pred}")

    predictor_binning_data = (
        datamart.last("predictorData", strategy="lazy")
        .filter(pl.col("PredictorName") == pred)
        .sort("BinIndex")
    )

    predictor_properties = predictors_summary_info.filter(pl.col("Predictor") == pred)

    base_propensity = (
        predictor_properties.select(pl.col("Positives") / pl.col("Responses"))
        .collect()
        .item()
    )

    # Predictor summary

    display(
        predictor_properties.collect()
        .to_pandas()
        .style.hide(axis="index")
        .format(precision=4, subset=["Univariate Performance"])
        .format(precision=2, subset=["Missing %", "Residual %"])
        .format(precision=0, subset=["Positives", "Responses"])
    )

    # Predictor binning

    fig = datamart.plotPredictorBinning(modelids=[modelid], predictors=[pred])

    # Customize some of the default styling of the plot

    fig.update_layout(title="Predictor Binning", xaxis_title="")
    # fig.layout.yaxis2.tickformat = ",.3%"
    fig.data[0].opacity = 0.5
    fig.data[1].line.color = "#EF8B08"
    fig.data[1].line.width = 3
    fig.data[1].marker.color = "black"

    # Add line for base propensity, TODO consider putting back in the library
    fig.add_hline(y=base_propensity, line_dash="dash", line_color="grey", yref="y2")
    # fig.update_xaxes(type='category') # prevent plotly from trying to guess

    fig.update_layout(width=700, height=300)
    fig.show()

    # "Philip Mann" plot with simple red/green lift bars relative to base propensity
    # TODO move below "Philip Mann" plot into a plot function (plotBinningLift)

    pm_plot_binning_table = (
        predictor_binning_data.select(
            pl.col(["BinSymbol", "BinPositives", "BinIndex"]),
            (lift(pl.col("BinPositives"), pl.col("BinNegatives")) - 1.0),
        )
        .with_columns(
            pl.when((pl.col("Lift") >= 0.0) & (pl.col("BinPositives") > 5))
            .then(pl.lit("pos"))
            .when((pl.col("Lift") >= 0.0) & (pl.col("BinPositives") <= 5))
            .then(pl.lit("pos_shaded"))
            .when((pl.col("Lift") < 0.0) & (pl.col("BinPositives") > 5))
            .then(pl.lit("neg"))
            .otherwise(pl.lit("neg_shaded"))
            .alias("Direction")
        )
        .sort("BinIndex")
        .collect()
    )

    fig = px.bar(
        x=pm_plot_binning_table["Lift"],
        y=pm_plot_binning_table["BinSymbol"],
        color=pm_plot_binning_table["Direction"],
        color_discrete_map={
            "neg": "#A01503",
            "pos": "#5F9F37",
            "neg_shaded": "#DAA9AB",
            "pos_shaded": "#C5D9B7",
        },
        orientation="h",
        template="pega",
    )
    fig.update_yaxes(type="category")
    fig.layout.xaxis.tickformat = ",.2%"
    fig.add_vline(x=0, line_color="black")
    fig.update_layout(
        yaxis=dict(autorange="reversed"),
        title="Propensity Lift",
        xaxis_title="",
        yaxis_title="",
    )
    # Not convinced why we need below. It should respect the binindex order but w/o this it doesnt always.
    # Stijn: I think, if there's no missing bin, it tries to cast to numeric and tries to do magic with sorting. If there's a missing bin, then casting to numeric will fail and it will just keep the order
    fig.update_yaxes(
        categoryorder="array", categoryarray=pm_plot_binning_table["BinSymbol"]
    )
    # TODO: when there are many bins not all are always shown, figure out how to fix
    # see nbins argument? https://plotly.com/python/histograms/#choosing-the-number-of-bins

    fig.update_layout(width=700, height=300, showlegend=False)
    fig.show()

    # Table with binning details

    human_friendly_binning_table = predictor_binning_data.select(
        pl.col("BinIndex").alias("Index"),
        pl.col("BinSymbol").alias("Bin"),
        pl.col("BinResponseCount").alias("Responses"),
        pl.col("BinPositives").alias("Positives"),
        zRatio(pl.col("BinPositives"), pl.col("BinNegatives"))
        .round(3)
        .alias("Z Ratio"),
        (lift(pl.col("BinPositives"), pl.col("BinNegatives")) * 100)
        .round(2)
        .alias("Lift (%)"),
        (pl.col("BinPropensity") * 100).round(3).alias("Propensity (%)"),
    ).collect()

    quarto_print(
        tabulate(
            [
                [f[i] for f in human_friendly_binning_table]
                for i in range(human_friendly_binning_table.shape[0])
            ],
            headers=human_friendly_binning_table.columns,
        )
    )

    quarto_print("   ")
    quarto_print("___")  # renders as horizontal line


if predictordetails_activeonly:
    status_filter = pl.col("Status") == "Active"
else:
    status_filter = pl.col("Status") != "Classifier"

for pred in (
    predictors_summary_info.filter(status_filter)
    .select("Predictor")
    .collect()
    .to_series(0)
):
    show_single_predictor(pred)
```

# Credits

```{python}
# | echo: false

# unfortunately no way to get the quarto source file name, so that is hardcoded
quarto_source = "pega-datascientist-tools/python/pdstools/reports/ModelReport.qmd"


def get_cmd_output(args):
    result = (
        subprocess.run(args, stdout=subprocess.PIPE).stdout.decode("utf-8").split("\n")
    )
    return result


def get_version_only(versionstr):
    return re.sub("[^.\d]", "", versionstr)


def get_pip_package_version(pck):
    package_info = get_cmd_output(["pip", "show", pck])
    if len(package_info) <= 1:
        package_version = f"WARNING: {pck} not installed"
    else:
        package_version = package_info[1]
    return get_version_only(package_version)


quarto_version = get_version_only(get_cmd_output(["quarto", "--version"])[0])
pandoc_version = get_version_only(get_cmd_output(["pandoc", "--version"])[0])
timestamp_str = datetime.datetime.now().strftime("%d %b %Y %H:%M:%S")

quarto_print(
    f"""

| Tool | Version |
|-----|------|
| Notebook | {quarto_source} |
| Quarto | {quarto_version} |
| Pandoc | {pandoc_version} |
| PDS Tools | {get_pip_package_version('pdstools')} |
| Jupyter Notebook format (nbformat) | {get_pip_package_version('nbformat')} |
| Created at | {timestamp_str} |

"""
)

```

Detailed version info for **pdstools**:

```{python}
# | echo: false

from pdstools import show_versions

show_versions()
```

For more information please see the [Pega Data Scientist Tools](https://github.com/pegasystems/pega-datascientist-tools).
