---
title: "ADM Health Check"
author: "Pega pdstools"
format:
  html:
    code-fold: true
    embed-resources: true
    standalone: true
    code-tools: true
    toc: true
    toc-title: Contents
    theme:
        light: flatly
        dark: darkly
jupyter: python3
---
```{python}
#| label: Imports
#| code-fold: true
#| code-summary: Python imports
#| output: false
import logging, sys
logging.disable()
from IPython.display import display, Markdown
def Text(d):
   return display(Markdown(d))
import sys
sys.path.append('..')
from pdstools import datasets, ADMDatamart
from pdstools import cdh_utils
from pdstools import defaultPredictorCategorization
from plotly.offline import iplot
from itables import show, JavascriptFunction
import plotly.express as px
import plotly.graph_objs as go
import polars as pl
import pandas as pd
import numpy as np
from pdstools.utils import pega_template 
import math


```


```{python}
#| tags: [parameters]
#| echo: false

# These parameters are overwritten when called by the app function.
# Though if running this healthcheck separately, you can populate this cell.

name = 'CDH Sample'
filters = dict()
kwargs = dict()
include_tables=True
globalQuery = None

```

```{python}
#| tags: [initialization]
#| code-fold: true
#| code-summary: Initialization of the datamart class.

# Initialize the class after the parameters have been overwritten.
 
if len(kwargs)>0: #override with keyword arguments
    datamart = ADMDatamart(**kwargs).fillMissing()
else: #fall back to 'default'
    datamart = datasets.CDHSample()

treatment = (
    "Treatment" in datamart.modelData.columns
    and datamart.modelData.schema["Treatment"] != pl.Null
)
    
last_data = (
    datamart.last(strategy='lazy')
    .with_columns(pl.col(pl.Categorical).cast(pl.Utf8))
    .with_columns(
        [
            pl.col(pl.Utf8).fill_null("NA"),
            pl.col(pl.Null).fill_null("NA"),
            pl.col("SuccessRate").fill_nan(0).fill_null(0),
            pl.col("Performance").fill_nan(0).fill_null(0),
            pl.col("ResponseCount").fill_null(0),
            (pl.concat_str("Channel/Direction".split("/"), separator="/")).alias("Channel/Direction"),
        ]
    )
).collect()
datamart_all_columns = datamart.combinedData.columns
standardNBADNames = [
    "Assisted_Click_Through_Rate",
    "CallCenter_Click_Through_Rate",
    "CallCenterAcceptRateOutbound",
    "Default_Inbound_Model",
    "Default_Outbound_Model",
    "Email_Click_Through_Rate",
    "Mobile_Click_Through_Rate",
    "OmniAdaptiveModel",
    "Other_Inbound_Click_Through_Rate",
    "Push_Click_Through_Rate",
    "Retail_Click_Through_Rate",
    "Retail_Click_Through_Rate_Outbound",
    "SMS_Click_Through_Rate",
    "Web_Click_Through_Rate",
]

```

This document gives a generic and global overview of the Adaptive models 
and predictors. It is generated from a Python markdown file in the [Pega Data Scientist Tools](https://github.com/pegasystems/pega-datascientist-tools). That repository
of tools and scripts also contains a notebook to generate stand-alone
model reports for individual models, please refer 
to the [Wiki](https://github.com/pegasystems/pega-datascientist-tools/wiki).

This document provides a first-level scan of the models after which a 
deeper and more customer specific deeper dive can be done.

For best viewing results, open the 
HTML document in a browser. Viewing it from platforms like e.g. Sharepoint or 
Github will loose the interactive charts.

Note that the notebook by default generates a single-page HTML, however you
can also export to PDF as well as other formats supported by Pandoc (e.g. Word) 
but you would loose interactivity.

In the cell below, all data preprocessing is executed, such as importing the data and applying global filters. By default, the values are populated by environment variables supplied when running the file, but for customization purposes, you can edit this cell.

# Overview of the Actions
In a standard setup, the offers/conversations are presented as treatments for actions in a hierarchical structure setup in NBA Designer. The recommendation is to have multiple treatments for an action. Treatments are often channel specific and you would typically expect more unique treatments than there are actions.

Adaptive Models are created per treatment (at least in the default setup) and the recommendation is to stick the default context keys of the models.
```{python}
context_keys= {'Channels':'Channel/Direction', 'Issues':'Issue', 'Groups':'Group','Actions':'Name', 'Treatments':'Treatment'}
value_keys = ['Actions', 'Treatments','Issues', 'Groups', 'Channels']
counts, values = dict(), dict()

for label, column in context_keys.items():
    if column in last_data.columns:
        if label in value_keys:
            datalist = ', '.join(filter(None, (last_data.select(context_keys[label]).to_series().unique().sort().to_list())[:5]))
        else:
            datalist = ''
        n = last_data.select(column).to_series().n_unique()
    else:
        datalist, n = '', 0
    counts[f'Number of {label}'] = [n, datalist]
overview_of_adaptive_models = pd.DataFrame(counts, index=['Counts', 'Values']).T
show(overview_of_adaptive_models, columnDefs=[{"className": "dt-left", "targets": "_all"}])
```

## Success Rates per Channel
Showing the current success rate of the treatments. Different channels usually have very different success rates. Just showing the top 20 here and limiting to the propositions that have received at least 100 responses (the rates reported by the models are unreliable otherwise).

### Guidance
- Look out for propositions that stand out, having a far higher success rate than the rest. Check with business if that is expected.

- Variation in the set of offered propositions across customers is also an important metric but not one that can be derived from the Adaptive Model data - this requires analysis of the actual interactions.
```{python}
facet = "Channel/Direction"
hover_columns = [col for col in  ["Issue", "Group", "Name"] if col in datamart_all_columns]
if treatment:
    hover_columns += ["Treatment"]
df = (
    last_data.lazy()
    .with_columns(pl.concat_str(facet.split("/"), separator="/").alias(facet))
    .with_columns(pl.col(pl.Categorical).cast(pl.Utf8))
)
df = (
    df.filter(pl.col("ResponseCount") > 100)
    .select(hover_columns + ["ModelID", "Channel/Direction","SuccessRate"])
    .with_columns(pl.col("SuccessRate").round(4))
    .sort(["Channel/Direction", "SuccessRate"], descending=True)
    .groupby(["Channel/Direction"])
    .head(20)
    .collect()
).to_pandas(use_pyarrow_extension_array=True)

hover_data = {
    "SuccessRate": ":.2%",
}
for col in hover_columns:
    hover_data[col] = ":.d"


facet = "Channel/Direction"
facet_col_wrap = 3
fig = px.bar(
    df.sort_values(["Channel/Direction", "SuccessRate"]),
    x="SuccessRate",
    y="ModelID",
    color="SuccessRate",
    facet_col=facet,
    facet_col_wrap=facet_col_wrap,
    template="pega",
    text="Name",
    title="Proposition success rates <br><sup>Issue/Group/Name/Treatment</sup>",
    hover_data=hover_data,
)
fig.update_xaxes(tickformat=",.0%")
fig.update_yaxes(matches=None, showticklabels=False, visible=False).update_xaxes(
    matches=None,
).update_traces(textposition="inside")
fig.for_each_annotation(
    lambda a: a.update(text=a.text.replace("Channel/Direction=", ""))
)
fig.update(layout_coloraxis_showscale=False)

unique_count = (
    datamart.last(strategy="lazy")
    .with_columns(pl.concat_str(facet.split("/"), separator="/").alias(facet))
    .select(facet)
    .collect()
    .to_series()
    .n_unique()
)

height = 200 + (math.ceil(unique_count / facet_col_wrap) * 250)

fig.update_layout(autosize=True, height=height)

display(fig)
```

## All Success Rates
Interactive chart with all success rates.

```{python}
levels = [col for col in ["Configuration",'Channel', 'Direction', 'Issue', 'Group', "Name", "Treatment"] if col in datamart_all_columns]
fig = datamart.plotTreeMap(color_var="SuccessRate",
                     groupby_col=None,
                     levels=levels, 
                     colorscale=pega_template.success,
                     query=pl.col("ResponseCount")>100,)
fig
```

## Success Rates over Time
Showing how the overall channel success rates evolved over the time that the data export covers. Split by Channel and model configuration. Usually there are separate model configurations for different channels but sometimes there are also additional model configurations for different outcomes (e.g. conversion) or different customers (e.g. anonymous).

### Guidance
- There shouldn’t be too sudden changes over time
```{python}
by = "Channel/Direction"
facet = "Configuration"
fig = datamart.plotOverTime('SuccessRate', by=by, facets=facet, facet_col_wrap=2, query=pl.col("ResponseCount") > 100)
fig.update_yaxes(matches=None)
fig.for_each_yaxis(lambda yaxis: yaxis.update(showticklabels=True))
unique_count = datamart.modelData.with_columns(pl.concat_str(facet.split("/"), separator="/").alias(facet)).select(facet).collect().unique().shape[0]
height = 200 + (math.ceil( unique_count / 2) * 250)
fig.update_layout(autosize=True, height=height)
fig.for_each_annotation(
    lambda a: a.update(text=a.text.replace(f"{facet}=", ""))
)
fig.for_each_xaxis(lambda xaxis: xaxis.update(showticklabels=True))
fig.show()
```

# Overview of Adaptive Models
The standard configuration is to have one model per treatment.

```{python}
if include_tables:
    model_overview = datamart.model_overview.to_pandas(use_pyarrow_extension_array=True)
    show(model_overview)
    print(f"There are a total of {len(last_data.select('ModelID').unique())} unique models in the latest snapshot. \n " )
else:
    print('Please refer to the `model_overview` tab in the included Excel file.')
```
## Model Performance 
 Recommended best practice is to have multiple treatments for an action. Too few gives less opportunity for personalization of the interactions.

### Model Performance vs Action Success Rates (the Bubble Chart)
This “Bubble Chart” - similar to the standard plot in the ADM reports in Pega - shows the relation between model performance and proposition success rates. In addition, the size of the bubbles indicates the number of responses.

#### Guidance
- If all the bubbles clutter too much on the left-hand side of the charts, this means the models are not predictive. These models may be still be ramping up, or they may not have predictive enough features to work with: consider if new/better predictors can be added.

- Bubbles at the bottom of the charts represent propositions with very low success rates - they may not be compelling enough.

- In an ideal scenario you will see the larger bubbles more on the top-right, so more volume for propositions with higher success rates and better models.

- There should be a positive correlation between success rate and performance - per channel.

- There should be a positive correlation between responses and performance - also per channel.

- There should be variation in response counts (not all dots of equal size)

- For small volumes of good models, see if the engagement rules in the Decision Strategy are overly restrictive or reconsider the arbitration of the propositions so they get more (or less) exposure.
```{python}
facet_col_wrap=2
facet = 'Configuration/Channel/Direction'
fig = datamart.plotPerformanceSuccessRateBubbleChart(facets=facet,facet_col_wrap=facet_col_wrap)
fig.layout.coloraxis.colorscale = pega_template.success
fig.for_each_xaxis(lambda xaxis: xaxis.update(showticklabels=True, visible=True))
fig.for_each_xaxis(lambda xaxis: xaxis.update(dict(
        tickmode = 'array',
        tickvals = [50, 60, 70, 80, 90,100],
    )))
height = 250 + (math.ceil( len(fig.layout.annotations) / facet_col_wrap) * 270)
fig.update_layout(autosize=True, height=height, title=None)

fig.for_each_annotation(
    lambda a: a.update(text=a.text.replace(f"{facet}=", ""))
)
fig.update_layout(font=dict(size=10))

fig.for_each_annotation(lambda a: a.update(text="<br> ".join(a.text.split("/", 1))))
fig.update_coloraxes(colorbar_len= 1 / math.ceil( len(fig.layout.annotations) / facet_col_wrap))

fig.show()
```

### Model Performance over Time
Showing how the model performance evolves over time. Note that ADM is by default configured to track performance over all time. You can configure a window for monitoring but this is not commonly done.

Aggregating up to Channel and splitting by model configuration.

#### Guidance
- No abrupt changes but gradual upward trend is good
```{python}
facet = "Configuration"
modelperformance = datamart.plotOverTime('weighted_performance', by="Channel/Direction", facets=facet, facet_col_wrap=2)

unique_count = datamart.modelData.with_columns(pl.concat_str(facet.split("/"), separator="/").alias(facet)).select(facet).collect().unique().shape[0]
height = 200 + (math.ceil( unique_count / 2) * 250)
modelperformance.update_layout(autosize=True, height=height)
modelperformance.for_each_annotation(
    lambda a: a.update(text=a.text.replace(f"{facet}=", ""))
)

modelperformance.for_each_xaxis(lambda xaxis: xaxis.update(showticklabels=True))

modelperformance.show()
```

### Model performance of all the actions (interactive Treemap)
Using an interactive treemap to visualize the performance. Lighter is better, darker is low performance.

It can be interesting to see which issues, groups or channels can be better predicted than others. Identifying categories of items for which the predictions are poor can help to drive the search for better predictors, for example.

```{python}
fig = datamart.plotTreeMap(color_var="performance_weighted", groupby_col= None, levels=levels)

fig.show()
```

### Response counts for all the actions
Using an interactive treemap to visualize the response counts.
Different channels will have very different numbers but within one channel the relative differences in response counts give an indication how skewed the distribution is.

Warning : Currently treemap calculates mean response count moving upwards in the hierarchy. 
```{python}
fig = datamart.plotTreeMap(color_var="responsecount", levels=levels, colorscale=pega_template.negative_positive)
fig

```
# Analysis of Predictors
This analysis focuses on finding which are top predictors that are driving the models.

The predictors are categorized (by color) by the “source”. By default this takes just the first part before the dot, so this typically distinguishes between Customer, Account, IH and parameterized (Param.) predictors. You can customize this to add patterns to identify for example external scores.

## Number of Predictors per model configuration 

```{python}
#| error: true
if include_tables:
    predictors_per_configuration = datamart.predictors_per_configuration.to_pandas(use_pyarrow_extension_array=False)
    show(predictors_per_configuration)
else:
    print('Please refer to the `predictors_per_configuration` tab in the included Excel file.')
```


## Predictor Importance across all models per configuration
Box plots of the predictor importance. Importance can be shown either as a global feature importance or simply as the univariate predictor importance.

### Guidance
- Number of predictors per model 200 - 700
- There should be IH* predictors
- There should not be > 100 IH predictors
- No more than a few dozen Param predictors

```{python}
#| error: true
if datamart.predictorData is not None:
    figs = datamart.plotPredictorPerformance(top_n=20, facets='Configuration',separate=True, active_only=True)
    if not isinstance(figs, list):
        figs = [figs]
    for fig in figs:
        fig.update_traces(width=0.3)
        fig.update_layout(font=dict(size=10))
        fig.layout.xaxis.tickformat = ".d"

        fig.show()
else:
    Text("Predictor data is not available.")
```

## Importance by Predictor Category 
Aggregating up to the category of the predictors. This gives a view at a glance of how well e.g. interaction history, external model scores or contextual data are doing overall.

### Predictor Category performance per Channel/Direction/Issue 

```{python}
#| error: true
facets = 'Configuration/Channel/Direction'
facet_col_wrap = 3
fig = datamart.plotPredictorCategoryPerformance(facets=facets, facet_col_wrap=facet_col_wrap)

fig.update_layout(font=dict(size=10))
fig.for_each_annotation(
    lambda a: a.update(text=a.text.replace(f"{facets}=", ""))
)
fig.for_each_annotation(lambda a: a.update(text=" <br> ".join(a.text.split("/"))))

facet_list = []
for data in fig.data:
    facet_list.append(data["yaxis"][1:])
facet_count = len(set(facet_list))

height = 200 + (math.ceil( facet_count/ facet_col_wrap) * 250)
fig.update_layout(autosize=True, height=height)
fig.show() 

```

### Relative Predictor Category importance per Configuration
Although the same could be achieved using the standard **plotPredictorImportance** method, now that we only split by Configuration this allows for a more compact visualization using a stacked bar chart.

```{python}
''' By dividing a predictor category's weighted performance to the sum of all predictor categories weighted performance in a configuration, creates a plot that displays relative importance of categories in a configuration.
Changes the Predictor performance range from 50-100 to 0-100 in order to increase visibilty of performance differences among categories.'''

fig = datamart.plotPredictorContribution()
height = 200 + (math.ceil(len(datamart.modelData.select(pl.col("Configuration").unique()).collect()['Configuration'].to_list()) / 2) * 50)
fig.update_layout(height=height)
fig.update_yaxes(
    automargin=True,
    dtick=1,
)
fig.show()

```

## Bad predictors across all models
See if there are predictors that are just always perform poorly.

### Guidance
- Predictors that consistently perform poorly could potentially be removed.
- Be sure to check for data problems
- Note we advise to be careful with predictor removal. Only remove if there is clearly no future value to other propositions as well or if there is always a related predictor that performs better.

```{python}
# weighted performance
if include_tables:
    responses_column_index = datamart.bad_predictors.columns.index("Response Count")
    bad_predictors = datamart.bad_predictors.to_pandas(
        use_pyarrow_extension_array=False
    )

    show(
        bad_predictors,
        scrollX=True,
        columnDefs=[
            {
                "className": "dt-left",
                "targets": [responses_column_index],
                "createdCell": JavascriptFunction(
                    """
function (td, cellData, rowData, row, col) {
    if (cellData < 200) {
        $(td).css('color', 'red')
    }
}
"""
                ),
            }
        ],
    )


else:
    print("Please refer to the `bad_predictors` tab in the included Excel file.")
```

## Number of Active and Inactive Predictors
Showing the number of active and inactive predictors per model.

### Guidance
- We expect a fewModels that have never been used
 dozen active predictors for every model instance
```{python}
facets= ["Configuration"]
fig = datamart.plotPredictorCount(facets = facets)

height = 250 + (math.ceil(len(fig.layout.annotations) / 2) * 270)
fig.update_layout(autosize=True, height=height)
fig.update_yaxes(categoryorder="array", automargin=True, dtick=1)
fig.for_each_annotation(
    lambda a: a.update(text=a.text.replace(f"{facets[0]}=", ""))
)
fig.show()
```

## Predictor Performance across Propositions
A view of predictor performance across all propositions, ordered so that the best performing predictors are at the top and the best performing propositions are on the left. Green indicates good performance, red means more problematic - either too low or too good to be true.

```{python}
index_cols = [col for col in ['Issue', 'Group', "Name", "Treatment"] if col in datamart_all_columns]
if datamart.predictorData is not None:
    unique_configurations = datamart.combinedData.collect().get_column("Configuration").unique().to_list()
    for conf in unique_configurations:
        try:
            fig = datamart.plotPredictorPerformanceHeatmap(top_n=20, 
                                                    by="/".join(index_cols),
                                                    query = pl.col("Configuration")==conf,
                                                    tickangle=45
                                                    )
            fig.update_layout(
                xaxis={
                    "tickmode": "array",
                    "tickvals": fig.data[0]["x"],
                    "ticktext": [
                        "..." + tick[-25:] if len(tick) > 25 else tick for tick in fig.data[0]["x"]
                    ],
                },
                font=dict(size=8),
                title=f"Top predictors over {conf}"
            )

            fig.show()
            
        except:
            print(f"Plot was not drawn for {conf} because of an error")    #TODO: add error message instead of typing "an error"
else:
    Text("There is no predictor data available.")
```

## Missing values
If a predictor is low performing: are there too many missing values? This could point to a technical problem
Missing % is number of missing vs all responses, really just a filter on model data
This TreeMap only shows the fields that have any missing values.

```{python}
path =  [col for col in ["Configuration", "PredictorCategory", "PredictorName"] if col in datamart_all_columns]
gb_cols = path
path = [px.Constant("All Models")] + path 

missing = (
    datamart.combinedData.filter(pl.col("BinSymbol") == "MISSING")
    .groupby(gb_cols)
    .agg(pl.sum("BinResponseCount").alias("MissingCount"))
)
whole_df = (
    datamart.combinedData.groupby(gb_cols).agg(pl.sum("BinResponseCount"))
)

with_missing = whole_df.join(missing, on=gb_cols, how="inner").with_columns(
    (pl.col("MissingCount") / pl.col("BinResponseCount")).alias("Percentage without responses")
).filter((~pl.col("Percentage without responses").is_nan())).collect()

hover_data = {
    "Percentage without responses": ":.2%",
}

fig = px.treemap(
    with_missing.to_pandas(),
    path=path,
    color="Percentage without responses",
    template="pega",
    hover_data=hover_data,
    title="Missing Data in Adaptive Models",
)

fig.layout.coloraxis.colorscale =  pega_template.positive_negative

fig.show()


```

# Responses

In the sections below we check which of these models have reached certain reliability (or “maturity”) thresholds. This is based on heuristics on both the number of positives (> 200 considered mature) and performance.

## Empty and Immature Models
All below lists are guidance. There should be just a small percentage of immature or empty models overall. Having no or just 1 active predictor is very suspicious

### Models that have never been used
These models have no responses at all: no positives but also no negatives. The models for these actions/treatments exist, so they must have been created in the evaluation of the actions/treatments, but they were never selected to show to the customer, so never received any responses.

Often these represent actions that never made it into production and were only used to test out logic. But it could also be that the response mechanism is broken. It could for example be caused by outcome labels that are returned by the channel application not matching the configuration of the adaptive models.

```{python}
if include_tables:
    zero_response = datamart.zero_response.to_pandas(use_pyarrow_extension_array=True)

    if zero_response.shape[0] >0:
        print(f"There are {zero_response.shape[0]} models with zero response")
        show(zero_response, scrollX = True)
    else:
        print("All models have received at least 1 response.")
else:
    print('Please refer to the `zero_response` tab in the included Excel file.')
```

```{python}
#| output: asis
if include_tables:
    zero_positives = datamart.zero_positives.to_pandas(use_pyarrow_extension_array=True)
    if zero_positives.shape[0] >0:
        print("### Models that have have been used but never received a positive response")
        show(zero_positives, scrollX = True)
    else:
        print("All models have received at least 1 positive Response.")
else:
    print('Please refer to the `zero_positives` tab in the included Excel file.')
```

### Models that are still in an immature phase of learning

These models have received at least one positive response but not enough yet to be qualified to be fully  “mature” - a concept that matters especially for outbound channels. 
These actions are typically new and still in early phases of learning. 
We show the “reach” of these actions as the percentage of the population that would be selected by the standard maturity capping algorithm in the NBA framework (which selects 2% for new models and 100% for models with 200 or more positive responses


```{python}
if include_tables:
    reach = datamart.reach.to_pandas()
    reach.style.format({
        'Reach': '{:,.2%}'.format})
    show(reach, scrollX=True)
else:
    print('Please refer to the `reach` tab in the included Excel file.')

```


```{python}
#| output: asis
if include_tables:
    min_perf_df = datamart.minimum_performance.to_pandas(use_pyarrow_extension_array=True)
    if min_perf_df.shape[0] >0:
        print('''### Models that have received sufficient responses but are still at their minimum performance 
    These models also have received over 200 positives but still show \n
    the minimum model performance. This could be an indication of data problems, \n
    or not having the right predictors but may also be caused by technical aspects \n
    like the order of the responses to the model. \n
        ''')
        show(min_perf_df, scrollX = True)
    else:
        print("All models with over 200 positive responses are above minimum performance")
else:
    print('Please refer to the `minimum_performance` tab in the included Excel file.')
```

## Number of Empty/Immature Models over time 

In the analysis below we count the number of models in each of the groups analysed before and show how that \ncount changes over time. The expectation is that the number of “non-empty” models increases steadily and the other lines are more or less stable.\n
Empty is defined as having no responses at all. Immature is defined as having < 200 positives, and no performance means model performance is still the initial 0.5 value while having matured already according to the definition.

## Guidance
- Empty models shouldnt be increasing too much
- Good models (AUC 55-80) should increase or at least not decrease
- Good models should be much higher than problem kids
```{python}
by= ["SnapshotTime", "Channel", "Direction"]
df = (
    datamart.modelData
    .with_columns(pl.col(pl.Categorical).cast(pl.Utf8))
    .with_columns(pl.col(pl.Utf8).fill_null("Missing"))
    .groupby(by)
    .agg(
        [
            ((pl.col("Positives") > 0) & (pl.col("Positives") < 100 )).sum().alias("immature_count"),
            (pl.col("ResponseCount")==0).sum().alias("EmptyModels"),
            (pl.col("Positives")==0).sum().alias("Models w/o positives"),
            (pl.col("Performance")==0.5).sum().alias("Models w/o Performance"),
            (pl.col("ResponseCount")!=0).sum().alias("Number of non-empty Models")
            
        ]
    )
    .sort(["Channel","Direction","SnapshotTime"], descending=True)
)
facet_col_wrap=3
facet = "Channel/Direction"
df = df.with_columns(pl.concat_str(facet.split("/"), separator="/").alias(facet)).with_columns(pl.col(facet).cast(pl.Utf8).fill_null("NA"))
df_pd = df.collect().to_pandas()
y = df_pd.iloc[:,len(by):].columns.tolist() #slice `df.columns` to get this easily
fig = px.line(df_pd,
    x="SnapshotTime",
    y=y,
    facet_col = facet,
    title="Immature and Empty Models over Time",
    template="pega",
    facet_col_wrap=facet_col_wrap,
    )
fig.for_each_annotation(
    lambda a: a.update(text=a.text.replace(f"{facet}=", ""))
)
unique_count = len(fig.layout["annotations"])
height = 200 + (math.ceil( unique_count / facet_col_wrap) * 250)
fig.update_layout(autosize=True, height=height)

fig.show()
```

## Number of Responses over time 


```{python}
facets = "Configuration"
facet_col_wrap = 2
response_counts = datamart.plotOverTime('ResponseCount', by="Channel/Direction", facets=facets, every="1h", facet_col_wrap=facet_col_wrap)

unique_count = datamart.modelData.select(facets).unique().collect().shape[0]
height = 200 + (math.ceil( unique_count / 2) * 250)
response_counts.update_layout(autosize=True, height=height)
response_counts.for_each_annotation(
    lambda a: a.update(text=a.text.replace(f"{facets}=", ""))
)
response_counts.show()
```

# Which Models drive most of the Volume
Showing the cumulative response count vs the number of models. Is there a larger percentage of models that take the vast majority of the responses? \n
If this line strongly deviates from the diagonal it means that relatively few models drive the majority of the responses. \n
In the left-hand plot we look at all responses, which really means that we are looking at “impressions” mostly. The right-hand plot looks at just the positives. Typically, the positives are driven more strongly by the models so often you see more skewness in that one. \n
However very skewed results may be caused by prioritization elements like levers and weights and can be a reason to check in with business and verify that this is expected. \n

## Analysis of skewness of the Responses
Showing the cumulative response count vs the number of models. Is there a larger percentage of models that take the vast majority of the responses?

If this line strongly deviates from the diagonal it means that relatively few models drive the majority of the responses.

In the left-hand plot we look at all responses, which really means that we are looking at “impressions” mostly. The right-hand plot looks at just the positives. Typically, the positives are driven more strongly by the models so often you see more skewness in that one.

However very skewed results may be caused by prioritization elements like levers and weights and can be a reason to check in with business and verify that this is expected.

### Guidance
- Area under this curve should be > 0.5 and perhaps more towards 1 - most of the responses driven by relatively few actions
```{python}
all_responses = datamart.plotResponseGain(title_text=" / All Responses")
only_positives = datamart.plotResponseGain(title_text=" / Only Positive Responses", query=pl.col("Positives") >0)

cdh_utils.legend_color_order(all_responses).show()
cdh_utils.legend_color_order(only_positives).show()
```

## Models with largest number of responses (positive or negative)
Zooming in into the models that drive most of the responses, here we list the top 20 models with the highest number of responses.

```{python}
subset = ['Configuration', 'Issue', 'Group', 'Name', 'Channel', 'Direction']
facet = '/'.join([col for col in subset if col in datamart_all_columns])

for split_facet in facet.split("/"):
    last_data = last_data.with_columns(
        pl.col(split_facet).cast(pl.Utf8).fill_null("NA")
    )
last_data = last_data.with_columns(pl.concat_str(facet.split("/"), separator="/").alias(facet))
response_counts = last_data.groupby([facet] + facet.split("/")).agg(
    [
        pl.sum("ResponseCount").alias("all_responses"),
        pl.sum("Positives").alias("positive_sum")
        ]
    )

all_responses = response_counts.sort("all_responses", descending=False).tail(20)
hover_data = {
    facet: False
}
for col in facet.split("/"):
    hover_data[col] = ":.d"

possible_color_vars = ['Channel', 'Issue', 'Name']
color = next((col for col in possible_color_vars if col in datamart_all_columns), None)

fig = px.bar(all_responses.to_pandas(use_pyarrow_extension_array=True), x ="all_responses", y=facet, color=color, title="Top 20 Highest Responses", template="pega", text=facet, hover_data=hover_data)
fig.update_yaxes(matches=None, showticklabels=False, visible=False).update_traces(textposition="inside")
fig.for_each_annotation(
    lambda a: a.update(text=a.text.replace(facet, ""))
)
fig.update_layout(yaxis={'categoryorder':'total ascending'})    
cdh_utils.legend_color_order(fig).show()
```

## Models with largest number of positive responses. 
And these are the 20 models with the largest number of positives.

```{python}
positives = response_counts.sort("positive_sum", descending=True).head(20)
fig = px.bar(positives.to_pandas(use_pyarrow_extension_array=True), x ="positive_sum", y=facet, color=color, title="Top 20 Highest Positives", template="pega", text=facet, hover_data=hover_data)
fig.update_yaxes(matches=None, showticklabels=False, visible=False).update_xaxes(matches=None).update_traces(textposition='inside')
fig.for_each_annotation(
    lambda a: a.update(text=a.text.replace(facet, ""))
)
fig.update_layout(yaxis={'categoryorder':'total ascending'})

cdh_utils.legend_color_order(fig).show()
```

## Analysis of Performance vs Volume
Is most volume driven by models that have a good predictive performance? Ideally yes, so the targeting of the customers is optimal. If a lot of volume is driven by models that are not very predictive, this could be a reason to look into the available predictor data.

The plot below shows this relation. Horizontally the model performance (the AUC, ranging from 50 to 100 as Pega usually scales this), descretized into a number of ranges, and vertically the percentage of responses.

A lot of volume on the first bins, where the performance is minimal, means that a lot of immature models are used. This is sub-optimal in terms of targeting. Ideally there is a smooth curve with a peak in the 60-80 range of AUC. Much higher AUC’s are possibly indicative of immature models or even outcome leakers (although that is effectively prevented by the standard delayed learning pattern). AUC’s below 60 are not uncommon but should be investigated - consider different predictors or outcomes.
```{python}
#| error: true
to_plot = "Performance"
df = (
    datamart.modelData.with_columns(pl.col(to_plot) * 100)
    .groupby([to_plot, "Channel", "Direction"])
    .agg(pl.sum("ResponseCount"))
    .with_columns(pl.col(to_plot).round(2))
    .collect()
)

cut_off_value = [percentile for percentile in range(50, 100, 3)]
df_pl = df.get_column(to_plot).fill_null(0).fill_nan(0).cut(bins=cut_off_value, category_label="PerformanceBin").lazy()
join = df.lazy().join(
    df_pl.select([to_plot, "PerformanceBin"]).unique(), on=to_plot, how="left"
)
grouped = join.groupby(["Channel", "PerformanceBin"]).agg(pl.sum("ResponseCount"))
out = (
    grouped.sort(["Channel", "PerformanceBin"])
    .select(
        [
            pl.col("Channel").cast(pl.Utf8),
            "PerformanceBin",
            "ResponseCount",
            pl.col("ResponseCount").sum().over("Channel").alias("sum"),
        ]
    )
    .with_columns([(pl.col("ResponseCount") / pl.col("sum")).alias("Volume")])
)

out = out.sort(["Channel", "PerformanceBin"]).collect().to_pandas()
fig = px.bar(
    out,
    x="PerformanceBin",
    y="Volume",
    color="Channel",
    template="pega",
    barmode="overlay",
)
for bar in fig.data:
  bar.visible = "legendonly"

channels = out["Channel"].unique()
for channel_num, channel in enumerate(channels):
    channel_df = out[out["Channel"] == channel]
    fig.add_traces(go.Scatter(x = channel_df["PerformanceBin"], y=channel_df.Volume, line_shape = 'spline', marker_color = fig.data[channel_num].marker.color, name=channel))


fig.update_yaxes(tickformat=",.0%")
fig.update_xaxes(categoryorder='array', categoryarray = out.sort_values("PerformanceBin")["PerformanceBin"].unique())

cdh_utils.legend_color_order(fig).show()

```

## Positives vs. Number of Models
Ideally, all models have received plenty of responses which will make them “mature” and makes sure they are as predictive as possible. \n
Often we see that there is a significant percentage of models that are still relatively new and have not received much feedback (yet). Below graph shows the percentages of models that have fewer than 200 positives. \n
Having many on the left-hand side (with very low or perhaps no positives) may or not be a problem. The models may still be there in the datamart but might represent actions/treatments that are not active. 

```{python}
fig = datamart.plotModelsByPositives()

fig = cdh_utils.legend_color_order(fig)
fig.show()

```

# Propensity Analysis
The distribution of propensities returned by the models is yet a different angle.

Higher propensities clearly indicate the offers are more attractive - people apparenty click/accept/convert more often.

## Propensity Distribution
In a more emphathetic setup, you would expect that the distribution of the propensities leans towards the right-hand side: more volume to more attractive offers, although the relation is of course more complex, we are not just blindly pushing the offers with the highest success rates, but take a personalized approach.

Often however, multiple factors are included in the prioritization, changing this picture.

Note that the propensity bins are not of equal width. Propensities are typically very low so with an equal width distribution, almost all volume would be in the first bins. The binning here is based on (roughly) equal volume across all data.

So when one of the graphs shows more volume on the left, that is to be interpreted as relative to the other graphs.

```{python}
#| error: true
to_plot = "Propensity"
if to_plot == "Propensity" and to_plot not in datamart.predictorData.columns:
    to_plot = "BinPropensity"
df = datamart.combinedData.filter(pl.col("PredictorName")!="Classifier").groupby([to_plot, "Channel", "Direction"]).agg(pl.sum("BinResponseCount")).with_columns(pl.col(to_plot).round(4).cast(pl.Float64)).collect()
color_col = "Channel"
smallest_bin = 0

for color in df.get_column(color_col).unique().to_list():
    color_df = df.filter(pl.col(color_col) == color)
    propensity_list = list(set(color_df.select(to_plot).fill_null(0).fill_nan(0).filter(pl.col(to_plot)>0).get_column(to_plot).to_list()))
    if len(propensity_list) > 0:
        if (np.percentile(propensity_list, 10) > smallest_bin):
            smallest_bin = np.percentile(propensity_list, 10)
            cut_off_value = np.percentile(propensity_list, [percentile for percentile in range(0,101,5)])

df_pl = df.get_column(to_plot).fill_null(0).fill_nan(0).cut(bins=cut_off_value, category_label="propensity").lazy()
join= df.lazy().join(df_pl.select([to_plot, "propensity"]).unique(), on=to_plot, how="left")
grouped = join.groupby(["Channel","propensity"]).agg(pl.sum("BinResponseCount"))

out = grouped.sort(["Channel","propensity"]).select(
    [
        "Channel",
        "propensity",
        "BinResponseCount",
        pl.col("BinResponseCount").sum().over("Channel").alias("sum"),
        
    ]
).with_columns([
    (pl.col("BinResponseCount")/ pl.col("sum")).alias("Responses")
    ])

out = out.sort(["Channel", "propensity"]).collect().to_pandas()
fig = px.bar(out, x='propensity', y='Responses', color=color_col,template="pega", barmode="overlay")
fig.update_yaxes(tickformat=",.0%")
fig.update_xaxes(categoryorder='array', categoryarray = out.sort_values("propensity")["propensity"].unique())

fig.show()

```

# Appendix - all the models

A list of all the models is written to a file so a script can iterate over all models and generate off-line model reports for each of them.

Generally you will want to apply some filtering, or do this for specific models only. This can be accomplished in either this script here, or by editing the generated file.
```{python}
if include_tables:
    responses_column_index = datamart.appendix.columns.index("Responses")
    appendix = datamart.appendix.to_pandas(use_pyarrow_extension_array=True)
    show(
        appendix,
        columnDefs=[
            {
                "targets": [responses_column_index],
                "createdCell": JavascriptFunction(
                    """
function (td, cellData, rowData, row, col) {
    if (cellData < 200) {
        $(td).css('color', 'red')
    }
}
"""
                ),
            }
        ],
    )
else:
    print("Please refer to the `appendix` tab in the included Excel file.")

```