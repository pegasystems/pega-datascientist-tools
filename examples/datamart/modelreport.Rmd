---
title: "`r params$title`"
subtitle: "`r params$modeldescription`"
author: "Pega"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document: default
always_allow_html: true
params:
  # Below default values for the parameters. This notebook is usually launched from a (bash)script in which
  # these values are set. That also allows to run the notebook multiple times for different values.
  
  title:
    value: "Adaptive Model Report"
    
  predictordatafile:
   # Full path to ADM predictor binning table data. Can be CSV, zipped CSV,
   # LD-JSON, parquet or more.
   value: "../../data/pr_data_dm_admmart_pred.csv"

  modeldescription:
    # Optional model description typically corresponding to the model info of the indicated model
    value: "Sales Model - PSDISCOUNT100"

  modelid:
    # Model ID that is used to filter the predictor binning table data. If the
    # binning data only contains a single Model ID it can be left empty.
    value: "68e1d164-81e3-5da0-816c-0bbc3c20ac6c" # default

  # Showing the binning of the predictors for all or only for the active ones.
  predictordetails_activeonly: true
---

```{r, echo=F, warning=F, error=F, include=FALSE}
library(pdstools)

# include below when developing the library
# sapply(list.files("~/Documents/pega/pega-datascientist-tools/r/R", "*.R", full.names = T), source)

library(data.table)
library(lubridate)
library(ggplot2)
library(scales)
library(knitr)
library(kableExtra)
library(plotly)
library(gridExtra)

theme_set(theme_minimal())
options(digits = 5)
knitr::opts_chunk$set(
  comment = ">", echo = FALSE, warning = FALSE, fig.width = 4, fig.height = 4
)
```


```{r, echo=F, warning=F, error=F, include=F}

# Code below reads the predictor data and is bloated somewhat to deal with various formatting issues,
# different product versions not always having exactly the same fields etc. In the end, it produces a
# well behaved modelPredictorBins object with the current (latest) snapshot.
# If there are multiple snapshots, predPerformanceOverTime will give that data.

if (!("predictordatafile" %in% names(params))) stop("Predictor binning missing. Please provide through parameter 'predictordatafile'.")
if (!file.exists(params$predictordatafile)) stop(paste("File does not exist:", params$predictordatafile))

datamart <- ADMDatamart(modeldata = F,
                        predictordata = params$predictordatafile, # TODO: could be more protective against using model data by mistake (missing columns...)
                        filterPredictorData = function(preds) {
                          if (!is.null(params$modelid)) {
                            if (params$modelid != "") {
                              preds <- preds[ModelID == params$modelid]
                            } else {
                              if (length(unique(preds$ModelID)) > 1) {
                                stop("No model ID specified but found multiple model instances in the data.")
                              }
                            }
                          } else {
                            if (length(unique(preds$ModelID)) > 1) {
                              stop("No model ID specified but found multiple model instances in the data.")
                            }
                          }
                          if (length(unique(preds$ModelID)) != 1) {
                            stop(paste("Expected one model in the data but got", length(unique(preds$ModelID))))
                          }
                          
                          if (hasMultipleSnapshots(preds)) {
                            # Take the latest snapshots from the last day. We're doing this carefully as we don't want to report on old bins
                            # so just keeping the last day, then per predictor finding the actual last snapshot. This may not work in a situation
                            # where not all models are updated frequently.
                            
                            lastDay <- max(lubridate::floor_date(preds$SnapshotTime, unit = "days"))
                            preds <- preds[lubridate::floor_date(SnapshotTime, unit="days") == lastDay]
                            preds <- filterLatestSnapshotOnly(preds)
                          }
                          return(preds)
                        })

if (nrow(datamart$predictordata) <= 1) {
  stop(paste("No data found for model ID", params$modelid))
}

modelPredictorBins <- datamart$predictordata
classifierBinning <- filterClassifierOnly(modelPredictorBins)
```

# Model Performance and Score Distribution

The model scores (sum of the log odds of the Naive Bayes classifier) are mapped to propensities in the Classifier of ADM. This classifier is constructed using the PAV (Pool Adjacent Violators) algorithm, a form of monotonic regression.

## Model Performance

The model reports a performance of **`r round(modelPredictorBins[EntryType == "Classifier"]$Performance[1],3)`** measured in AUC-ROC. This number is calculated from the "active" bins of the Classifier.

The "active" bins are the ones that can be reached from the current binning
of the predictors. When calculating the AUC from all the classifier binning
all bins will be taken into account, also bins (typically
at the start or end) that can no longer be reached from the current predictor
binning.

Finding the actual range is not trivially done outside of the platform. Here we 
report both the AUC-ROC as well as the AUC-PR from the full range.

```{r, echo=F, warning=F, error=F, include=F}
fullrange_auc_roc <- pdstools::auc_from_bincounts(classifierBinning$BinPositives, classifierBinning$BinNegatives)
fullrange_auc_pr <- pdstools::aucpr_from_bincounts(classifierBinning$BinPositives, classifierBinning$BinNegatives)
```


|Total Positives|Total Negatives|Total Responses|Success Rate|AUC|AUC-ROC (Full Range)|AUC-PR (Full Range)|
|--------------:|--------------:|--------------:|-----------------:|-----------------:|-----------------:|-----------------:|
|`r sum(classifierBinning$BinPositives)`|`r sum(classifierBinning$BinNegatives)`|`r sum(classifierBinning$BinResponseCount)`|`r sprintf("%.2f%%", 100*sum(classifierBinning$BinPositives)/sum(classifierBinning$BinResponseCount))`|`r round(modelPredictorBins[EntryType == "Classifier"]$Performance[1],3)`|`r round(fullrange_auc_roc,3)`|`r round(fullrange_auc_pr,3)`|

```{r, results="asis", echo=F, warning=F, error=F, fig.align = "center"}
if (nrow(classifierBinning) < 1) {
  cat("<p style='color:Red;'>NO data available for Classifier for date:", max(modelPredictorBins$SnapshotTime), "</p>", fill=T)
}
```

## Score Distribution

The Score Distribution shows the volume and average propensity in every bin of the 
score ranges of the Classifier.

Note the interactive plot below (created with Plotly) does not render well 
when viewed in portals like Sharepoint or Box, and will also not render to PDF. 
Make sure to open the HTML file in a browser.

```{r fig.align="center", fig.width=8, results="asis"}
subtitle <- paste0("Performance: ", round(classifierBinning$Performance[1],5), " (AUC)")

# Using a native Plotly version of the graph to make the hover-over work. Just 
# ggplotly(p) doesnt work perfectly, unfortunately.

classifierBinning[, propensity := 100*BinPositives/BinResponseCount]

pegaClassifierBlueBar <- "#278DC1"
pegaClassifierYellowLine <- "EF8B08"

if (nrow(classifierBinning) >= 1) {
  ply <- plot_ly(classifierBinning) %>%
    add_bars(x = ~BinIndex, y = ~BinResponseCount, 
             colors = pegaClassifierBlueBar,
             alpha = 0.8,
             hoverinfo = "text", 
             text = ~paste0("Score Range: ", BinSymbol, "\nResponses: ", BinResponseCount, "\nPropensity: ", sprintf("%.2f%%", propensity)),
             yaxis = 'y') %>%
    add_lines(x = ~BinIndex, y = ~propensity,
              line = list(color = pegaClassifierYellowLine, width = 4),
              yaxis = 'y2') %>%
    add_markers(x = ~BinIndex, y = ~propensity,
                marker = list(color="black"),
                hoverinfo = "text", text = ~sprintf("%.2f%%", propensity),
                yaxis = 'y2') %>%
    layout(title = paste0("Score Distribution of the Classifier"),
           xaxis = list(title = "Bin Index"), # to put values instead of bin indices: , tickangle = -45, tickmode = "array", tickvals = ~bin, ticktext = ~BinSymbol
           yaxis = list(side = 'left', title = "Responses"),
           yaxis2 = list(side = 'right', overlaying = "y", title = 'Propensity (%)', 
                         showgrid = FALSE, zeroline = FALSE, automargin = TRUE, rangemode = "tozero"),
           showlegend = FALSE,
           annotations = list(list(x = 0.5 , y = 1.02, 
                                   text = subtitle, showarrow = F, 
                                   xref='paper', yref='paper'))) %>% 
    config(displayModeBar = F)
  
  ply
}
```

Propensity is defined as $\frac{positives}{positives+negatives}$ per bin. 

The adjusted propensity that is returned is a small modification (*Laplace smoothing*) to this and calculated as $\frac{0.5+positives}{1+positives+negatives}$ so new models initially return a propensity of 0.5. This helps 
to address the cold start problem when introducing new actions.

```{r, echo=F, warning=F, error=F, include=T}
if (nrow(classifierBinning) >= 1) {
  kable(userFriendlyADMBinning(classifierBinning)) %>% kable_styling()
}
```


## Cumulative Gains and Lift charts

Below are alternative ways to view the Classifier.

The Cumulative Gains chart shows the percentage of he overall cases in the "positive" category gained by targeting a percentage of the total number of cases. For example, this view shows how large a percentage of the total expected responders you target by targeting only the top decile.

The Lift chart is derived from this and shows the ratio of the cumulative gain and the targeted volume.

```{r, fig.align = "left", fig.width=8, fig.height=3}
# right align is nicer but plotly doesnt do that

# for inspiration on the charts:
# see http://dmg.org/pmml/v4-0-1/ModelExplanation.html#gainscharts
# and https://www.ibm.com/support/knowledgecenter/de/SSLVMB_24.0.0/spss/tutorials/mlp_bankloan_outputtype_02.html

if (nrow(classifierBinning) >= 1 & (sum(classifierBinning$BinNegatives) + sum(classifierBinning$BinPositives) > 0)) {
  cumGains <- plotCumulativeGains(classifierBinning)
  
  cumLift <- plotCumulativeLift(classifierBinning)
  
  # TODO: add a cum Success rate one. Like lift but w/o the division.
  # TODO: consider ggplot of these, those work just fine
  # TODO: classic one on separate line. Make bars less prominent. Swap y-axes - they're opposite what product does.
  
  classic <- plotBinning(classifierBinning) + ggtitle("Score Distribution")
  
  #ggplotly(cumGains) - TODO convert to plotly plot as ggplotly doesnt show both axes
  
  grid.arrange(cumGains, cumLift, classic, nrow=1)
}
```

# Performance by Predictor Category

Showing the performance across all predictors. The predictor categories default to the string before the first dot. This 
can be customized when reading the data for a particular customer.

```{r Predictor Category}
plotPredictorImportance(datamart, categoryAggregateView = T) +
  scale_fill_discrete(guide="none")
```

# Predictor Overview

Showing all the predictors for this model. Predictors are sorted by performance
and grouped if they are correlated (shown subtly with an indentation).

The negatives and postives count are usually the same across all the predictors
but will be different when predictors have been removed or added.

For Adaptive Gradient Boosting models ("AGB") the number of positives and 
negatives is not set

```{r echo=F, error=F, include=T}

# TODO - the grouping could be displayed in more fancy ways using kableExtra options for grouping
# TODO - consider colouring the predictor names by part before first dot ; unless there are > 10 of those

predSummary <- modelPredictorBins[EntryType != "Classifier", list(Negatives = sum(BinNegatives),
                                                                  Positives = sum(BinPositives),
                                                                  Active = EntryType[1],
                                                                  Type = Type[1],
                                                                  Bins = .N,
                                                                  Performance = Performance[1],
                                                                  Group = GroupIndex[1]), by=PredictorName]
names(predSummary)[1] <- "Predictor"
if (nrow(predSummary) == 0) {
  cat("The model has no predictors", fill=T)
} else {
  predSummary[, maxGroupPerformance := max(Performance), by=Group]
  setorder(predSummary, -maxGroupPerformance, -Performance)
  if (uniqueN(predSummary$Group) > 1) {
    predSummary[, isFirstOfGroup := seq(.N)==1, by=Group]
  } else {
    predSummary[, isFirstOfGroup := T]
  }
  
  kable(predSummary[,-c("maxGroupPerformance", "isFirstOfGroup")]) %>%
    kable_styling() %>%
    add_indent(which(!predSummary$isFirstOfGroup))
}
```


# Binning of the Predictors

Binning of all individual predictors. Predictors are listed in the same order as in the summary above.
Here we show `r ifelse(params$predictordetails_activeonly, "only the active", "all")` predictors. This 
can be configured with one of the parameters in the Yaml header of this notebook.

```{r, results="asis", fig.height = 4, fig.width = 6, fig.align = "center"}

# to print all instead of only the active ones, change condition to: EntryType != "Classifier"

for (f in unique(modelPredictorBins[!params$predictordetails_activeonly | EntryType == "Active"]$PredictorName)) {
  predictorBinning <- modelPredictorBins[PredictorName==f]
  
  if (nrow(predictorBinning[!is.na(Propensity)]) < 2) {
    cat("<p style='color:Red;'>NO data available for", f, "for date:", max(modelPredictorBins$SnapshotTime), "</p>", fill=T)
  } else {
    
    # write.csv(predictorBinning, "tempbin.csv")
    
    # Find other predictors in the same group
    correlatedPreds <- predSummary[Group != 0 & Group == predSummary[Predictor==f]$Group & Predictor != f]
    if (nrow(correlatedPreds) > 0) { 
      extraPredInfo <- list("Correlated Predictors" = paste(sort(correlatedPreds$Predictor), collapse = ", "))
    } else {
      extraPredInfo <- list("Correlated Predictors" = "--")
    }
    
    # Table prelude with some overall info about the predictor
    printADMPredictorInfo(f, predictorBinning, extraPredInfo)
    
    # colour names: http://sape.inf.usi.ch/quick-reference/ggplot2/colour
    
    if (nrow(predictorBinning) > 1) {
      p <- plotBinning(predictorBinning)+ ggtitle(f)
      
      print(p)
    }  
    
    print(kable(userFriendlyADMBinning(predictorBinning), format = "markdown"))
  }
}
```

