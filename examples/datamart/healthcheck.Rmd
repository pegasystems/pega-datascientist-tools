---
title: "`r params$title`"
subtitle: "`r params$subtitle`"
author: "Pega"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
    toc: yes
  pdf_document: default
always_allow_html: true
params:
  # Below default values for the parameters. This notebook is usually launched from a (bash)script in which
  # for the paths to the data.
  
  
  # Number of propositions, models and predictors to put in lists and plots
  # these values are set, although you can also run it from R Studio: Knit with Parameters to be prompted 
  title:
    
    value: "ADM Health Check"
  subtitle:
    
    value: ""
  modelfile:
    # Full path to the source file which should be an export of the ADM model table
    # can be a plain CSV file, a zipped CSV, or the full path of a dataset export
    value: "../../data/pr_data_dm_admmart_mdl_fact.csv"
  predictordatafile:
    # Optional full path to ADM predictor binning table data. If given, the model 
    # overview will also contain predictor plots. Like the model file, there is 
    # different, you'll need to load, filter and re-write as CSV first.
    # flexibility in format: CSV, zipped CSV or dataset export. If you have something
    value: "../../data/pr_data_dm_admmart_pred.csv"
  modellist:
    # Facilitates automatic generation of individual model reports (calling the
    # off-line model report notebook). Optional. When given, is the name of a 
    # text file that will be generated from the model data with a list of 
    # model ID's (and some other info). This file can be iterated
    # over in a batch script to generate individual model reports.
    value: ""
  max_propositions_in_plot: 20
  min_responses_to_show_propositions: 100
  min_positives_for_maturity: 200
  max_models_in_list: 50
  max_predictors_in_plot: 30
editor_options: 
  markdown: 
    wrap: 72
---

```{r Initialization, echo=F, include=FALSE}
library(pdstools)

# include below when developing the library
# sapply(list.files("~/Documents/pega/pega-datascientist-tools/r/R", "*.R", full.names = T), source)

library(data.table)
library(lubridate)
library(ggplot2)
library(plotly)
library(colorspace)
library(scales)
library(knitr)
library(kableExtra)
library(stringi)
# library(arrow) # for parquet read/write

theme_set(theme_light())
options(digits = 5)
knitr::opts_chunk$set(
  comment = ">", echo = FALSE, warning = FALSE, fig.width = 10, fig.height = 10
)
```

# ADM Datamart Health Check

This document gives a generic and global overview of the Adaptive models
and predictors. It is generated from an R notebook in the [Pega Data
Scientist
Tools](https://github.com/pegasystems/pega-datascientist-tools). That
repository of tools and scripts also contains a notebook to generate
stand-alone model reports for individual models, please refer to the
[Wiki](https://github.com/pegasystems/pega-datascientist-tools/wiki).

This document provides a first-level scan of the models after which a
deeper and more customer specific deeper dive can be done.

## Guidance

Where possible, we try to provide guidance and best practices in the
form of bulletted lists of attention points. However these are only
generic practices and may or may not be applicable to the specific use
case and situation of the implementation.

## User experience

Some of the plots provide some interactivity. For best viewing results,
open the HTML document in a browser. Viewing it from platforms like
Sharepoint or Github will loose the interactive charts.

## Output format

Note that the notebook by default generates a single-page HTML, however
you can also export to PDF as well as other formats supported by Pandoc
(e.g. Word) but you would loose interactivity.

```{r Read Model Data, echo=F, include=F}

if (!("modelfile" %in% names(params))) stop("Model data missing. Please provide through parameter 'modelfile'.")
if (!file.exists(params$modelfile)) stop(paste("File does not exist:", params$modelfile))

if (params$predictordatafile == "") {
  datamart <- pdstools::ADMDatamart(params$modelfile, predictordata = F)  
} else {
  if (!is.null(params$predictordatafile)) {
    if (!file.exists(params$predictordatafile)) stop(paste("File does not exist:", params$predictordatafile))  
  }
  
  datamart <- pdstools::ADMDatamart(params$modelfile, params$predictordatafile)
}

# Hmm. Perhaps the 'ADMDatamart' function should be doing this. We need Issue
# in some of the snippets of this health check.
if (!"Issue" %in% names(datamart$modeldata)) {
  datamart$modeldata[["Issue"]] <- "-"    
}
if (!"Group" %in% names(datamart$modeldata)) {
  datamart$modeldata[["Group"]] <- "-"    
}

# standardized coloring for this script
# see also https://www.nceas.ucsb.edu/sites/default/files/2020-04/colorPaletteCheatsheet.pdf
scale_color_Channel <- scale_colour_brewer(limits=levels(datamart$modeldata$Channel), 
                                           name="Channel", palette="Set1", drop=T)
scale_fill_Channel <- scale_fill_brewer(limits=levels(datamart$modeldata$Channel), 
                                        name="Channel", palette="Set1", drop=T)
scale_fill_PredictorCategory <- scale_fill_discrete_qualitative(palette="Dark 3",
                                                                limits=levels(datamart$predictordata$PredictorCategory), 
                                                                name="Predictor Category", drop=T)

# myColors <- brewer.pal(3, "Spectral")
# names(myColors) <- levels(iris$Species)
# custom_colors <- scale_colour_manual(name = "Species Names", values = myColors)
# 
# scale_colour_hue(limits=levels(data$type),drop=TRUE)

# Use qualitative for categorical data. To show the palettes (qualitative in the middle):
# library(RColorBrewer)
# par(mar=c(3,4,2,2))
# display.brewer.all()

aggregateTreeData <- function(dt, hierarchy = intersect(c("Direction", "Channel", "Issue", "Group", "Name"), names(dt)))
{
  treeData <- rbindlist(lapply(0 : length(hierarchy), function(i) {
    if (i == 0) {
      groupingLevels <- "" # data.table supports groupby ""
    } else {
      groupingLevels <- hierarchy[1:i]
    }
    dt[, .(ResponseCount = sum(as.double(ResponseCount)),
           MaxResponseCount = max(as.double(ResponseCount)),
           Positives = sum(Positives),
           SuccessRate = sum(Positives)/sum(ResponseCount),
           Performance = weighted.mean(AUC, ResponseCount, na.rm=T),
           TreeNodeID = paste0("/", paste(lapply(.BY, as.character), collapse="/")),
           TreeLabel = ifelse(i==0, "All Models", as.character(lapply(.BY, as.character)[i])),
           TreeNodeParent = ifelse(i==0, "", 
                                   ifelse(i==1, "/",
                                          paste0("/",
                                                 paste(lapply(.BY, as.character)[1:(i-1)],
                                                       collapse="/"))))), 
       by=eval(groupingLevels)]  
  }), fill = T)
}
```

# Overview of the Channels

```{r Check standard NBAD configurations}
standardNBADModelNames <- c("Assisted_Click_Through_Rate",
                            "CallCenter_Click_Through_Rate",
                            "CallCenterAcceptRateOutbound",
                            "Default_Inbound_Model",
                            "Default_Outbound_Model",
                            "Email_Click_Through_Rate",
                            "Mobile_Click_Through_Rate",
                            "OmniAdaptiveModel",
                            "Other_Inbound_Click_Through_Rate",
                            "Push_Click_Through_Rate",
                            "Retail_Click_Through_Rate",
                            "Retail_Click_Through_Rate_Outbound",
                            "SMS_Click_Through_Rate",
                            "Web_Click_Through_Rate")

configurationNamesInStandardNBADModelNames <- sapply(unique(datamart$modeldata$ConfigurationName), function(x) { x %in% standardNBADModelNames})
```

In a typical NBAD setup, channels are served by both one channel
specific model configuration as well as a cross-channel
*OmniAdaptiveModel* configuration. That *OmniAdaptiveModel* is typically
used only as a fall-back option when the treatment model is not mature
enough.

If a channel has two model configurations with a naming pattern like
"Adm_12345678912", this could indicate the usage of the (no longer
recommended) "2-stage model" predictions for conversion modeling,
generated by Prediction Studio.

The standard Pega Next Best Action Designer framework defines a number
of standard Adaptive Models for channels. By looking at the names of the
configurations it seems that the framework
**`r ifelse(any(configurationNamesInStandardNBADModelNames), ifelse(all(configurationNamesInStandardNBADModelNames), "is being used", "is being used with additional configurations"), "is not being used")`**.

## Guidance

-   Look out for channels supported by more than two model
    configurations, although there may be valid reasons to do so (e.g.
    different sets of predictors for certain issues)
-   Channels with no responses at all
-   Channels with no positive feedback

```{r Channel overview}
channelSummary <- datamart$modeldata[, .(`Responses` = max(ResponseCount), 
                                         `Positives` = max(Positives), 
                                         `Base rate` = sprintf("%.2f%%", 100*max(Positives)/max(ResponseCount)),
                                         `Supported by Configurations` = paste(sort(unique(ConfigurationName)), collapse = ", "),
                                         N = uniqueN(ConfigurationName)), 
                                     by=c("Channel","Direction")][order(Channel)]

channelSummary[, 1:(ncol(channelSummary)-1)] %>%
  kbl() %>%
  kable_paper(c("striped", "hover"), full_width = F) %>%
  column_spec(3, background = ifelse(channelSummary[[3]] > 0 ,"white", "red")) %>%
  column_spec(4, background = ifelse(channelSummary[[4]] > 0 ,"white", "red")) %>%
  column_spec(6, background = ifelse(channelSummary$N <= 2 ,"white", "orange"))
```

# Overview of the Actions

In a standard setup, the offers/conversations are presented as
treatments for actions in a hierarchical structure setup in NBA
Designer. The recommendation is to have multiple treatments for an
action. Treatments are often channel specific and you would typically
expect more unique treatments than there are actions.

Adaptive Models are created per treatment (at least in the default
setup) and the recommendation is to stick the default context keys of
the models.

```{r Overview Helpers}
nUniqueValues <- function(dm, fld) { 
  values <- as.vector(na.omit(unique(dm$modeldata[[fld]])))
  length(values)
}
nUniqueChannelDirections <- function(dm, fld) { uniqueN(paste(dm$modeldata$Direction, 
                                                              dm$modeldata$Channel, sep="/")) }
allUniqueValues <- function(dm, fld, max = .Machine$integer.max) { 
  values <- as.vector(na.omit(unique(dm$modeldata[[fld]])))
  
  if (length(values) < 1) {
    return("-")
  } else {
    if (length(values) > max) {
      return(paste(c(head(sort(values), max), "..."), collapse=", ")) 
    } else {
      return(paste(sort(values), collapse=", ")) 
    }
  }
}
allUniqueChannelDirectionValues <- function(dm) { 
  paste(sort(unique(paste(dm$modeldata$Direction, 
                          dm$modeldata$Channel, sep="/"))), 
        collapse=", ") }

avgGroupsPerIssue <- function(dm) { 
  round(mean(dm$modeldata[, .(Groups = uniqueN(Group)), by="Issue"]$Groups), 2)
}

actionsOverview <- data.table(
  Item = c("Overall Number of Actions", 
           "Max number of Actions within an Issue and Group",
           "Number of Treatments across all Actions",
           "Max number of Treatments per Channel",
           "Max number of Treatments for any single Action",
           "Number of unique Issues", 
           "Average number of Groups per Issue", 
           "Max number of Groups per Issue",
           "Channels"),
  Number = c(as.character(nUniqueValues(datamart, "Name")),
             max(datamart$modeldata[, .(N = uniqueN(Name)), by=intersect(c("Channel","Direction","Issue","Group"), 
                                                                         names(datamart$modeldata))]$N, na.rm = T),
             nUniqueValues(datamart, "Treatment"),
             max(datamart$modeldata[, .(N = uniqueN(.SD[["Treatment"]])), by=c("Channel","Direction")]$N, na.rm = T),
             max(datamart$modeldata[, .(N = uniqueN(.SD[["Treatment"]])), by=intersect(c("Direction", "Channel", "Issue", "Group", "Name"), 
                                                                              names(datamart$modeldata))]$N, na.rm = T),
             nUniqueValues(datamart, "Issue"),
             avgGroupsPerIssue(datamart),
             max(datamart$modeldata[, .(N = uniqueN(.SD[["Group"]])), by=intersect(c("Channel","Direction","Issue"),
                                                                          names(datamart$modeldata))]$N, na.rm = T),
             nUniqueChannelDirections(datamart)),
  `(Example) Values` = c(allUniqueValues(datamart, "Name", 10),
                         "-",
                         allUniqueValues(datamart, "Treatment", 10),
                         
                         "-",
                         "-",
                         allUniqueValues(datamart, "Issue"),
                         allUniqueValues(datamart, "Group"),
                         "-",
                         allUniqueChannelDirectionValues(datamart)),
  
  # from service limits
  
  LimitLo = c(1000, 100, 2500, 1000, 5,  5, -Inf,  5, -Inf),
  LimitHi = c(2500, 250, 5000, 2500, 5, 25, +Inf, 25, +Inf)
)

actionsOverview[,1:3] %>% 
  kbl() %>% kable_paper(c("striped", "hover"), full_width = T) %>%
  column_spec(2, background = ifelse(as.numeric(actionsOverview$Number) >= actionsOverview$LimitLo &
                                       as.numeric(actionsOverview$Number) <= actionsOverview$LimitHi,"white", "orange"))
```

### Guidance

-   Recommended best practice is to have multiple treatments for an
    action. Too few gives less opportunity for personalization of the
    interactions.
-   Pega Customer Decision Hub deployed in Pega Cloud has limits in
    place that constrain certain elements of the service and its use to
    ensure high quality of service for your team. Where relevant, these
    limits are used in the guidance in this health check. See [Service
    and data health limits for Pega Customer Decision Hub on Pega
    Cloud](https://docs-previous.pega.com/pega-customer-decision-hub-user-guide/87/service-and-data-health-limits-pega-customer-decision-hub-pega-cloud).

## Success Rates per Channel

Showing the current success rate of the treatments. Different channels
usually have very different success rates. Just showing the top
`r params$max_propositions_in_plot` here and limiting to the
propositions that have received at least
`r params$min_responses_to_show_propositions` responses (the rates
reported by the models are unreliable otherwise).

### Guidance

-   Look out for propositions that stand out, having a far higher
    success rate than the rest. Check with business if that is expected.

-   Variation in the set of offered propositions across customers is
    also an important metric but not one that can be derived from the
    Adaptive Model data - this requires analysis of the actual
    interactions.

```{r Plot Action Success Rates}
if (nrow(filterLatestSnapshotOnly(datamart$modeldata)[ResponseCount >= as.integer(params$min_responses_to_show_propositions)]) > 0) {
  p <- plotPropositionSuccessRates(datamart, 
                                   filter=function(mdls){ filterLatestSnapshotOnly(mdls)[ResponseCount >= as.integer(params$min_responses_to_show_propositions)] }, 
                                   limit=params$max_propositions_in_plot, 
                                   facets=c("Direction", "Channel")) + 
    scale_fill_continuous_divergingx()
} else {
  p <- plotPropositionSuccessRates(datamart, 
                                   filter=function(mdls){ filterLatestSnapshotOnly(mdls) }, 
                                   limit=params$max_propositions_in_plot, 
                                   facets=c("Direction", "Channel")) + 
    scale_fill_continuous_divergingx()
}

# ggplotly(p) %>% layout(showlegend=FALSE) # plotly no use here given the way the plot is constructed
p
```

## All Success Rates (interactive chart)

Interactive chart with all success rates. Whiter is lower success rates,
green is higher. Indicated values are percentages.

```{r Success rates interactive}
treeData <- aggregateTreeData(filterLatestSnapshotOnly(datamart$modeldata))

fig <- plot_ly(
  type="treemap",
  ids=treeData$TreeNodeID,
  labels=treeData$TreeLabel,
  parents=treeData$TreeNodeParent,
  values=round(100*treeData$SuccessRate,3),
  marker=list(colorscale='Greens', reversescale = T),
  textinfo="label+value"
)
fig %>% layout(title = "Success Rate")
```

## Success Rates over Time

Showing how the overall channel success rates evolved over the time that
the data export covers. Split by Channel and model configuration.
Usually there are separate model configurations for different channels
but sometimes there are also additional model configurations for
different outcomes (e.g. conversion) or different customers (e.g.
anonymous).

### Guidance

-   There shouldn't be too sudden changes over time

```{r Success Rate over Time, message=FALSE, warning=FALSE}
if (!hasMultipleSnapshots(datamart$modeldata))
{
  cat("Trend plots will only be available when the model data contains multiple snapshots.", fill=T)
} else {
  if (max(datamart$modeldata$ResponseCount) < 100) {
    p <- plotSuccessRateOverTime(datamart, 
                                 aggregation = c("Channel"),
                                 facets = c("Direction", "ConfigurationName")) + 
      scale_color_Channel
  } else {
    p <- plotSuccessRateOverTime(datamart, 
                                 filter=function(mdls){ mdls[ResponseCount >= 100] }, 
                                 aggregation = c("Channel"),
                                 facets = c("Direction", "ConfigurationName")) + 
      scale_color_Channel
    #geom_smooth(aes(fill=Channel), alpha=0.1)
  }
  p
}
```

# Overview of Adaptive Models

There are a total of
**`r nrow(filterLatestSnapshotOnly(datamart$modeldata))` models** in the
latest snapshot.

The standard configuration is to have one model per treatment.

```{r Overview adaptive}
# Name and Treatment will be separate columns when read properly via ADMDatamart
groupingCols <- intersect(c("ConfigurationName", "Direction", "Channel"), names(datamart$modeldata))

actionsOverview <- filterLatestSnapshotOnly(datamart$modeldata)[, .(Treatments = .N), 
                                                                by=c(groupingCols, "Name")]

actionsOverviewSummary <- actionsOverview[, .(`Standard in NBAD Framework` = first(ConfigurationName) %in% standardNBADModelNames,
                                              `Number of Actions` = .N, 
                                              `Number of Models` = sum(Treatments), 
                                              `Average number of Treatments per Action` = mean(Treatments)), 
                                          by=groupingCols][order(Direction, Channel, ConfigurationName)]

actionsOverviewSummary %>%
  kbl() %>%
  kable_paper(c("striped", "hover"), full_width = F) %>%
  column_spec(4, background = ifelse(actionsOverviewSummary[[4]],"white", "orange")) %>%
  column_spec(7, background = ifelse(actionsOverviewSummary[[7]] >= 2,"white", "orange"))
```

## Model Performance

### Model Performance vs Action Success Rates (aka the Bubble Chart)

This "Bubble Chart" - similar to the standard plot in the ADM reports in
Pega - shows the relation between model performance and proposition
success rates. In addition, the size of the bubbles indicates the number
of responses.

#### Guidance

-   Bubbles stacked up against the left-hand vertical axis represent
    actions/treatments for which the models are not predictive. These
    models may be still be ramping up, or they may not have predictive
    enough features to work with: consider if new/better predictors can
    be added.

-   Charts should not be empty or contain only a few bubbles. Such
    charts may represent channels or configurations not (or no longer).

-   Bubbles at the bottom of the charts represent propositions with very
    low success rates - they may not be compelling enough.

-   In an ideal scenario you will see the larger bubbles more on the
    top-right, so more volume for propositions with higher success rates
    and better models.

-   There should - roughly - be a positive correlation between success
    rate and performance per channel.

-   There should be a positive correlation between responses and
    performance - also per channel.

-   There should be variation in response counts (not all dots of equal
    size)

-   For small volumes of good models, see if the engagement rules in the
    Decision Strategy are overly restrictive or reconsider the
    arbitration of the propositions so they get more (or less) exposure.

```{r Bubble Chart, message=FALSE, warning=FALSE}

## todo consider if we can color like we used to do in the static versions
## with something like
## OhMyGoodness := (pdstools::auc2GINI(Performance) + pmin(Positives, as.integer(params$min_positives_for_maturity)

if (nrow(filterLatestSnapshotOnly(datamart$modeldata)[ResponseCount >= 100]) > 0) {
  plt <- plotPerformanceSuccessRateBubbleChart(datamart, 
                                               filter=function(mdls){ filterLatestSnapshotOnly(mdls)[ResponseCount >= 100] }, 
                                               facets = c("ConfigurationName", "Direction", "Channel"))
} else {
  plt <- plotPerformanceSuccessRateBubbleChart(datamart, 
                                               filter=function(mdls){ filterLatestSnapshotOnly(mdls) }, 
                                               facets = c("ConfigurationName", "Direction", "Channel"))
}

ggplotly(plt) %>% 
  layout(showlegend=F) 
```

### Model Performance over Time

Showing how the model performance evolves over time. Note that ADM is by
default configured to track performance over all time. You can configure
a window for monitoring but this is not commonly done.

Aggregating up to Channel and splitting by model configuration.

#### Guidance

-   No abrupt changes but gradual upward trend is good

```{r Performance over Time, message=FALSE, warning=FALSE}
if (!hasMultipleSnapshots(datamart$modeldata))
{
  cat("Trend plots will only be available when the model data contains multiple snapshots.", fill=T)
} else {
  p <- plotPerformanceOverTime(datamart, aggregation = "Channel", 
                               facets = c("Direction", "ConfigurationName")) +
    scale_color_Channel  
  #geom_smooth(aes(fill=Channel), alpha=0.1)
  p
  # ggplotly(p)
}
```

### Model performance of all the actions

Using an interactive treemap to visualize the performance. Lighter is
better, darker is low performance.

It can be interesting to see which issues, groups or channels can be
better predicted than others. Identifying categories of items for which
the predictions are poor can help to drive the search for better
predictors, for example.

```{r Treemap action performance}
treeData <- aggregateTreeData(filterLatestSnapshotOnly(datamart$modeldata))

fig <- plot_ly(
  type="treemap",
  ids=treeData$TreeNodeID,
  labels=treeData$TreeLabel,
  parents=treeData$TreeNodeParent,
  values=round(treeData$Performance,2),
  # see https://plotly.com/r/builtin-colorscales/
  # https://plotly.com/r/reference/#heatmap-colorscale
  # https://plotly.com/r/treemaps/
  # https://plotly.com/r/colorscales/
  marker=list(colorscale="Viridis", reversescale = F),
  textinfo="label+value"
)
fig %>% layout(title = "Model Performance")
```

## Response counts for all the actions

Using an interactive treemap to visualize the response counts. Darker is
more.

Different channels will have very different numbers but within one
channel the relative differences in response counts give an indication
how skewed the distribution is.

### Guidance

-   If there are actions that have a much higher response count than the
    rest see why that is. Possibly they are levered up for valid
    business reasons.

```{r Treemap response count}
fig <- plot_ly(
  type="treemap",
  ids=treeData$TreeNodeID,
  labels=treeData$TreeLabel,
  parents=treeData$TreeNodeParent,
  values=treeData$ResponseCount,
  # see https://plotly.com/r/builtin-colorscales/
  # https://plotly.com/r/reference/#heatmap-colorscale
  # https://plotly.com/r/treemaps/
  # https://plotly.com/r/colorscales/
  marker=list(colorscale="Viridis", reversescale = T),
  textinfo="label+value"
)
fig %>% layout(title = "Response Count")
```

# Analysis of Predictors

This analysis focuses on finding which are top predictors that are
driving the models.

The predictors are categorized (by color) by the "source". By default
this takes just the first part before the dot, so this typically
distinguishes between Customer, Account, IH and parameterized (Param.)
predictors. You can customize this to add patterns to identify for
example external scores.

## Number of Predictors per model configuration

Both active and inactive predictors.

Note that the total number of predictors in the model data does not
always equate the data from the more detailed view split by category
below.

```{r}
predictorsPerModelConfiguration <- filterLatestSnapshotOnly(datamart$modeldata)[, .(Predictors = max(TotalPredictors), 
                                                 `Used In (Channels)` = paste(sort(unique(paste(Direction, Channel, sep="/"))), collapse=", "), 
                                                 `Used For (Issues)` = paste(sort(unique(Issue)), collapse=", ")), 
                                             by=c("ConfigurationName")][order(ConfigurationName)] 

if (all(is.na(predictorsPerModelConfiguration$Predictors)))
{
  cat("No predictor information present in the model data.", fill=T)
} else {
  predictorsPerModelConfiguration %>%
    kbl() %>%
    kable_paper(c("striped", "hover"), full_width = F) %>%
    column_spec(2, background = ifelse(predictorsPerModelConfiguration[[2]] >= 200 & predictorsPerModelConfiguration[[2]] <= 700,"white", "orange"))
}
```

## Number of Predictors per Predictor Category

Split by category (defaults to the string before the first dot, can be
overridden when reading the data).

The numbers here can differ from the totals above, these ones are
leading.

```{r Predictor Type Counts}
if (!is.null(datamart$predictordata) && nrow(datamart$predictordata) > 0) {
  combined <- merge(filterClassifierOnly(filterLatestSnapshotOnly(datamart$predictordata), reverse = T), 
                    filterLatestSnapshotOnly(datamart$modeldata)[, .N, by=c("ModelID", "ConfigurationName")], by="ModelID")
  counts <- combined[, .(N = uniqueN(PredictorName)), keyby=c("ConfigurationName", "PredictorCategory")]
  
  predictorTypeCounts <- dcast(counts, ConfigurationName ~ PredictorCategory, value.var="N", fill = 0)
  predictorTypeCounts[, Total := rowSums(.SD[,2:ncol(predictorTypeCounts)])]
  
  tbl <-  predictorTypeCounts %>%
    kbl() %>%
    kable_paper(c("striped", "hover"), full_width = F)

  if ("IH" %in% names(predictorTypeCounts)) {
    colIdx <- which(names(predictorTypeCounts) == "IH")[1]
    tbl <- tbl %>% column_spec(colIdx, background = ifelse(predictorTypeCounts[[colIdx]] <= 100,"white", "orange"))    
  }
  
  if ("Param" %in% names(predictorTypeCounts)) {
    colIdx <- which(names(predictorTypeCounts) == "Param")[1]
    tbl <- tbl %>% column_spec(colIdx, background = ifelse(predictorTypeCounts[[colIdx]] <= 24,"white", "orange"))    
  }
  
  tbl <- tbl %>% 
    column_spec(ncol(predictorTypeCounts), background = ifelse(predictorTypeCounts$Total >= 200 & predictorTypeCounts$Total <= 700,"white", "orange"))
  
  # for (i in 2:ncol(predictorTypeCounts)) {
  #   tbl <- tbl %>% column_spec(i, background = ifelse(predictorTypeCounts[[i]] >= 200 & predictorTypeCounts[[i]] <= 700,"white", "orange"))  
  # }
  
  tbl
} else {
  cat("Predictor analysis not available.", fill=T)
}
```

### Guidance

-   Total number of predictors per model 200 - 700 to stay within
    service limits
-   There should be some "IH" predictors but no more than ca 100 of them
-   No more than a few dozen Param predictors
-   Consistency in the numbers across configurations

## Predictor Importance across all models per configuration

Box plots of the predictor importance. Importance can be shown either as
a global feature importance or simply as the univariate predictor
importance, here we use univariate performance measured in AUC (Area
under the ROC curve) but the PDS tools functions also support using
Feature Importance.

### Guidance

-   You expect most predictors to have a spread in the performance
    range, doing better for some actions than for others
-   Predictors only showing as a single bar (no range) are suspicious
-   A variation of predictors from different categories in the top
    `r params$max_predictors_in_plot`
-   A min/max of the univariate AUC performance somewhere between 55 and
    75

```{r Univariate Predictors Boxplot}
if (!is.null(datamart$predictordata) && nrow(datamart$predictordata) > 0) {
  for (c in levels(datamart$modeldata$ConfigurationName)) {
    # applicableModels <- filterLatestSnapshotOnly(datamart$modeldata) [ConfigurationName==c]
    # applicablePredictorBins <- modelPredictorBins[ModelID %in% applicableModels$ModelID & EntryType=="Active"]
    
    plt <- plotPredictorImportance(datamart,
                                   limit = params$max_predictors_in_plot, 
                                   showAsImportance = F,
                                   filter = function(mdls) {
                                     filterLatestSnapshotOnly(mdls) [ConfigurationName==c]
                                   })
    
    if (is.null(plt)) {
      warning("Empty plot for configuration", c)
    } else {
      plt <- plt + ggtitle("Predictor Performance", 
                           subtitle = paste0("(Top ", params$max_predictors_in_plot, " from ", uniqueN(filterClassifierOnly(datamart$predictordata[ModelID %in% unique(datamart$modeldata[ConfigurationName == c]$ModelID)], reverse=T)$PredictorName), " in ", c, ")")) +
        scale_fill_PredictorCategory
      print(plt)
      # ggplotly(plt)
    }
  }
} else {
  cat("Predictor analysis not available.", fill=T)
}
```

## Importance by Predictor Category

Aggregating up to the category of the predictors. This gives a view at a
glance of how well e.g. interaction history, external model scores or
contextual data are doing overall.

### Predictor Category performance per Channel

```{r Univariate Predictor Category Boxplot}
if (!is.null(datamart$predictordata) && nrow(datamart$predictordata) > 0) {
  plt <- plotPredictorImportance(datamart,  
                                 categoryAggregateView = T,
                                 facets = c("ConfigurationName", "Direction", "Channel"),
                                 showAsImportance = F,
                                 filter = function(mdls) {filterLatestSnapshotOnly(mdls)} ) +
    scale_fill_PredictorCategory
  print(plt)
} else {
  cat("Predictor analysis not available.", fill=T)
}
```

### Relative Predictor Category importance per Configuration

Although the same could be achieved using the standard
`plotPredictorImportance` method, now that we only split by
Configuration this allows for a more compact visualization using a
stacked bar chart.

```{r Predictor categories by Config only, fig.height=5, fig.width=8}
if (!is.null(datamart$predictordata) && nrow(datamart$predictordata) > 0) {
  predCategoryOverview <- merge(filterActiveOnly(datamart$predictordata), 
                                unique(datamart$modeldata[,c("ModelID", "ConfigurationName")]), 
                                by="ModelID", all.x=F, all.y=F)[,.(Count = .N, Performance = sum(Performance-0.5)), by=c("PredictorCategory","ConfigurationName")]
  # weighted.mean(Performance,ResponseCount)
  plt <- ggplot(predCategoryOverview,
                aes(ConfigurationName, Performance, fill=PredictorCategory)) +
    geom_col(position = position_fill()) +
    scale_y_continuous(labels = scales::percent_format()) +
    theme(axis.text.x = element_text(angle = 45, hjust=1)) +
    scale_fill_PredictorCategory +
    coord_flip() + theme(legend.position="top") +
    ggtitle("Contribution of different sources") +
    xlab("Model")
  
  ggplotly(plt)
} else {
  cat("Predictor analysis not available.", fill=T)
}
```

## Bad predictors across all models

See if there are predictors that are just always perform poorly.

### Guidance

-   Predictors that consistently perform poorly could potentially be
    removed.
-   Be sure to check for data problems
-   Note we advise to be careful with predictor removal. Only remove if
    there is clearly no future value to other propositions as well or if
    there is always a related predictor that performs better.

### TODO different viz

-   Provide as a (scrollable) list instead of plot (predictor, usage,
    performance range etc)

```{r Bad predictors}
if (!is.null(datamart$predictordata) && nrow(datamart$predictordata) > 0) {
  plt <- plotPredictorImportance(datamart, facets=NULL, 
                                 limit = -params$max_predictors_in_plot,
                                 showAsImportance = F,
                                 filter = function(mdls) {filterLatestSnapshotOnly(mdls)})
  
  plt <- plt + ggtitle("Predictors with Low Performance", 
                       subtitle = paste0("(Bottom ", params$max_predictors_in_plot, " from total ", uniqueN(filterClassifierOnly(datamart$predictordata, reverse=T)$PredictorName), ")")) +
    scale_fill_PredictorCategory
  print(plt)
} else {
  cat("Predictor analysis not available.", fill=T)
}
```

## Number of Active and Inactive Predictors

Showing the number of active and inactive predictors per model.

### Guidance

-   We expect a few dozen active predictors for every model instance

```{r Active and Inactive Predictors, fig.height=5, fig.width=8}
if (!is.null(datamart$predictordata) && nrow(datamart$predictordata) > 0) {
  typeDistributionPlotData <- 
    merge(unique(filterLatestSnapshotOnly(datamart$predictordata)[EntryType!="Classifier", 
                                                                  c("ModelID", "PredictorName", "EntryType", "Type")]) [, .N, by=c("EntryType", "Type", "ModelID")], 
          unique(filterLatestSnapshotOnly(datamart$modeldata)[, c("ModelID", "ConfigurationName")]), by="ModelID")
  
  # add entries for counts of numeric and symbolic together
  typeDistributionPlotData <- rbindlist(list(typeDistributionPlotData, typeDistributionPlotData[,.(N = sum(N)), by=c("ModelID", "ConfigurationName", "EntryType")][, Type := "overall"]), use.names = T)
  
  plt <-
    ggplot(typeDistributionPlotData, aes(N, Type, color=Type, linetype=EntryType)) + 
    geom_boxplot() +
    scale_color_discrete_qualitative(name="Predictor Type") +
    scale_linetype_discrete(name="Status") +
    ylab("")+ xlab("") + 
    facet_wrap(. ~ ConfigurationName, strip.position = "right") +
    # theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    ggtitle("Number of Predictors per Model", subtitle = "by Type, Status and per model configuration")
  
  plt
} else {
  cat("Predictor analysis not available.", fill=T)
}
```

## Predictor Performance across Propositions

A view of predictor performance across all propositions, ordered so that
the best performing predictors are at the top and the best performing
propositions are on the left. Green indicates good performance, red
means more problematic - either too low or too good to be true.

```{r Predictors vs Propositions}
if (!is.null(datamart$predictordata) && nrow(datamart$predictordata) > 0) {
  for (c in levels(datamart$modeldata$ConfigurationName)) {
    # applicableModels <- filterLatestSnapshotOnly(mdls) [ConfigurationName==c]
    # applicablePredictorBins <- modelPredictorBins[EntryType=="Active" & ModelID %in% applicableModels$ModelID]
    datamart$predictordata[, ResponseCount := as.numeric(ResponseCount)] # temp hack
    plt <- plotPredictorImportanceHeatmap(datamart, 
                                          limit=params$max_propositions_in_plot,
                                          filter = function(mdls) {
                                            filterLatestSnapshotOnly(mdls) [ConfigurationName==c]
                                          } ) +
      theme(axis.text.y = element_text(size=8),
            axis.text.x = element_text(size=8, angle = 45, hjust = 1),
            strip.text = element_text(size=8))
    if (is.null(plt)) {
      warning("Empty plot for configuration", c)
    } else {
      print(plt)
    }
  }
} else {
  cat("Predictor analysis not available.", fill=T)
}
```

## Missing values

If a predictor is low performing: are there too many missing values?
This could point to a technical problem

Missing % is number of missing vs all responses, really just a filter on
model data

This (currently) only shows the fields that have any missing values

```{r missing values}
if (!is.null(datamart$predictordata) && nrow(datamart$predictordata) > 0) {
  missingData <- filterLatestSnapshotOnly(datamart$predictordata) [, .(ResponseCount = max(as.numeric(ResponseCount)), 
                                                                       Missing = max(c(0,BinResponseCount[BinType=="MISSING"]))), 
                                                                   by=c("PredictorName", "PredictorCategory", "ModelID", "Type")]
  
  missingData[, MissingPct := Missing/ResponseCount]
  missingData <- merge(missingData, unique(datamart$modeldata[, c("ModelID", "ConfigurationName")]))
  
  head(missingData)
} else {
  cat("Predictor analysis not available.", fill=T)
}
```

```{r Treemap missing values}
if (!is.null(datamart$predictordata) && nrow(datamart$predictordata) > 0) {
  hierarchy <- c("ConfigurationName", "PredictorCategory", "PredictorName")
  mainLabel <- "Missing data in Adaptive Models"
  
  missingDataTree <- rbindlist(lapply(seq(length(hierarchy)), function(i) {
    missingData[, .(ResponseCount = sum(ResponseCount),
                    Missing = sum(Missing),
                    MissingPct = sum(Missing) / sum(ResponseCount),
                    TreeNodeID = paste(lapply(.BY, as.character), collapse="/"),
                    TreeLabel = as.character(lapply(.BY, as.character)[i]),
                    TreeNodeParent = ifelse(i==1, mainLabel, paste(lapply(.BY, as.character)[1:(i-1)], collapse="/"))), 
                by=eval(hierarchy[1:i])]  
  }), fill = T)
  
  
  fig <- plot_ly(
    type="treemap",
    ids=missingDataTree$TreeNodeID,
    labels=missingDataTree$TreeLabel,
    parents=missingDataTree$TreeNodeParent,
    values=ifelse(is.na(missingDataTree$MissingPct), 0, round(missingDataTree$MissingPct*100,2)),
    marker=list(colorscale='YlOrRd', reversescale = T),
    textinfo="label+value"
  )
  fig # %>% layout(uniformtext=list(minsize=6, mode='hide'))
} else {
  cat("Predictor analysis not available.", fill=T)
}
```

# Responses

In the sections below we check which of these models have reached
certain reliability (or "maturity") thresholds. This is based on
heuristics on both the number of positives (\>
`r params$min_positives_for_maturity` considered mature) and
performance.

## Empty and Immature Models

### Guidance

All below lists are guidance. There should be just a small percentage of
immature or empty models overall. Having no or just 1 active predictor
is very suspicious

```{r CategorizeModels}
models4MaturityChecks <- copy(datamart$modeldata)

models4MaturityChecks[, isEmpty := ResponseCount == 0]
models4MaturityChecks[, noPositives := ResponseCount > 0 & Positives == 0]
models4MaturityChecks[, isImmature := Positives > 0 & Positives < as.integer(params$min_positives_for_maturity)]
models4MaturityChecks[, noPerformance := Positives >= as.integer(params$min_positives_for_maturity) & Performance == 0.5]

availableStandardContextKeys <- intersect(c("ConfigurationName", "Issue","Group","Name","Channel","Direction","Treatment"), names(models4MaturityChecks))
fieldsForModelAnalysis <- c(availableStandardContextKeys, c("ResponseCount", "Positives", "Negatives", "Performance"))

setorderv(models4MaturityChecks, availableStandardContextKeys)

modelsNeverUsed <- filterLatestSnapshotOnly(models4MaturityChecks)[(isEmpty), fieldsForModelAnalysis, with=F]
modelsNoPositives <- filterLatestSnapshotOnly(models4MaturityChecks)[(noPositives), fieldsForModelAnalysis, with=F]
modelsImmature <- filterLatestSnapshotOnly(models4MaturityChecks)[(isImmature), fieldsForModelAnalysis, with=F]
modelsNoPerformance <- filterLatestSnapshotOnly(models4MaturityChecks)[(noPerformance), fieldsForModelAnalysis, with=F]
```

### Models that have never been used

These models have no responses at all: no positives but also no
negatives. The models for these actions/treatments exist, so they must
have been created in the evaluation of the actions/treatments, but they
were never selected to show to the customer, so never received any
responses.

Often these represent actions that never made it into production and
were only used to test out logic. But it could also be that the response
mechanism is broken. It could for example be caused by outcome labels
that are returned by the channel application not matching the
configuration of the adaptive models.

#### TODO: consider creating an Excel file with these and all subsequent lists

Showing first `r min(nrow(modelsNeverUsed), params$max_models_in_list)`
from total of `r nrow(modelsNeverUsed)`.

```{r Never used models}
modelsNeverUsed %>% 
  head(params$max_models_in_list) %>% 
  kbl() %>%
  kable_paper(c("striped", "hover"), full_width = F) %>% 
  scroll_box(height = "300px")
```

### Models that have have been used but never received a positive response

These models have been used but never received a "positive" response.
This means the action/treatments that these models represent have been
selected, so an "impression" has been made to the customer, but they
never received a "positive" response.

This could be because no customer found the proposition attractive, but
it could also be because the response loop is not working correctly.
Possibly Pega "believes" the user received the action but the actual
channel application did never show it.

Showing first
`r min(nrow(modelsNoPositives), params$max_models_in_list)` from total
of `r nrow(modelsNoPositives)`.

```{r No positives models}
modelsNoPositives %>% 
  head(params$max_models_in_list) %>% 
  kbl() %>%
  kable_paper(c("striped", "hover"), full_width = F) %>% 
  scroll_box(height = "300px")
```

### Models that are still in an immature phase of learning

These models have received at least one positive response but not enough
yet to be qualified to be fully "mature" - a concept that matters
especially for outbound channels.

These actions are typically new and still in early phases of learning.

We show the "reach" of these actions as the percentage of the population
that would be selected by the standard maturity capping algorithm in the
NBA framework (which selects 2% for new models and 100% for models with
`r params$min_positives_for_maturity` or more positive responses).

Showing first `r min(nrow(modelsImmature), params$max_models_in_list)`
from total of `r nrow(modelsImmature)`.

```{r Immature models}
modelsImmature[, -"ResponseCount"][, Reach := sprintf("%.2f%%", 100.0*(0.02+0.98*pmin(as.integer(params$min_positives_for_maturity), Positives)/as.integer(params$min_positives_for_maturity)))] %>% 
  head(params$max_models_in_list) %>% 
  kbl() %>%
  kable_paper(c("striped", "hover"), full_width = T) %>% 
  scroll_box(height = "300px")
```

### Models that have received sufficient responses but are still at their minimum performance

These models also have received over
`r params$min_positives_for_maturity` positives but still show the
minimum model performance.

This could be an indication of data problems, or not having the right
predictors but may also be caused by technical aspects like the order of
the responses to the model.

Showing first
`r min(nrow(modelsNoPerformance), params$max_models_in_list)` from total
of `r nrow(modelsNoPerformance)`.

```{r No performance models}
modelsNoPerformance %>% 
  head(params$max_models_in_list) %>% 
  kbl() %>%
  kable_paper(c("striped", "hover"), full_width = F) %>% 
  scroll_box(height = "300px")
```

### Number of Empty/Immature Models over time

In the analysis below we count the number of models in each of the
groups analysed before and show how that count changes over time. The
expectation is that the number of "non-empty" models increases steadily
and the other lines are more or less stable.

Empty is defined as having no responses at all. Immature is defined as
having \< `r params$min_positives_for_maturity` positives, and no
performance means model performance is still the initial 0.5 value while
having matured already according to the definition.

### Guidance

-   Empty models shouldnt be increasing too much
-   Good models (AUC 55-80) should increase or at least not decrease
-   Good models should be much higher than problem kids

```{r Number of Empty or Immature models over time, message=FALSE, warning=FALSE}
if (!hasMultipleSnapshots(datamart$modeldata))
{
  cat("Trend plots will only be available when the model data contains multiple snapshots.", fill=T)
} else {
  
  immatureModelsByChannel <- models4MaturityChecks[, list(`Empty Models` = length(unique(ModelID[(isEmpty)])), 
                                                          `Models w/o Positives` = length(unique(ModelID[(noPositives)])), 
                                                          `Immature Models` = length(unique(ModelID[(isImmature)])), 
                                                          `Models w/o Performance` = length(unique(ModelID[(noPerformance)])),
                                                          `Number of non-empty Models` = length(unique(ModelID[(!isEmpty)]))), 
                                                   by=c("Direction", "Channel", "SnapshotTime")]
  
  ggplot(melt(immatureModelsByChannel, c("Direction", "Channel", "SnapshotTime"), 
              variable.name="Count", value.name="N"),
         aes(SnapshotTime, N, color=Count)) + 
    geom_line(size=1) + xlab("") + ylab("Count") +
    scale_color_discrete_qualitative() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    facet_wrap(. ~ Direction+Channel) +
    ggtitle("Immature and Empty Models", subtitle = "by Channel")
}
```

## Number of Responses over time

Showing the response rate over time. You would expect this to be more or
less constant, only changing if there is a sudden change in targeted
population.

Big spikes and very noisy behavior may be caused by changes in the
setup.

```{r Responses over time}
datamart$modeldata[["SnapshotTimeDay"]] <- ceiling_date(datamart$modeldata$SnapshotTime, unit="days")

responseRates <- datamart$modeldata[, .(ResponseCount = sum(ResponseCount)), 
                                    by=c("SnapshotTimeDay", "Direction", "Channel", "ConfigurationName")][order(SnapshotTimeDay)]

responseRates[, PrevResponseCount := shift(ResponseCount), by=c("Channel", "ConfigurationName")]
responseRates[, DeltaResponseCount := ResponseCount - PrevResponseCount]
responseRates[, ResponseRate := DeltaResponseCount/(24*60)] # per minute

if (uniqueN(responseRates$SnapshotTimeDay) <= 1)
{
  cat("Response rate analysis only available when there are multiple days of snapshots.", fill=T)
} else if (nrow(responseRates[!is.na(ResponseRate) & (ResponseRate > 0)]) == 0) {
  cat("No changes in response counts in provided data.", fill=T)  
} else {
  ggplot(responseRates[!is.na(ResponseRate) & (ResponseRate > 0)], aes(SnapshotTimeDay, ResponseRate, colour=Channel)) +
    geom_line() +
    scale_color_Channel +
    facet_wrap(. ~ Direction+ConfigurationName, scales="free_y") +
    ggtitle("Responses per minute") +
    xlab("Date") + ylab("Response Rate")
}
```

# Which Models drive most of the Volume

## Analysis of skewness of the Responses

Showing the cumulative response count vs the number of models. Is there
a larger percentage of models that take the vast majority of the
responses?

If this line strongly deviates from the diagonal it means that
relatively few models drive the majority of the responses.

In the left-hand plot we look at all responses, which really means that
we are looking at "impressions" mostly. The right-hand plot looks at
just the positives. Typically, the positives are driven more strongly by
the models so often you see more skewness in that one.

However very skewed results may be caused by prioritization elements
like levers and weights and can be a reason to check in with business
and verify that this is expected.

### Guidance

-   Area under this curve should be \> 0.5 and perhaps more towards 1 -
    most of the responses driven by relatively few actions

```{r Response gain chart, fig.height=3, fig.width=8}
responseGainData <- 
  filterLatestSnapshotOnly(datamart$modeldata) [, list(Responses = max(as.double(ResponseCount)), Positives = max(Positives)), 
                                                by=c("Channel", "ModelID")]
responseGainData <- rbindlist(list(copy(responseGainData)[, Label := "All responses"],
                                   copy(responseGainData)[, Label := "Just the Positive responses"]))


setorder(responseGainData, Channel, -Responses)
responseGainData[Label == "All responses", TotalResponseFraction := cumsum(Responses) / sum(Responses) , by=Channel]
responseGainData[Label == "All responses", TotalModelsFraction := seq(.N)/.N , by=Channel]
setorder(responseGainData, Channel, -Positives)
responseGainData[Label == "Just the Positive responses", TotalResponseFraction := cumsum(Positives) / sum(Positives) , by=Channel]
responseGainData[Label == "Just the Positive responses", TotalModelsFraction := seq(.N)/.N , by=Channel]

# make it start at 0,0
responseGainData <- rbindlist(list(responseGainData, 
                                   responseGainData[, list(TotalResponseFraction = 0, TotalModelsFraction = 0, Responses = 0, Positives = 0), 
                                                    by=c("Channel", "ModelID", "Label")]), use.names = T)

plt <- ggplot(responseGainData, aes(TotalModelsFraction, TotalResponseFraction, color=Channel)) + 
  geom_line(size=1) +
  scale_color_Channel +
  scale_x_continuous(name="Percentage of Models", labels = scales::percent, limits = c(0,1)) +
  scale_y_continuous(name="Percentage of Responses", labels = scales::percent, limits = c(0,1)) +
  facet_grid(. ~ Label) +
  geom_abline(slope=1, intercept=0, color="grey", linetype="dashed") +
  ggtitle("Cumulative Responses by Models", subtitle = "by Channel")

ggplotly(plt)
```

## Models with largest number of responses (positive or negative).

Zooming in into the models that drive most of the responses, here we
list the top `r params$max_propositions_in_plot` models with the highest
number of responses.

### Guidance

-   There shouldn't be very large outliers

```{r Models with top responses, fig.height=8, fig.width=8}
topNResponses <- filterLatestSnapshotOnly(datamart$modeldata) [frank(-ResponseCount, ties.method="dense") <= params$max_propositions_in_plot][order(-ResponseCount)]
topNResponses[, Label := apply(topNResponses[,availableStandardContextKeys,with=F], 1, paste, collapse="/")]
topNResponses[, Rank := factor(seq(.N), levels=rev(seq(.N)))]

ggplot(topNResponses,
       aes(Rank, ResponseCount, fill=Channel)) +
  geom_col() +
  geom_text(aes(y=0,label=Label), size=3, hjust=0) +
  coord_flip() +
  scale_fill_Channel +
  ggtitle(paste("Top", params$max_propositions_in_plot, "highest Responses"))
```

## Models with largest number of positive responses.

And these are the `r params$max_propositions_in_plot` models with the
largest number of positives.

```{r Models with top positives, fig.height=8, fig.width=8}
topNPositives <- filterLatestSnapshotOnly(datamart$modeldata) [frank(-Positives, ties.method="dense") <= params$max_propositions_in_plot][order(-Positives)]
topNPositives[, Label := apply(topNPositives[,availableStandardContextKeys,with=F], 1, paste, collapse="/")]
topNPositives[, Rank := factor(seq(.N), levels=rev(seq(.N)))]

ggplot(topNPositives,
       aes(Rank, Positives, fill=Channel)) +
  geom_col() +
  geom_text(aes(y=0,label=Label), size=3, hjust=0) +
  coord_flip() +
  scale_fill_Channel +
  ggtitle(paste("Top", params$max_propositions_in_plot, "highest Positives"))
```

## Analysis of Performance vs Volume

Is most volume driven by models that have a good predictive performance?
Ideally yes, so the targeting of the customers is optimal. If a lot of
volume is driven by models that are not very predictive, this could be a
reason to look into the available predictor data.

The plot below shows this relation. Horizontally the model performance
(the AUC, ranging from 50 to 100 as Pega usually scales this),
descretized into a number of ranges, and vertically the percentage of
responses.

A lot of volume on the first bins, where the performance is minimal,
means that a lot of immature models are used. This is sub-optimal in
terms of targeting. Ideally there is a smooth curve with a peak in the
60-80 range of AUC. Much higher AUC's are possibly indicative of
immature models or even outcome leakers (although that is effectively
prevented by the standard delayed learning pattern). AUC's below 60 are
not uncommon but should be investigated - consider different predictors
or outcomes.

```{r AUC vs Volume gains, fig.height=5, fig.width=8, message=FALSE, warning=FALSE}
cumAUCOverVolumeData <- 
  filterLatestSnapshotOnly(datamart$modeldata) [, list(Performance = 100*max(Performance), 
                                                       ResponseCount = max(as.double(ResponseCount))), 
                                                by=c("Direction", "Channel", "ModelID")]
cumAUCOverVolumeData[, TotalResponses := sum(ResponseCount), by=Channel]
cumAUCOverVolumeData[, PerformanceBin := cut(Performance, breaks=15)]

plt <- ggplot(cumAUCOverVolumeData[, list(N = sum(ResponseCount)/TotalResponses), 
                                   by=c("PerformanceBin", "Direction", "Channel")], 
              aes(PerformanceBin, N, color=Channel)) + 
  #geom_col(position = position_dodge()) +
  geom_smooth(aes(group=Channel), se = F, method = "loess") +
  scale_color_Channel +
  scale_y_continuous(name="Percentage of Responses", labels = scales::percent) +
  xlab("Model Performance") +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  facet_wrap(. ~ Direction) +
  ggtitle("Distribution of Volume vs Model Performance", subtitle = "by Channel")

ggplotly(plt)
```

## Positives vs. Number of Models

Ideally, all models have received plenty of responses which will make
them "mature" and makes sure they are as predictive as possible.

Often we see that there is a significant percentage of models that are
still relatively new and have not received much feedback (yet). Below
graph shows the percentages of models that have fewer than
`r params$min_positives_for_maturity` positives.

Having many on the left-hand side (with very low or perhaps no
positives) may or not be a problem. The models may still be there in the
datamart but might represent actions/treatments that are not active.

```{r Postives gains, fig.height=5, fig.width=8, message=FALSE}
modelsByPositives <- filterLatestSnapshotOnly(datamart$modeldata)
modelsByPositives[, PositivesBin := cut(Positives, breaks=c(seq(0,as.integer(params$min_positives_for_maturity),by=10), Inf), 
                                        right=F)]
modelsByPositives <- modelsByPositives[, list(Positives = min(Positives), 
                                              Models = uniqueN(ModelID)),
                                       by=c("Direction", "Channel", "PositivesBin")]
setorder(modelsByPositives, Positives)
modelsByPositives[, cumModels := Models/sum(Models), by=c("Direction", "Channel")]

plt <- ggplot(modelsByPositives, aes(PositivesBin, cumModels, color=Channel)) + 
  geom_line(aes(group=Channel), size=1) + geom_point() +
  scale_color_Channel +
  scale_y_continuous(name="Percentage of Models", labels = scales::percent) +
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  xlab("Positives") +
  facet_wrap(. ~ Direction) +
  ggtitle("Percentage of models vs number of positive responses", subtitle = "by Channel")

ggplotly(plt)
```

# Propensity Analysis

The distribution of propensities returned by the models is yet a
different angle.

Higher propensities clearly indicate the offers are more attractive -
people apparenty click/accept/convert more often.

## Success Rate distribution

```{r Success Rate distribution, fig.height=5}

channelz <- as.character((datamart$modeldata[Performance > 0.5,.N,by=Channel])[N>0]$Channel)

sampledRates <- datamart$modeldata[ResponseCount > 0, .(SuccessRate = sample(SuccessRate, prob=ResponseCount, size=10000, replace=T)), 
                                   by=c("Direction", "Channel", "ConfigurationName")]
quantiles <- sampledRates[, .(p5 = quantile(SuccessRate, 0.05)), by=c("Channel", "Direction", "ConfigurationName")]

plt <- ggplot(datamart$modeldata, aes(SuccessRate, color=ConfigurationName)) + 
  geom_density() + 
  facet_wrap(.~Channel+Direction) + 
  ggtitle("Distribution of Success Rates", subtitle = "dashed line is 5th quantile") + 
  scale_x_continuous(labels=scales::percent) +
  geom_vline(data=quantiles, aes(xintercept=p5, color=ConfigurationName), linetype="dashed")
print(plt)    
    
plt <- ggplot(sampledRates, aes(SuccessRate, color=ConfigurationName)) + 
  geom_density() + 
  facet_wrap(.~Channel+Direction) + 
  ggtitle("Distribution of Sampled Success Rates", subtitle = "taking number of responses into account") +
  scale_x_continuous(labels=scales::percent) +
  geom_vline(data=quantiles, aes(xintercept=p5, color=ConfigurationName), linetype="dashed")
print(plt)
```

## Propensity distribution from Binning

In a more emphathetic setup, you would expect that the distribution of
the propensities leans towards the right-hand side: more volume to more
attractive offers, although the relation is of course more complex, we
are not just blindly pushing the offers with the highest success rates,
but take a personalized approach.

Often however, multiple factors are included in the prioritization,
changing this picture.

Note that the propensity bins are not of equal width. Propensities are
typically very low so with an equal width distribution, almost all
volume would be in the first bins. The binning here is based on
(roughly) equal volume across all data.

So when one of the graphs shows more volume on the left, that is to be
interpreted as relative to the other graphs.

```{r Propensity distribution, fig.height=3, fig.width=8, message=FALSE, warning=FALSE}
if (!is.null(datamart$predictordata) && nrow(datamart$predictordata) > 0) {
  classifiers <- merge(filterClassifierOnly(datamart$predictordata), 
                       unique(datamart$modeldata[,c("ModelID", "Direction", "Channel")]), by="ModelID")
  # classifiers[, Propensity := (0.5+BinPositives)/(1+BinResponseCount)]
  classifiers[, PropensityRange := Hmisc::cut2(Propensity, g=15)]
  classifiers[, TotalVolume := sum(BinResponseCount), by=c("Direction", "Channel")]
  
  setorder(classifiers, Direction, Channel, PropensityRange)
  
  propensityDistributionData <- classifiers[,list(RelVolume = sum(BinResponseCount)/TotalVolume, 
                                                  BinVolume = sum(BinResponseCount),
                                                  Propensity = max(Propensity)), 
                                            by=c("Direction", "Channel", "PropensityRange", "TotalVolume")]
  propensityDistributionData[, CumVolume := cumsum(BinVolume), by=c("Direction", "Channel")]
  propensityDistributionData[, CumFraction := 1 - CumVolume/TotalVolume]
  
  ggplot(propensityDistributionData,
         aes(PropensityRange, RelVolume, fill=Channel, group=Channel)) +
    geom_col() + geom_smooth(se=F, method = "loess") +
    theme(axis.text.x=element_text(angle=45, hjust=1, size=7)) +
    facet_wrap(. ~ Channel, strip.position = "right") + #, scales = "free_x") +
    xlab("Propensity") +
    scale_y_continuous(name="Volume", labels = scales::percent) +
    scale_fill_Channel + 
    facet_wrap(. ~ Direction) +
    ggtitle("Volume vs. Propensity") 
} else {
  cat("Propensity analysis will only be available when predictor binning data is available.", fill=T)
}
```

## Propensity thresholding

If you would apply a threshold on the propensities, in the plot below
you see the portion of the customers that you would reach based on the
propensities from the models.

Note that in CDH, prioritization typically includes value and often also
weights and levers that are not taken into account in this view.

```{r Cumulative Propensity distribution, fig.height=5, fig.width=8, message=FALSE, warning=FALSE}
if (!is.null(datamart$predictordata) && nrow(datamart$predictordata) > 0) {
  
  plt <- ggplot(propensityDistributionData, aes(Propensity, CumFraction)) +
    geom_line(aes(color=Channel, group=Channel), size=1) +
    scale_color_Channel +
    scale_y_continuous(name="Fraction of Customers", labels = scales::percent) +
    facet_wrap(. ~ Direction) +
    ggtitle("Cumulative Volume reached by thresholding on Propensity") +
    xlab("Propensity Threshold")
  
  ggplotly(plt)
} else {
  cat("Propensity analysis will only be available when predictor binning data is available.", fill=T)
}
```

# Appendix - all the models

```{r List all models}
datamart$modeldata[, list(Snapshots = .N, Responses = max(as.double(ResponseCount))), by=c(availableStandardContextKeys, "ModelID")] #[order(ConfigurationName, Name)]
```

A list of all the models is written to a file `r params$modellist` so a
script can iterate over all models and generate off-line model reports
for each of them.

Generally you will want to apply some filtering, or do this for specific
models only. This can be accomplished in either this script here, or by
editing the generated file.

```{r File with all models}
if (params$modellist != "") {
  inclKeys <- availableStandardContextKeys[sapply(availableStandardContextKeys, 
                                                  function(x) {
                                                    return(length(unique(datamart$modeldata[[x]]))>1)
                                                  })]
  
  modelIDandSanitizedNames <- unique(datamart$modeldata[, list(make.names(apply(.SD, 1,
                                                                                function(x) {
                                                                                  return(paste(x,collapse="_"))
                                                                                }))), 
                                                        by=ModelID, .SDcols=inclKeys])
  
  write.table(modelIDandSanitizedNames[ModelID %in% unique(datamart$predictordata$ModelID)], 
              params$modellist, row.names = F, col.names = F, quote=F, sep=";")
}
```
