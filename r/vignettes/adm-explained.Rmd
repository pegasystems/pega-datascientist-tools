---
title: "adm-explained"
author: "Pega"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{adm-explained}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include = FALSE}
# knitr options: https://yihui.name/knitr/options/
knitr::opts_chunk$set(
  collapse = TRUE,
  fig.width = 7,
  fig.height = 5,
  fig.align = "center",
  comment = "#>"
)

library(ggplot2)
library(scales)

theme_set(theme_minimal())

scoreCalculationWarning <- "Log odds do not match up. Please issue a GitHub issue to PDS tools with all details."
```

# ADM Model Report Explained

This notebook shows exactly how all the values in an ADM model report
are calculated. It also shows how the propensity is calculated for a
particular customer.

We use one of the shipped datamart exports for the example. This is a
model very similar to one used in some of the ADM PowerPoint/Excel deep
dive examples. You can change this notebook to apply to your own data.

```{r include=F}
library(pdstools)

# include below when developing the library
# sapply(list.files("~/Documents/pega/pds-datascientist-tools/r/R", "*.R", full.names = T), source)

library(data.table)

data(adm_datamart)
```

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# Commented-out chunk useful to find a good example in the data
# this is just used internally, not run as part of the notebook

# Find models that have active numeric predictors with multiple bins
modelsGoodPreds <- unique((filterLatestSnapshotOnly(adm_datamart$predictordata) [TotalBins>4 & Type=="numeric" & EntryType == "Active"])$ModelID )
print(filterLatestSnapshotOnly(adm_datamart$modeldata)[ModelID %in% modelsGoodPreds, c("ModelID", "Positives","Performance","ActivePredictors","Name","Channel","AUC")][order(-ActivePredictors)])

adm_datamart$predictordata[ModelID == "68e1d164-81e3-5da0-816c-0bbc3c20ac6c" & TotalBins>4 & Type=="numeric" & EntryType == "Active", .(AUC = first(AUC)), by=c("ModelID", "PredictorName")]
```

```{r}
example.Name <- "AutoNew84Months"
example.Channel <- "Web"
example.Predictor <- "Customer.NetWealth"
```

For the example we use one particular model: *`r example.Name`* over
*`r example.Channel`*. You can use your own data and select a different
model.

To explain the ADM model report, we use one of the active predictors as
an example. Swap for any other predictor when using different data.

```{r include=F}
model <- filterLatestSnapshotOnly(adm_datamart$modeldata) [Name == example.Name & Channel == example.Channel]
modelpredictors <- filterLatestSnapshotOnly(adm_datamart$predictordata) [ModelID == model$ModelID & EntryType != "Inactive"]

predictorbinning <- modelpredictors[PredictorName == example.Predictor][order(BinIndex)]
```

```{r include=F}
# For verification of the code, see if the results correspond to the official version
activeRangeInfo <- pdstools::getActiveRanges(ADMDatamart(model, modelpredictors[,Contents := "N/A"]))
```

```{r include=F}
# Add back a few fields that will get dropped by the ADMDatamart function.
predictorbinning[, BinResponseCountPercentage := BinResponseCount/sum(BinResponseCount)]
predictorbinning[, BinPositivesPercentage := BinPositives/sum(BinPositives)]
predictorbinning[, BinNegativesPercentage := BinNegatives/sum(BinNegatives)]
```

# Model overview

The selected model is shown below. Only the currently active predictors
are used for the propensity calculation, so only showing those.

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
modelmetadata <- data.table(Action = paste(model$Issue, model$Group, sep="/"),
                            Channel = model$Channel,
                            Name = model$Name,
                            `Active Predictors` = paste(setdiff(unique(modelpredictors$PredictorName),"Classifier"), collapse = ", "),
                            `Model Performance (AUC)` = model$Performance*100)
kable(t(modelmetadata))  %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left") %>%
  column_spec(1, bold=T)
```

## Predictor binning for `r predictorbinning$PredictorName[1]`

The Model Report in Prediction Studio for this model will have a
predictor binning plot like below.

All numbers can be derived from just the number of positives and
negatives in each bin that are stored in the ADM Data Mart. The next
sections will show exactly how that is done.

```{r, echo=FALSE}
predictormetadata <- data.table(Name = predictorbinning$PredictorName[1],
                                Range = predictorbinning$Contents[1],
                                Responses = predictorbinning$ResponseCount[1],
                                `# Bins` = predictorbinning$TotalBins[1],
                                `Predictor Performance (AUC)` =  predictorbinning$Performance[1]*100)
kable(t(predictormetadata))  %>%
  kable_styling(bootstrap_options = "striped", full_width = F, position = "left") %>%
  column_spec(1, bold=T)
```

```{r, echo=FALSE, fig.height=3, fig.width=4}
pdstools::plotBinning(predictorbinning)
```

```{r, echo=FALSE}
predictorbinning2 <- data.table( `Range/Symbols` = predictorbinning$BinSymbol,
                                 `Responses (%)` = predictorbinning$BinResponseCountPercentage,
                                 `Positives` = predictorbinning$BinPositives,
                                 `Positives (%)` = predictorbinning$BinPositivesPercentage,
                                 `Negatives` = predictorbinning$BinNegatives,
                                 `Negatives (%)` = predictorbinning$BinNegativesPercentage,
                                 # strange that propensity would not be available
                                 `Propensity (%)` = round(predictorbinning$BinPositives/(predictorbinning$BinResponseCount), digits = 4),
                                 `Z-Ratio` = predictorbinning$ZRatio,
                                 `Lift` = predictorbinning$Lift
)
totals <- data.table(`Range/Symbols` = "Total")[, names(predictorbinning2)[2:9] := c(lapply(predictorbinning2[, 2:6], sum), as.numeric(predictorbinning$Positives[1])/predictorbinning$ResponseCount[1], 0.0, 1.0)]

predictorbinning <- predictorbinning2
predictorbinning2 <- rbind(predictorbinning2, totals)

kable(predictorbinning2) %>%
  kable_styling(bootstrap_options = "striped", full_width = F) %>% 
  row_spec(nrow(predictorbinning2), bold=T)
```

# Bin statistics

## Positive and Negative ratios

Internally, ADM only keeps track of the total counts of positive and
negative responses in each bin. Everything else is derived from those
numbers. The percentages and totals are trivially derived, and the
propensity is just the number of positives divided by the total. The
numbers calculated here match the numbers from the datamart table
exactly.

```{r}
binningDerived <- predictorbinning[, c("Range/Symbols","Positives","Negatives")] # copy over only the labels, pos and neg counts
binningDerived[, `Responses %` := (Positives+Negatives)/(sum(Positives)+sum(Negatives))]
binningDerived[, `Positives %` := Positives/sum(Positives)]
binningDerived[, `Negatives %` := Negatives/sum(Negatives)]
binningDerived[, Propensity := (Positives)/(Positives+Negatives)]
```

```{r, echo=F}
binningDerived[, `Responses %` := round(100*`Responses %`,2)]
binningDerived[, `Positives %` := round(100*`Positives %`,2)]
binningDerived[, `Negatives %` := round(100*`Negatives %`,2)]
binningDerived[, Propensity := round(Propensity,4)]
kable(binningDerived) %>%
  kable_styling(bootstrap_options = "striped", full_width = F) %>%
  column_spec(2:3, bold = T, border_left = T, border_right = T) %>%
  column_spec(4:7, color = "blue") 
binningDerived[, Propensity := (Positives)/(Positives+Negatives)] # put back as we changed it for display purposes
```

## Lift

Lift is the ratio of the propensity in a particular bin over the average
propensity. So a value of 1 is the average, larger than 1 means higher
propensity, smaller means lower propensity:

```{r}
binningDerived[, Lift := (Positives/(Positives+Negatives)) / (sum(Positives)/sum(Positives+Negatives))]
```

```{r, echo=F}
binningDerived[, `Responses %` := NULL]
binningDerived[, `Positives %` := NULL]
binningDerived[, `Negatives %` := NULL]
binningDerived[, Propensity := NULL]

binningDerived[, Lift := round(Lift,4)]
kable(binningDerived) %>%
  kable_styling(bootstrap_options = "striped", full_width = F) %>%
  column_spec(c(2,3), bold = T, border_left = T, border_right = T) %>%
  column_spec(4, color = "blue") 
```

## Z-Ratio

The Z-Ratio is also a measure of the how the propensity in a bin differs
from the average, but takes into account the size of the bin and thus is
statistically more relevant. It represents the number of standard
deviations from the average, so centers around 0. The wider the spread,
the better the predictor is.

$$\frac{posFraction-negFraction}{\sqrt(\frac{posFraction*(1-posFraction)}{\sum positives}+\frac{negFraction*(1-negFraction)}{\sum negatives})}$$

See the calculation here, which is also included in
[pdstools::zratio](https://pegasystems.github.io/pega-datascientist-tools/R/reference/zratio.html).

```{r}
binningDerived[, posFraction := Positives/sum(Positives)]
binningDerived[, negFraction := Negatives/sum(Negatives)]
binningDerived[, `Z-Ratio` := (posFraction-negFraction)/sqrt(posFraction*(1-posFraction)/sum(Positives) + negFraction*(1-negFraction)/sum(Negatives))]
```

```{r, echo=F}
binningDerived[, Lift := NULL]

kable(binningDerived) %>%
  kable_styling(bootstrap_options = "striped", full_width = F) %>%
  column_spec(c(2,3), bold = T, border_left = T, border_right = T) %>%
  column_spec(6, color = "blue") 
```

# Predictor AUC

The predictor AUC is the univariate performance of this predictor
against the outcome. This too can be derived from the positives and
negatives, e.g. using the *pROC* package.

```{r, warning=F,message=F}
library(pROC)

response = unlist(sapply(1:nrow(predictorbinning),
                         function(r){return(c(rep(T, predictorbinning$Positives[r]), 
                                              rep(F, predictorbinning$Negatives[r])))}))

prediction = unlist(sapply(1:nrow(predictorbinning),
                           function(r){return(rep(predictorbinning$`Propensity (%)`[r], 
                                                  predictorbinning$Positives[r] +
                                                    predictorbinning$Negatives[r]))}))

plot.roc(response, prediction, print.auc=T, col="darkgreen", levels=c(T,F), direction=">")
```

There is also a convenient function in *pdstools* to calculate it
directly from the positives and negatives:
[`pdstools::auc_from_bincounts()`](https://pegasystems.github.io/pega-datascientist-tools/R/reference/auc_from_bincounts.html).

```{r}
pdstools::auc_from_bincounts(predictorbinning$Positives, predictorbinning$Negatives)
```

# Naive Bayes and Log Odds

The basis for the Naive Bayes algorithm is Bayes' Theorem:

$$p(C_k|x) = \frac{p(x|C_k)*p(C_k)}{p(x)}$$

with $C_k$ the outcome and $x$ the customer. Bayes' theorem turns the
question "what's the probability to accept this action given a customer" around to 
"what's the probability of this customer given an action". With the independence
assumption, and after applying a log odds transformation we get a log odds score 
that can be calculated efficiently and in a numerically stable manner:

$$log\ odds\ score = \sum_{p\ \in\ active\ predictors}log(p(x_p|Positive)) + log(p_{positive}) - \sum_plog(p(x_p|Negative)) - log(p_{negative})$$
note that the _prior_ can be written as:

$$log(p_{positive}) - log(p_{negative}) = log(\frac{TotalPositives}{Total})-log(\frac{TotalNegatives}{Total}) = log(TotalPositives) - log(TotalNegatives)$$

## Predictor Contribution

The contribution (_conditional log odds_) of an active predictor $p$ for bin $i$ with the number
of positive and negative responses in $Positives_i$ and $Negatives_i$ is calculated as (note the "laplace smoothing" to avoid log 0 issues):

$$contribution_p = \log(Positives_i+\frac{1}{nBins}) - \log(Negatives_i+\frac{1}{nBins}) - \log(1+\sum_{i\ = 1..nBins}{Positives_i}) + \log(1+\sum_i{Negatives_i})$$

```{r}
binningDerived[, posFraction := Positives/sum(Positives)]
binningDerived[, negFraction := Negatives/sum(Negatives)]
binningDerived[, `Log odds` := log(posFraction/negFraction)]
binningDerived[, `Modified Log odds` := 
                 (log(Positives+1/.N) - log(sum(Positives)+1)) - 
                 (log(Negatives+1/.N) - log(sum(Negatives)+1))]
```

```{r include=FALSE}
# internal validation
if (abs(min(binningDerived$`Modified Log odds`) - activeRangeInfo[["predictordetails"]][PredictorName == predictormetadata$Name]$logOddsMin) > 1e-5) {
  stop(paste0(scoreCalculationWarning, " (", predictormetadata$Name, " minimum log odds contribution)"))
}
if (abs(max(binningDerived$`Modified Log odds`) - activeRangeInfo[["predictordetails"]][PredictorName == predictormetadata$Name]$logOddsMax) > 1e-5) {
  stop(paste0(scoreCalculationWarning, " (", predictormetadata$Name, " maximum log odds contribution)"))
}
```

```{r, echo=F}
binningDerived[, `Z-Ratio` := NULL]
kable(binningDerived) %>%
  kable_styling(bootstrap_options = "striped", full_width = F) %>%
  column_spec(c(2,3), bold = T, border_left = T, border_right = T) %>%
  column_spec(c(6,7), color = "blue") 
```

# Propensity mapping

## Log odds contribution for all the predictors

The final score is normalized by the number of active predictors, 1's are added to avoid avoid numerical instability:

$$score = \frac{\log(1 + TotalPositives) â€“ \log(1 + TotalNegatives) + \sum_p contribution_p}{1 + nActivePredictors}$$

Here, $TotalPositives$ and $TotalNegatives$ are the total number of
positive and negative responses to the model.

Below an example. From all the active predictors of the model for
`r model$name` we pick a value (in the middle for numerics, first symbol
for symbolics) and show the (modified) log odds. The final score is
calculated per the above formula, and this is the value that is mapped
to a propensity value by the classifier (which is constructed using the
[PAV(A)](https://en.wikipedia.org/wiki/Isotonic_regression) algorithm).

```{r, echo=F}
binning <- modelpredictors[EntryType=="Active", c("PredictorName", "BinSymbol", "BinIndex", "BinPositives", "BinNegatives", "Type","BinLowerBound", "BinUpperBound"), with=F][order(PredictorName, BinIndex)]
setnames(binning, c("Name", "Value", "Bin", "Positives", "Negatives", "Type", "lobound", "hibound"))

binning[, `Log odds` := (log(Positives+1/.N) - log(1+sum(Positives))) - (log(Negatives+1/.N) - log(1+sum(Negatives))), by=Name]

# double check against the official results
binningSummaryByPredictor <- binning[, .(logOddsMin = min(`Log odds`), logOddsMax = max(`Log odds`)), by=Name]
predictorMinMaxLogOddsValidation <- merge(binningSummaryByPredictor,
                                activeRangeInfo[["predictordetails"]], by.x="Name", by.y="PredictorName", all.x = T)
if (any(abs(predictorMinMaxLogOddsValidation$logOddsMin.x - predictorMinMaxLogOddsValidation$logOddsMin.y) > 1e-5)) {
  stop(paste(scoreCalculationWarning, "(all predictors minimum log odds contribution)"))
}
if (any(abs(predictorMinMaxLogOddsValidation$logOddsMax.x - predictorMinMaxLogOddsValidation$logOddsMax.y) > 1e-5)) {
  stop(paste(scoreCalculationWarning, "(all predictors maximum log odds contribution)"))
}

# classifier factor
classifier <- modelpredictors[EntryType == "Classifier"][order(BinIndex)]
classifier[, Propensity := BinPositives/(BinPositives+BinNegatives)]
classifier[, AdjustedPropensity := (0.5+BinPositives)/(1+BinPositives+BinNegatives)]
classifierLogOffset <- log(1+sum(classifier$BinPositives)) - log(1+sum(classifier$BinNegatives))

# double check against official results
score_min <- (sum(binningSummaryByPredictor$logOddsMin) + classifierLogOffset) / (nrow(binningSummaryByPredictor)+1)
score_max <- (sum(binningSummaryByPredictor$logOddsMax) + classifierLogOffset) / (nrow(binningSummaryByPredictor)+1)
if (abs(classifierLogOffset - activeRangeInfo[[1]]$classifierLogOffset) > 1e-5) {
  stop(paste(scoreCalculationWarning, "(Classifier offset)"))
}
if (any(abs(score_min - activeRangeInfo[[1]]$score_min) > 1e-5)) {
  stop(paste(scoreCalculationWarning, "(minimum Classifier score)"))
}
if (any(abs(score_max - activeRangeInfo[[1]]$score_max) > 1e-5)) {
  stop(paste(scoreCalculationWarning, "(maximum Classifier score)"))
}

binning[,nbins := max(Bin), by=Name]
binning <- binning[Bin == trunc(nbins/2)] # take middle bin
for (r in 1:nrow(binning)) {
  if (binning$Type[r] == "numeric") {
    binning$Value[r] <- trunc((as.numeric(binning$lobound[r]) + as.numeric(binning$hibound[r]))/2) # middle value
  } else {
    binning$Value[r] <- strsplit(binning$Value[r], ",", fixed=T)[[1]][1] # first symbol
  }
}
binning <- binning[, c(1:5,9)]

# last row becomes the total score
binning <- rbindlist(list(binning, data.table(Name = "Final Score",
                                              Value = "",
                                              `Log odds` = (sum(binning$`Log odds`)+classifierLogOffset)/(nrow(binning)+1))), 
                     use.names = T, fill = T)
kable(binning) %>%
  kable_styling(bootstrap_options = "striped", full_width = F) %>%
  column_spec(6, color = "blue") %>%
  row_spec(nrow(binning), bold=T)
```

## Classifier

The success rate is defined as $\frac{positives}{positives+negatives}$
per bin.

The adjusted propensity that is returned is a small modification
(Laplace smoothing) to this and calculated as
$\frac{0.5+positives}{1+positives+negatives}$ so empty models return a
propensity of 0.5.

Bins that are not reachable given the current predictor binning are
greyed out.

```{r echo=F, error=F, warning=FALSE}
score <- binning[Name == "Final Score"]$`Log odds`
scorebin <- findInterval(score, as.numeric(classifier$BinLowerBound), left.open = T, all.inside = T)

classifierTable <- userFriendlyADMBinning(classifier)

# grey out unreachable bins
if (activeRangeInfo[[1]]$is_full_indexrange) {
  kable(classifierTable) %>%
    kable_styling(bootstrap_options = "striped", full_width = F) %>%
    column_spec(c(2,3,4), bold = T, border_left = T, border_right = T) %>%
    column_spec(c(7), color = "blue") 
} else {
  kable(classifierTable) %>%
    kable_styling(bootstrap_options = "striped", full_width = F) %>%
    column_spec(c(2,3,4), bold = T, border_left = T, border_right = T) %>%
    column_spec(c(7), color = "blue") %>%
    row_spec(setdiff((1:nrow(classifier)), activeRangeInfo[[1]]$active_index_min : activeRangeInfo[[1]]$active_index_max), color = "grey")
}
```

## Final propensity

Below the classifier mapping. On the x-axis the binned scores (log odds
values), on the y-axis the Propensity. Note the returned propensities
are following a slightly adjusted formula, see the table above. The bin
that contains the calculated score is highlighted.

The score *`r score`* falls in **bin `r scorebin`** of the classifier, so
for this customer, the model returns a propensity of
**`r sprintf("%.2f%%", 100*classifier$AdjustedPropensity[scorebin])`**.

```{r, echo=F}
responsesMax <- max(classifier$BinResponseCount, na.rm = T)
if (0 == responsesMax) { responsesMax <- 1 }
secAxisFactor <- max(classifier$BinPositives/classifier$BinResponseCount) / responsesMax

plotBinning(classifier) +
  geom_col(aes(y=BinResponseCount), classifier[BinIndex < activeRangeInfo[[1]]$active_index_min | BinIndex > activeRangeInfo[[1]]$active_index_max], fill="grey")+
  geom_col(aes(y=BinResponseCount), classifier[!(BinIndex < activeRangeInfo[[1]]$active_index_min | BinIndex > activeRangeInfo[[1]]$active_index_max) & (BinIndex != scorebin)], fill="lightblue")+
  geom_col(aes(y=BinResponseCount), classifier[BinIndex == scorebin], fill="steelblue3")+
  geom_line(aes(y=Propensity/secAxisFactor), colour="orange", linewidth=2)+
  geom_point(aes(y=Propensity/secAxisFactor)) +
  geom_hline(data=classifier[1,], 
             mapping = aes(yintercept = (Positives/ResponseCount)/secAxisFactor),
             colour="orange", linetype="dashed") +
  geom_label(data=classifier[scorebin,],
             mapping = aes(y=Propensity/secAxisFactor, 
                           label=sprintf("Propensity: %.2f%%", 100*AdjustedPropensity)),
             # y=(classifier[scorebin,]$BinPositives/classifier[scorebin,]$BinResponseCount)/secAxisFactor + 0.0002,
             vjust=0, alpha=0.6)
```
