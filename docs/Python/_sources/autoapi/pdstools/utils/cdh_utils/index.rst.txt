:py:mod:`pdstools.utils.cdh_utils`
==================================

.. py:module:: pdstools.utils.cdh_utils

.. autoapi-nested-parse::

   cdhtools: Data Science add-ons for Pega.

   Various utilities to access and manipulate data from Pega for purposes
   of data analysis, reporting and monitoring.



Module Contents
---------------


Functions
~~~~~~~~~

.. autoapisummary::

   pdstools.utils.cdh_utils.readDSExport
   pdstools.utils.cdh_utils.import_file
   pdstools.utils.cdh_utils.readZippedFile
   pdstools.utils.cdh_utils.get_latest_file
   pdstools.utils.cdh_utils.getMatches
   pdstools.utils.cdh_utils.cache_to_file
   pdstools.utils.cdh_utils.defaultPredictorCategorization
   pdstools.utils.cdh_utils.safe_range_auc
   pdstools.utils.cdh_utils.auc_from_probs
   pdstools.utils.cdh_utils.auc_from_bincounts
   pdstools.utils.cdh_utils.aucpr_from_probs
   pdstools.utils.cdh_utils.aucpr_from_bincounts
   pdstools.utils.cdh_utils.auc2GINI
   pdstools.utils.cdh_utils._capitalize
   pdstools.utils.cdh_utils.fromPRPCDateTime
   pdstools.utils.cdh_utils.toPRPCDateTime
   pdstools.utils.cdh_utils.readClientCredentialFile
   pdstools.utils.cdh_utils.getToken



.. py:function:: readDSExport(filename: Union[pandas.DataFrame, str], path: str = '.', verbose: bool = True, **kwargs) -> pandas.DataFrame

   Read a Pega dataset export file.
   Can accept either a Pandas DataFrame or one of the following formats:
   - .csv
   - .json
   - .zip (zipped json or CSV)

   It automatically infers the default file names for both model data as well as predictor data.
   If you supply either 'modelData' or 'predictorData' as the 'file' argument, it will search for them.
   If you supply the full name of the file in the 'path' directory, it will import that instead.

   :param filename: Either a Pandas DataFrame with the source data (for compatibility),
                    or a string, in which case it can either be:
                    - The name of the file (if a custom name) or
                    - Whether we want to look for 'modelData' or 'predictorData' in the path folder.
   :type filename: [pd.DataFrame, str]
   :param path: The location of the file
   :type path: str, default = '.'
   :param verbose: Whether to print out which file will be imported
   :type verbose: bool, default = True
   :param Keyword arguments: Any arguments to plug into the read csv or json function, from either PyArrow or Pandas.

   :returns: * *pd.DataFrame* -- The read data from the given file
             * *Examples* -- >>> df = readDSExport(filename = 'modelData', path = './datamart')
               >>> df = readDSExport(filename = 'ModelSnapshot.json', path = 'data/ADMData')

               >>> df = pd.read_csv('file.csv')
               >>> df = readDSExport(filename = df)


.. py:function:: import_file(file, extension, **kwargs)


.. py:function:: readZippedFile(file: str, verbose: bool = False, **kwargs) -> pandas.DataFrame

   Read a zipped file.
   Reads a dataset export file as exported and downloaded from Pega. The export
   file is formatted as a zipped multi-line JSON file or CSV file
   and the data is read into a pandas dataframe.

   :param file: The full path to the file
   :type file: str
   :param verbose: Whether to print the names of the files within the unzipped file for debugging purposes
   :type verbose: str, default=False

   :returns: A pandas dataframe with the contents.
   :rtype: pd.DataFrame


.. py:function:: get_latest_file(path: str, target: str, verbose: bool = False) -> str

   Convenience method to find the latest model snapshot.
   It has a set of default names to search for and finds all files who match it.
   Once it finds all matching files in the directory, it chooses the most recent one.
   It only looks at .json, .csv and .zip files for now, as they are supported.
   Needs a path to the directory and a target of either 'modelData' or 'predictorData'.

   :param path: The filepath where the data is stored
   :type path: str
   :param target: Whether to look for data about the predictive models ('modelData')
                  or the predictor bins ('predictorData')
   :type target: str in ['modelData', 'predictorData']
   :param verbose: Whether to print all found files before comparing name criteria for debugging purposes
   :type verbose: bool, default = False

   :returns: The most recent file given the file name criteria.
   :rtype: str


.. py:function:: getMatches(files_dir, target)


.. py:function:: cache_to_file(df: pandas.DataFrame, path: os.PathLike, name: str, cache_type='parquet') -> os.PathLike

   Very simple convenience function to cache data.
   Caches in parquet format for very fast reading.

   :param df: The dataframe to cache
   :type df: pd.DataFrame
   :param path: The location to cache the data
   :type path: os.PathLike
   :param name: The name to give to the file
   :type name: str
   :param cache_type: The type of file to export.
                      Currently, only Parquet is supported,
                      will support Arrow/Feather/IPC soon.
   :type cache_type: str

   :returns: The filepath to the cached file
   :rtype: os.PathLike


.. py:function:: defaultPredictorCategorization(x: str) -> str


.. py:function:: safe_range_auc(auc: float) -> float

   Internal helper to keep auc a safe number between 0.5 and 1.0 always.

   :param auc: The AUC (Area Under the Curve) score
   :type auc: float

   :returns: 'Safe' AUC score, between 0.5 and 1.0
   :rtype: float


.. py:function:: auc_from_probs(groundtruth: List[int], probs: List[float]) -> List[float]

   Calculates AUC from an array of truth values and predictions.
   Calculates the area under the ROC curve from an array of truth values and
   predictions, making sure to always return a value between 0.5 and 1.0 and
   returns 0.5 when there is just one groundtruth label.

   :param groundtruth: The 'true' values, Positive values must be represented as
                       True or 1. Negative values must be represented as False or 0.
   :type groundtruth: List[int]
   :param probs: The predictions, as a numeric vector of the same length as groundtruth
   :type probs: List[float]
   :param Returns: The AUC as a value between 0.5 and 1.
   :type Returns: List[float]
   :param Examples: >>> auc_from_probs( [1,1,0], [0.6,0.2,0.2])


.. py:function:: auc_from_bincounts(pos: List[int], neg: List[int], probs: List[float] = None) -> float

   Calculates AUC from counts of positives and negatives directly
   This is an efficient calculation of the area under the ROC curve directly from an array of positives
   and negatives. It makes sure to always return a value between 0.5 and 1.0
   and will return 0.5 when there is just one groundtruth label.

   :param pos: Vector with counts of the positive responses
   :type pos: List[int]
   :param neg: Vector with counts of the negative responses
   :type neg: List[int]
   :param probs: Optional list with probabilities which will be used to set the order of the bins. If missing defaults to pos/(pos+neg).
   :type probs: List[float]

   :returns: * *float* -- The AUC as a value between 0.5 and 1.
             * *Examples* -- >>> auc_from_bincounts([3,1,0], [2,0,1])


.. py:function:: aucpr_from_probs(groundtruth: List[int], probs: List[float]) -> List[float]

   Calculates PR AUC (precision-recall) from an array of truth values and predictions.
   Calculates the area under the PR curve from an array of truth values and
   predictions. Returns 0.0 when there is just one groundtruth label.

   :param groundtruth: The 'true' values, Positive values must be represented as
                       True or 1. Negative values must be represented as False or 0.
   :type groundtruth: List[int]
   :param probs: The predictions, as a numeric vector of the same length as groundtruth
   :type probs: List[float]
   :param Returns: The AUC as a value between 0.5 and 1.
   :type Returns: List[float]
   :param Examples: >>> auc_from_probs( [1,1,0], [0.6,0.2,0.2])


.. py:function:: aucpr_from_bincounts(pos: List[int], neg: List[int], probs: List[float] = None) -> float

   Calculates PR AUC (precision-recall) from counts of positives and negatives directly.
   This is an efficient calculation of the area under the PR curve directly from an
   array of positives and negatives. Returns 0.0 when there is just one
   groundtruth label.

   :param pos: Vector with counts of the positive responses
   :type pos: List[int]
   :param neg: Vector with counts of the negative responses
   :type neg: List[int]
   :param probs: Optional list with probabilities which will be used to set the order of the bins. If missing defaults to pos/(pos+neg).
   :type probs: List[float]

   :returns: * *float* -- The PR AUC as a value between 0.0 and 1.
             * *Examples* -- >>> aucpr_from_bincounts([3,1,0], [2,0,1])


.. py:function:: auc2GINI(auc: float) -> float

   Convert AUC performance metric to GINI

   :param auc: The AUC (number between 0.5 and 1)
   :type auc: float

   :returns: * *float* -- GINI metric, a number between 0 and 1
             * *Examples* -- >>> auc2GINI(0.8232)


.. py:function:: _capitalize(fields: list) -> list

   Applies automatic capitalization, aligned with the R couterpart.

   :param fields: A list of names
   :type fields: list

   :returns: **fields** -- The input list, but each value properly capitalized
   :rtype: list


.. py:function:: fromPRPCDateTime(x: str, return_string: bool = False) -> Union[datetime.datetime, str]

   Convert from a Pega date-time string.

   :param x: String of Pega date-time
   :type x: str
   :param return_string: If True it will return the date in string format. If
                         False it will return in datetime type
   :type return_string: bool, default=False

   :returns: * *Union[datetime.datetime, str]* -- The converted date in datetime format or string.
             * *Examples* -- >>> fromPRPCDateTime("20180316T134127.847 GMT")
               >>> fromPRPCDateTime("20180316T134127.847 GMT", True)
               >>> fromPRPCDateTime("20180316T184127.846")
               >>> fromPRPCDateTime("20180316T184127.846", True)


.. py:function:: toPRPCDateTime(x: datetime.datetime) -> str

   Convert to a Pega date-time string

   :param x: A datetime object
   :type x: datetime.datetime

   :returns: * *str* -- A string representation in the format used by Pega
             * *Examples* -- >>> toPRPCDateTime(datetime.datetime.now())


.. py:function:: readClientCredentialFile(credentialFile)


.. py:function:: getToken(credentialFile, verify=True, **kwargs)


