:py:mod:`pdstools`
==================

.. py:module:: pdstools


Subpackages
-----------
.. toctree::
   :titlesonly:
   :maxdepth: 3

   healthcheck/index.rst


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   ADMDatamart/index.rst
   ADMTrees/index.rst
   IHanalysis/index.rst
   ValueFinder/index.rst
   cdh_utils/index.rst
   datasets/index.rst
   pds_app/index.rst
   plot_base/index.rst
   plots_mpl/index.rst
   plots_plotly/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   pdstools.ADMDatamart
   pdstools.ADMTrees
   pdstools.MultiTrees
   pdstools.ValueFinder



Functions
~~~~~~~~~

.. autoapisummary::

   pdstools.readDSExport
   pdstools.CDHSample



.. py:class:: ADMDatamart(path: str = '.', overwrite_mapping: Optional[dict] = None, query: Union[str, Dict[str, list]] = None, plotting_engine='plotly', **kwargs)

   Bases: :py:obj:`pdstools.plot_base.Plots`

   Main class for importing, preprocessing and structuring Pega ADM Datamart snapshot data.
   Gets all available data, properly names and merges into one main dataframe

   :param path: The path of the data files
   :type path: str, default = "."
   :param overwrite_mapping: A dictionary to overwrite default feature names in the input data
   :type overwrite_mapping: dict, default = None
   :param query: The query to supply to _apply_query
                 If a string, uses the default Pandas query function
                 Else, a dict of lists where the key is the column name of the dataframe
                 and the corresponding value is a list of values to keep in the dataframe
                 Can be applied to an individual function, or used here to apply to the whole dataset
   :type query: Union[str, dict], default = None
   :param plotting_engine: Either 'mpl' for matplotlib, or 'plotly' for plotly.
                           Determines what package to use for plotting.
                           Can also be supplied to most plotting functions directly.
   :type plotting_engine: str, default = "plotly"

   :keyword verbose: Whether to print out information during importing
   :kwtype verbose: bool
   :keyword model_filename: The name, or extended filepath, towards the model file
   :kwtype model_filename: str
   :keyword predictor_filename: The name, or extended filepath, towards the predictors file
   :kwtype predictor_filename: str
   :keyword model_df: Optional override to supply a dataframe instead of a file
   :kwtype model_df: pd.DataFrame
   :keyword predictor_df: Optional override to supply a dataframe instead of a file
   :kwtype predictor_df: pd.DataFrame
   :keyword subset: Whether to only select the renamed columns,
                    set to False to keep all columns
   :kwtype subset: bool, default = True
   :keyword drop_cols = list: Supply columns to drop from the dataframe
   :keyword include_cols = list: Supply columns to include with the dataframe
   :keyword prequery: A way to apply a query to the data before any preprocessing
                      Uses the Pandas querying function, and beware that the columns
                      have not been renamed, so use the original naming
   :kwtype prequery: str
   :keyword context_keys: Which columns to use as context keys
   :kwtype context_keys: list
   :keyword extract_treatment: Treatments are typically hidden within the pyName column,
                               extract_treatment can expand that cell to also show treatments.
                               To extract, give the column name as the 'extract_treatment' argument
   :kwtype extract_treatment: str
   :keyword force_pandas: If pyarrow is installed, you can force the import through Pandas
   :kwtype force_pandas: bool

   .. rubric:: Notes

   Depending on the importing function, typically it is possible to
   supply more arguments. For instance, if the importing is done through
   Pandas (because pyarrow is not installed or through force_pandas),
   you can supply the column separator from the pandas function as a keyword argument

   .. attribute:: modelData

      If available, holds the preprocessed data about the models

      :type: pd.DataFrame

   .. attribute:: predictorData

      If available, holds the preprocessed data about the predictor binning

      :type: pd.DataFrame

   .. attribute:: combinedData

      If both modelData and predictorData are available,
      holds the merged data about the models and predictors

      :type: pd.DataFrame

   .. rubric:: Examples

   >>> Data =  ADMDatamart(f"/CDHSample")
   >>> Data =  ADMDatamart(f"Data/Adaptive Models & Predictors Export",
               model_filename = "Data-Decision-ADM-ModelSnapshot_AdaptiveModelSnapshotRepo20201110T085543_GMT/data.json",
               predictor_filename = "Data-Decision-ADM-PredictorBinningSnapshot_PredictorBinningSnapshotRepo20201110T084825_GMT/data.json")
   >>> Data =  ADMDatamart(f"Data/files",
               model_filename = "ModelData.csv",
               predictor_filename = "PredictorData.csv")

   .. py:method:: get_engine(plotting_engine)
      :staticmethod:


   .. py:method:: import_data(path: Optional[str] = '.', overwrite_mapping: Optional[dict] = None, subset: bool = True, model_df: Optional[pandas.DataFrame] = None, predictor_df: Optional[pandas.DataFrame] = None, query: Union[str, Dict[str, list]] = None, **kwargs) -> Union[pandas.DataFrame, pandas.DataFrame]

      Method to automatically import & format the relevant data.

      The method first imports the model data, and then the predictor data.
      If model_df or predictor_df is supplied, it will use those instead
      After reading, some additional values (such as success rate) are
      automatically computed.
      Lastly, if there are missing columns from both datasets,
      this will be printed to the user if verbose is True.

      :param path: The path of the data files
                   Default = current path (',')
      :type path: str
      :param overwrite_mapping: A dictionary to overwrite default feature names in the input data
                                Default = None
      :type overwrite_mapping: dict
      :param subset: Whether to only select the renamed columns,
                     set to False to keep all columns
      :type subset: bool, default = True
      :param model_df: Optional override to supply a dataframe instead of a file
      :type model_df: pd.DataFrame
      :param predictor_df: Optional override to supply a dataframe instead of a file
      :type predictor_df: pd.DataFrame
      :param query: The query to supply to _apply_query
                    If a string, uses the default Pandas query function
                    Else, a dict of lists where the key is the column name of the dataframe
                    and the corresponding value is a list of values to keep in the dataframe
      :type query: Union[str, dict], default = None

      :returns: The model data and predictor binning data as dataframes
      :rtype: (pd.DataFrame, pd.DataFrame)


   .. py:method:: _import_utils(name: Union[str, pandas.DataFrame], path: str = None, overwrite_mapping: dict = None, subset: bool = True, query: Union[str, Dict[str, list]] = None, verbose: bool = True, **kwargs) -> Tuple[pandas.DataFrame, dict, dict]

      Handler function to interface to the cdh_utils methods

      :param name: One of {modelData, predictorData}
                   or a dataframe
      :type name: Union[str, pd.DataFrame]
      :param path: The path of the data file
      :type path: str, default = None
      :param overwrite_mapping: A dictionary to overwrite default feature names in the input data
      :type overwrite_mapping: dict, default = None
      :param subset: Whether to only select the renamed columns,
                     set to False to keep all columns
      :type subset: bool, default = True
      :param query: The query to supply to _apply_query
                    If a string, uses the default Pandas query function
                    Else, a dict of lists where the key is the column name of the dataframe
                    and the corresponding value is a list of values to keep in the dataframe
      :type query: Union[str, dict], default = None
      :param verbose: Whether to print out information during importing
      :type verbose: bool

      :keyword drop_cols: Supply columns to drop from the dataframe
      :kwtype drop_cols: list
      :keyword include_cols: Supply columns to include with the dataframe
      :kwtype include_cols: list
      :keyword prequery: A way to apply a query to the data before any preprocessing
                         Uses the Pandas querying function, and beware that the columns
                         have not been renamed, so use the original naming
      :kwtype prequery: str
      :keyword extract_treatment: Treatments are typically hidden within the pyName column,
                                  extract_treatment can expand that cell to also show treatments.
                                  To extract, give the column name as the 'extract_treatment' argument
      :kwtype extract_treatment: str
      :keyword Additional keyword arguments:
      :keyword -----------------:
      :keyword See readDSExport in cdh_utils:

      :returns: The requested dataframe,
                The renamed columns
                The columns missing in both dataframes
      :rtype: (pd.DataFrame, dict, dict)


   .. py:method:: _available_columns(df: pandas.DataFrame, overwrite_mapping: Optional[dict] = None, **kwargs) -> Tuple[pandas.DataFrame, dict, list]

      Based on the default names for variables, rename available data to proper formatting

      :param df: Input dataframe
      :type df: pd.DataFrame
      :param overwrite_mapping: If given, adds 'search terms' to the default names to look for
                                If an extra variable is given which is not in default_names, it will also be included
      :type overwrite_mapping: dict

      :keyword include_cols: Supply columns to include with the dataframe
      :kwtype include_cols: list

      :returns: The original dataframe, but renamed for the found columns &
                The original and updated names for all renamed columns &
                The variables that were not found in the table
      :rtype: (pd.DataFrame, dict, list)


   .. py:method:: _set_types(df: pandas.DataFrame, verbose=True) -> pandas.DataFrame
      :staticmethod:

      A method to change columns to their proper type

      :param df: The input dataframe
      :type df: pd.DataFrame
      :param verbose: Whether to print out issues with casting variable types
      :type verbose: bool, default = True

      :returns: The input dataframe, but the proper typing applied
      :rtype: pd.DataFrame


   .. py:method:: last(table='modelData') -> pandas.DataFrame

      Convenience function to get the last values for a table

      :param table: Which table to get the last values for
                    One of {modelData, predictorData, combinedData}
      :type table: str, default = modelData

      :returns: The last snapshot for each model
      :rtype: pd.DataFrame


   .. py:method:: _last(df: pandas.DataFrame) -> pandas.DataFrame
      :staticmethod:

      Method to retrieve only the last values for a given dataframe.


   .. py:method:: get_combined_data(modelData: pandas.DataFrame = None, predictorData: pandas.DataFrame = None, last=True) -> pandas.DataFrame

      Combines the model data and predictor data into one dataframe.

      :param modelData: Optional dataframe to override 'self.modelData' for merging
      :type modelData: pd.DataFrame
      :param predictorData: Optional dataframe to override 'self.predictorData' for merging
      :type predictorData: pd.DataFrame

      :returns: The combined dataframe
      :rtype: pd.DataFrame


   .. py:method:: fix_pdc(df: pandas.DataFrame) -> pandas.DataFrame
      :staticmethod:


   .. py:method:: defaultPredictorCategorization(x: str) -> str
      :staticmethod:


   .. py:method:: _apply_query(df, query: Union[str, dict] = None) -> pandas.DataFrame
      :staticmethod:

      Given an input pandas dataframe, it filters the dataframe based on input query

      :param query: If a string, uses the default Pandas query function
                    Else, a dict of lists where the key is column name in the dataframe
                    and the corresponding value is a list of values to keep in the dataframe
      :type query: Union[str or dict]

      :returns: Filtered Pandas DataFrame
      :rtype: pd.DataFrame


   .. py:method:: extract_treatments(df, overwrite_mapping, extract_col, verbose=True)


   .. py:method:: _extract(df, extract_col)

      Simple function to extract treatments from column


   .. py:method:: load_if_json(extracted, extract_col='pyname')
      :staticmethod:

      Either extracts the whole column, or just the json strings


   .. py:method:: discover_modelTypes(df)
      :staticmethod:


   .. py:method:: get_AGB_models(last: bool = False, by: str = 'Configuration', n_threads: int = 6, query: Union[str, dict] = None, verbose: bool = True, **kwargs) -> Dict

      Method to automatically extract AGB models.

      Recommended to subset using the querying functionality
      to cut down on execution time, because it checks for each
      model ID. If you only have AGB models remaining after the query,
      it will only return proper AGB models.

      :param last: Whether to only look at the last snapshot for each model
      :type last: bool, default = False
      :param by: Which column to determine unique models with
      :type by: str, default = 'Configuration'
      :param n_threads: The number of threads to use for extracting the models.
                        Since we use multithreading, setting this to a reasonable value
                        helps speed up the import.
      :type n_threads: int, default = 6
      :param query: The pandas query to apply to the modelData frame
      :type query: Union[str, dict], default = None
      :param verbose: Whether to print out information while importing
      :type verbose: bool, default = False


   .. py:method:: _create_sign_df(df: pandas.DataFrame) -> pandas.DataFrame
      :staticmethod:

      Generates dataframe to show whether responses decreased/increased from day to day
      For a given dataframe where columns are dates and rows are model names,
      subtracts each day's value from the previous day's value per model. Then masks the data.
      If decreased (desired situtation), it will put 1 in the cell, if no change, it will
      put 0, and if decreased it will put -1. This dataframe then could be used in the heatmap

      :param df: This typically is pivoted ModelData
      :type df: pd.DataFrame

      :returns: The dataframe with signs for increase or decrease in day to day
      :rtype: pd.DataFrame


   .. py:method:: _create_heatmap_df(df: pandas.DataFrame, lookback: int = 5, query: Union[str, dict] = None, fill_null_days: bool = False) -> Tuple[pandas.DataFrame, pandas.DataFrame]

      Generates dataframes needed to plot calendar heatmap
      The method generates two dataframes where one is used to annotate the heatmap
      and the other is used to apply colors based on the sign dataframe.
      If there are multiple snapshots per day, the latest one will be selected

      :param lookback: Defines how many days to look back at data from the last snapshot
      :type lookback: int
      :param query: The query to supply to _apply_query
                    If a string, uses the default Pandas query function
                    Else, a dict of lists where the key is column name in the dataframe
                    and the corresponding value is a list of values to keep in the dataframe
      :type query: Union[str, dict]
      :param fill_null_days: If True, null values will be generated in the dataframe for
                             days where there is no model snapshot
      :type fill_null_days: bool

      :returns: Tuple of annotate and sign dataframes
      :rtype: Tuple[pd.DataFrame, pd.DataFrame]


   .. py:method:: _calculate_impact_influence(df: pandas.DataFrame, context_keys: list = None, ModelID: str = None) -> pandas.DataFrame
      :staticmethod:


   .. py:method:: model_summary(by: str = 'ModelID', query: Union[str, dict] = None, **kwargs)

      Convenience method to automatically generate a summary over models
      By default, it summarizes ResponseCount, Performance, SuccessRate & Positives by model ID.
      It also adds weighted means for Performance and SuccessRate,
      And adds the count of models without responses and the percentage.

      :param by: By what column to summarize the models
      :type by: str, default = ModelID
      :param query: The query to supply to _apply_query
                    If a string, uses the default Pandas query function
                    Else, a dict of lists where the key is column name in the dataframe
                    and the corresponding value is a list of values to keep in the dataframe
      :type query: Union[str, dict]

      :returns: Groupby dataframe over all models
      :rtype: pd.DataFrame


   .. py:method:: pivot_df(df: pandas.DataFrame) -> pandas.DataFrame
      :staticmethod:

      Simple function to extract pivoted information


   .. py:method:: response_gain_df(df: pandas.DataFrame, by: str = 'Channel') -> pandas.DataFrame
      :staticmethod:

      Simple function to extract the response gain per model


   .. py:method:: models_by_positives_df(df: pandas.DataFrame, by: str = 'Channel') -> pandas.DataFrame
      :staticmethod:


   .. py:method:: get_model_stats(last: bool = True) -> dict


   .. py:method:: describe_models(**kwargs) -> NoReturn

      Convenience method to quickly summarize the models



.. py:function:: readDSExport(filename: Union[pandas.DataFrame, str], path: str = '.', verbose: bool = True, **kwargs) -> pandas.DataFrame

   Read a Pega dataset export file.
   Can accept either a Pandas DataFrame or one of the following formats:
   - .csv
   - .json
   - .zip (zipped json or CSV)

   It automatically infers the default file names for both model data as well as predictor data.
   If you supply either 'modelData' or 'predictorData' as the 'file' argument, it will search for them.
   If you supply the full name of the file in the 'path' directory, it will import that instead.

   :param filename: Either a Pandas DataFrame with the source data (for compatibility),
                    or a string, in which case it can either be:
                    - The name of the file (if a custom name) or
                    - Whether we want to look for 'modelData' or 'predictorData' in the path folder.
   :type filename: [pd.DataFrame, str]
   :param path: The location of the file
   :type path: str, default = '.'
   :param verbose: Whether to print out which file will be imported
   :type verbose: bool, default = True
   :param Keyword arguments: Any arguments to plug into the read csv or json function, from either PyArrow or Pandas.

   :returns: * *pd.DataFrame* -- The read data from the given file
             * *Examples* -- >>> df = readDSExport(filename = 'modelData', path = './datamart')
               >>> df = readDSExport(filename = 'ModelSnapshot.json', path = 'data/ADMData')

               >>> df = pd.read_csv('file.csv')
               >>> df = readDSExport(filename = df)


.. py:function:: CDHSample(plotting_engine='plotly', query=None)


.. py:class:: ADMTrees

   .. py:method:: getMultiTrees(file: pandas.DataFrame, n_threads=6, verbose=True, **kwargs)
      :staticmethod:



.. py:class:: MultiTrees

   .. py:property:: first


   .. py:property:: last


   .. py:attribute:: trees
      :annotation: :dict

      

   .. py:attribute:: model_name
      :annotation: :str

      

   .. py:attribute:: context_keys
      :annotation: :list

      

   .. py:method:: __repr__()

      Return repr(self).


   .. py:method:: __getitem__(index)


   .. py:method:: __len__()


   .. py:method:: __add__(other)


   .. py:method:: computeOverTime(predictorCategorization=None)


   .. py:method:: plotSplitsPerVariableType(predictorCategorization=None, **kwargs)



.. py:class:: ValueFinder(path: Optional[str] = None, df: Optional[Union[pandas.DataFrame, polars.DataFrame, pyarrow.Table]] = None, verbose: bool = True, **kwargs)

   Class to analyze Value Finder datasets.

   Relies heavily on polars for faster reading and transformations.
   See https://pola-rs.github.io/polars/py-polars/html/index.html

   Requires either df or a path to be supplied,
   If a path is supplied, the 'filename' argument is optional.
   If path is given and no filename is, it will look for the most recent.

   :param path: Path to the ValueFinder data files
   :type path: Optional[str]
   :param df: Override to supply a dataframe instead of a file.
              Supports pandas or polars dataframes or pyarrow tables
   :type df: Optional[DataFrame]
   :param verbose: Whether to print out information during importing
   :type verbose: bool

   :keyword filename: The name, or extended filepath, towards the file
   :kwtype filename: Optional[str]
   :keyword subset: Whether to select only a subset of columns.
                    Will speed up analysis and reduce unused information
   :kwtype subset: bool

   .. py:method:: getCustomerSummary(th: Optional[float] = None, verbose: bool = True) -> polars.DataFrame

      Computes the summary of propensities for all customers

      :param th: The threshold to consider an action 'good'.
                 If a customer has actions with propensity above this,
                 the customer has at least one relevant action.
                 If not given, will default to 5th quantile.
      :type th: Optional[float]
      :param verbose: Whether to print out the execution times
      :type verbose: bool, default = True


   .. py:method:: getCountsPerStage(customersummary: Optional[polars.DataFrame] = None, verbose: bool = True) -> polars.DataFrame

      Generates an aggregated view per stage.

      :param customersummary: Optional override of the customer summary,
                              which can be generated by getCustomerSummary().
      :type customersummary: Optional[pl.DataFrame]
      :param verbose: Whether to print execution times.
      :type verbose: bool, default = True


   .. py:method:: addCountsPerThresholdRange(start: float, stop: float, step: float, verbose: bool = True)

      Adds the counts per stage for a range of quantiles.

      In the background, uses numpy's arange function
      to generate the range of quantiles.
      Then, for each quantile the counts per stage are computed,
      and added to the 'countsPerThreshold' dictionary.
      As optimization, if a quantile produces a threshold that is
      already previously computed, it simply adds that computed dataframe.

      :param start: The starting quantile
      :type start: float
      :param stop: The ending quantile
      :type stop: float
      :param step: The steps to compute between start and stop
      :type step: float
      :param verbose: Whether to print out the progress of computation
      :type verbose: bool, default = True


   .. py:method:: plotPropensityDistribution(sampledN: int = 10000) -> plotly.graph_objects.Figure

      Plots the distribution of the different propensities.

      For optimization reasons (storage for all points in a boxplot and
      time complexity for computing the distribution plot),
      we have to sample to a reasonable amount of data points.

      :param sampledN: The number of datapoints to sample
      :type sampledN: int, default = 10_000


   .. py:method:: plotPropensityThreshold(sampledN=10000, stage='Eligibility') -> plotly.graph_objects.Figure

      Plots the propensity threshold vs the different propensities.

      :param sampledN: The number of datapoints to sample
      :type sampledN: int, default = 10_000


   .. py:method:: plotPieCharts(start: Optional[float] = None, stop: Optional[float] = None, step: Optional[float] = None, verbose: bool = True)

      Plots the pie chart, split per stage.

      If start, stop and step are supplied,
      it will generate a slider to set the threshold
      dynamically throughout the given range of quantiles.
      If any of the is None, it will simply use the default
      threshold based on the 5th quantile.

      :param start: The starting quantile
      :type start: float
      :param stop: The ending quantile
      :type stop: float
      :param step: The steps to compute between start and stop
      :type step: float
      :param verbose: Whether to print out the progress of computation
      :type verbose: bool, default = True


   .. py:method:: plotDistributionPerThreshold(target: str = 'Quantile', **kwargs)

      Plots the distribution of customers per threshold, per stage.

      Based on the precomputed data in self.countsPerThreshold,
      this function will plot the distribution per stage.

      To add more data points between a given range,
      simply pass all three arguments to this function:
      start, stop and step. Alternatively, you may
      call the self.addCountsPerThresholdRange() function,
      with the start, stop and step arguments outside of this call.

      :param target: Determines which threshold to plot:
                     based on the quantiles or the raw propensities.
                     One of: {'Quantile', 'Propensity'}
      :type target: str, default = Quantile

      :keyword start: The starting quantile
      :kwtype start: float
      :keyword stop: The ending quantile
      :kwtype stop: float
      :keyword step: The steps to compute between start and stop
      :kwtype step: float
      :keyword verbose: Whether to print out the progress of computation
      :kwtype verbose: bool, default = True


   .. py:method:: plotFunnelChart(level: str = 'Action', query=None)

      Plots the funnel of actions or issues per stage.

      :param level: Which element to plot:
                    - If 'Actions', plots the distribution of actions.
                    - If 'Issues', plots the distribution of issues
      :type level: str, default = 'Actions'



