:py:mod:`cdhtools`
==================

.. py:module:: cdhtools


Submodules
----------
.. toctree::
   :titlesonly:
   :maxdepth: 1

   ADMDatamart/index.rst
   ADMTrees/index.rst
   IHanalysis/index.rst
   ValueFinder/index.rst
   cdh_utils/index.rst
   datasets/index.rst
   plot_base/index.rst
   plots_mpl/index.rst
   plots_plotly/index.rst


Package Contents
----------------

Classes
~~~~~~~

.. autoapisummary::

   cdhtools.ADMDatamart
   cdhtools.ADMTrees
   cdhtools.ValueFinder



Functions
~~~~~~~~~

.. autoapisummary::

   cdhtools.readDSExport
   cdhtools.CDHSample



.. py:class:: ADMDatamart(path: str = '.', overwrite_mapping: Optional[dict] = None, query: Union[str, Dict[str, list]] = None, plotting_engine='plotly', **kwargs)

   Bases: :py:obj:`cdhtools.plot_base.Plots`

   Main class for importing, preprocessing and structuring Pega ADM Datamart snapshot data.
   Gets all available data, properly names and merges into one main dataframe

   :param path: The path of the data files
   :type path: str, default = "."
   :param overwrite_mapping: A dictionary to overwrite default feature names in the input data
   :type overwrite_mapping: dict, default = None
   :param query: The query to supply to _apply_query
                 If a string, uses the default Pandas query function
                 Else, a dict of lists where the key is the column name of the dataframe
                 and the corresponding value is a list of values to keep in the dataframe
                 Can be applied to an individual function, or used here to apply to the whole dataset
   :type query: Union[str, dict], default = None
   :param plotting_engine: Either 'mpl' for matplotlib, or 'plotly' for plotly.
                           Determines what package to use for plotting.
                           Can also be supplied to most plotting functions directly.
   :type plotting_engine: str, default = "plotly"

   :keyword verbose: Whether to print out information during importing
   :kwtype verbose: bool
   :keyword model_filename: The name, or extended filepath, towards the model file
   :kwtype model_filename: str
   :keyword predictor_filename: The name, or extended filepath, towards the predictors file
   :kwtype predictor_filename: str
   :keyword model_df: Optional override to supply a dataframe instead of a file
   :kwtype model_df: pd.DataFrame
   :keyword predictor_df: Optional override to supply a dataframe instead of a file
   :kwtype predictor_df: pd.DataFrame
   :keyword subset: Whether to only select the renamed columns,
                    set to False to keep all columns
   :kwtype subset: bool, default = True
   :keyword drop_cols = list: Supply columns to drop from the dataframe
   :keyword prequery: A way to apply a query to the data before any preprocessing
                      Uses the Pandas querying function, and beware that the columns
                      have not been renamed, so use the original naming
   :kwtype prequery: str
   :keyword context_keys: Which columns to use as context keys
   :kwtype context_keys: list
   :keyword extract_treatment: Treatments are typically hidden within the pyName column,
                               extract_treatment can expand that cell to also show treatments.
                               To extract, give the column name as the 'extract_treatment' argument
   :kwtype extract_treatment: str
   :keyword force_pandas: If pyarrow is installed, you can force the import through Pandas
   :kwtype force_pandas: bool

   .. rubric:: Notes

   Depending on the importing function, typically it is possible to
   supply more arguments. For instance, if the importing is done through
   Pandas (because pyarrow is not installed or through force_pandas),
   you can supply the column separator from the pandas function as a keyword argument

   .. attribute:: modelData

      If available, holds the preprocessed data about the models

      :type: pd.DataFrame

   .. attribute:: predictorData

      If available, holds the preprocessed data about the predictor binning

      :type: pd.DataFrame

   .. attribute:: combinedData

      If both modelData and predictorData are available,
      holds the merged data about the models and predictors

      :type: pd.DataFrame

   .. rubric:: Examples

   >>> Data =  ADMDatamart(f"/CDHSample")
   >>> Data =  ADMDatamart(f"Data/Adaptive Models & Predictors Export",
               model_filename = "Data-Decision-ADM-ModelSnapshot_AdaptiveModelSnapshotRepo20201110T085543_GMT/data.json",
               predictor_filename = "Data-Decision-ADM-PredictorBinningSnapshot_PredictorBinningSnapshotRepo20201110T084825_GMT/data.json")
   >>> Data =  ADMDatamart(f"Data/files",
               model_filename = "ModelData.csv",
               predictor_filename = "PredictorData.csv")

   .. py:method:: get_engine(plotting_engine)
      :staticmethod:


   .. py:method:: import_data(self, path: Optional[str] = '.', overwrite_mapping: Optional[dict] = None, subset: bool = True, model_df: Optional[pandas.DataFrame] = None, predictor_df: Optional[pandas.DataFrame] = None, query: Union[str, Dict[str, list]] = None, **kwargs) -> Union[pandas.DataFrame, pandas.DataFrame]

      Method to automatically import & format the relevant data.

      The method first imports the model data, and then the predictor data.
      If model_df or predictor_df is supplied, it will use those instead
      After reading, some additional values (such as success rate) are
      automatically computed.
      Lastly, if there are missing columns from both datasets,
      this will be printed to the user if verbose is True.

      :param path: The path of the data files
                   Default = current path (',')
      :type path: str
      :param overwrite_mapping: A dictionary to overwrite default feature names in the input data
                                Default = None
      :type overwrite_mapping: dict
      :param subset: Whether to only select the renamed columns,
                     set to False to keep all columns
      :type subset: bool, default = True
      :param model_df: Optional override to supply a dataframe instead of a file
      :type model_df: pd.DataFrame
      :param predictor_df: Optional override to supply a dataframe instead of a file
      :type predictor_df: pd.DataFrame
      :param query: The query to supply to _apply_query
                    If a string, uses the default Pandas query function
                    Else, a dict of lists where the key is the column name of the dataframe
                    and the corresponding value is a list of values to keep in the dataframe
      :type query: Union[str, dict], default = None

      :returns: The model data and predictor binning data as dataframes
      :rtype: (pd.DataFrame, pd.DataFrame)


   .. py:method:: _import_utils(self, name: Union[str, pandas.DataFrame], path: str = None, overwrite_mapping: dict = None, subset: bool = True, query: Union[str, Dict[str, list]] = None, verbose: bool = True, **kwargs) -> Tuple[pandas.DataFrame, dict, dict]

      Handler function to interface to the cdh_utils methods

      :param name: One of {modelData, predictorData}
                   or a dataframe
      :type name: Union[str, pd.DataFrame]
      :param path: The path of the data file
      :type path: str, default = None
      :param overwrite_mapping: A dictionary to overwrite default feature names in the input data
      :type overwrite_mapping: dict, default = None
      :param subset: Whether to only select the renamed columns,
                     set to False to keep all columns
      :type subset: bool, default = True
      :param query: The query to supply to _apply_query
                    If a string, uses the default Pandas query function
                    Else, a dict of lists where the key is the column name of the dataframe
                    and the corresponding value is a list of values to keep in the dataframe
      :type query: Union[str, dict], default = None
      :param verbose: Whether to print out information during importing
      :type verbose: bool

      :keyword drop_cols = list: Supply columns to drop from the dataframe
      :keyword prequery: A way to apply a query to the data before any preprocessing
                         Uses the Pandas querying function, and beware that the columns
                         have not been renamed, so use the original naming
      :kwtype prequery: str
      :keyword extract_treatment: Treatments are typically hidden within the pyName column,
                                  extract_treatment can expand that cell to also show treatments.
                                  To extract, give the column name as the 'extract_treatment' argument
      :kwtype extract_treatment: str
      :keyword Additional keyword arguments:
      :keyword -----------------:
      :keyword See readDSExport in cdh_utils:

      :returns: The requested dataframe,
                The renamed columns
                The columns missing in both dataframes
      :rtype: (pd.DataFrame, dict, dict)


   .. py:method:: _available_columns(self, df: pandas.DataFrame, overwrite_mapping: Optional[dict] = None) -> Tuple[pandas.DataFrame, dict, list]

      Based on the default names for variables, rename available data to proper formatting

      :param df: Input dataframe
      :type df: pd.DataFrame
      :param overwrite_mapping: If given, adds 'search terms' to the default names to look for
                                If an extra variable is given which is not in default_names, it will also be included
      :type overwrite_mapping: dict

      :returns: The original dataframe, but renamed for the found columns &
                The original and updated names for all renamed columns &
                The variables that were not found in the table
      :rtype: (pd.DataFrame, dict, list)


   .. py:method:: _capitalize(fields: list) -> list
      :staticmethod:

      Applies automatic capitalization, aligned with the R couterpart.

      :param fields: A list of names
      :type fields: list

      :returns: **fields** -- The input list, but each value properly capitalized
      :rtype: list


   .. py:method:: _set_types(df: pandas.DataFrame, verbose=True) -> pandas.DataFrame
      :staticmethod:

      A method to change columns to their proper type

      :param df: The input dataframe
      :type df: pd.DataFrame
      :param verbose: Whether to print out issues with casting variable types
      :type verbose: bool, default = True

      :returns: The input dataframe, but the proper typing applied
      :rtype: pd.DataFrame


   .. py:method:: last(self, table='modelData') -> pandas.DataFrame

      Convenience function to get the last values for a table

      :param table: Which table to get the last values for
                    One of {modelData, predictorData, combinedData}
      :type table: str, default = modelData

      :returns: The last snapshot for each model
      :rtype: pd.DataFrame


   .. py:method:: _last(df: pandas.DataFrame) -> pandas.DataFrame
      :staticmethod:

      Method to retrieve only the last values for a given dataframe.


   .. py:method:: get_combined_data(self, modelData: pandas.DataFrame = None, predictorData: pandas.DataFrame = None, last=True) -> pandas.DataFrame

      Combines the model data and predictor data into one dataframe.

      :param modelData: Optional dataframe to override 'self.modelData' for merging
      :type modelData: pd.DataFrame
      :param predictorData: Optional dataframe to override 'self.predictorData' for merging
      :type predictorData: pd.DataFrame

      :returns: The combined dataframe
      :rtype: pd.DataFrame


   .. py:method:: fix_pdc(df: pandas.DataFrame) -> pandas.DataFrame
      :staticmethod:


   .. py:method:: defaultPredictorCategorization(x: str) -> str
      :staticmethod:


   .. py:method:: _apply_query(df, query: Union[str, dict] = None) -> pandas.DataFrame
      :staticmethod:

      Given an input pandas dataframe, it filters the dataframe based on input query

      :param query: If a string, uses the default Pandas query function
                    Else, a dict of lists where the key is column name in the dataframe
                    and the corresponding value is a list of values to keep in the dataframe
      :type query: Union[str or dict]

      :returns: Filtered Pandas DataFrame
      :rtype: pd.DataFrame


   .. py:method:: extract_treatments(self, df, overwrite_mapping, extract_col, verbose=True)


   .. py:method:: _extract(self, df, extract_col)

      Simple function to extract treatments from column


   .. py:method:: load_if_json(extracted, extract_col='pyname')
      :staticmethod:

      Either extracts the whole column, or just the json strings


   .. py:method:: _create_sign_df(df: pandas.DataFrame) -> pandas.DataFrame
      :staticmethod:

      Generates dataframe to show whether responses decreased/increased from day to day
      For a given dataframe where columns are dates and rows are model names,
      subtracts each day's value from the previous day's value per model. Then masks the data.
      If decreased (desired situtation), it will put 1 in the cell, if no change, it will
      put 0, and if decreased it will put -1. This dataframe then could be used in the heatmap

      :param df: This typically is pivoted ModelData
      :type df: pd.DataFrame

      :returns: The dataframe with signs for increase or decrease in day to day
      :rtype: pd.DataFrame


   .. py:method:: _create_heatmap_df(self, df: pandas.DataFrame, lookback: int = 5, query: Union[str, dict] = None, fill_null_days: bool = False) -> Tuple[pandas.DataFrame, pandas.DataFrame]

      Generates dataframes needed to plot calendar heatmap
      The method generates two dataframes where one is used to annotate the heatmap
      and the other is used to apply colors based on the sign dataframe.
      If there are multiple snapshots per day, the latest one will be selected

      :param lookback: Defines how many days to look back at data from the last snapshot
      :type lookback: int
      :param query: The query to supply to _apply_query
                    If a string, uses the default Pandas query function
                    Else, a dict of lists where the key is column name in the dataframe
                    and the corresponding value is a list of values to keep in the dataframe
      :type query: Union[str, dict]
      :param fill_null_days: If True, null values will be generated in the dataframe for
                             days where there is no model snapshot
      :type fill_null_days: bool

      :returns: Tuple of annotate and sign dataframes
      :rtype: Tuple[pd.DataFrame, pd.DataFrame]


   .. py:method:: _calculate_impact_influence(df: pandas.DataFrame, context_keys: list = None, ModelID: str = None) -> pandas.DataFrame
      :staticmethod:


   .. py:method:: model_summary(self, by: str = 'ModelID', query: Union[str, dict] = None, **kwargs)

      Convenience method to automatically generate a summary over models
      By default, it summarizes ResponseCount, Performance, SuccessRate & Positives by model ID.
      It also adds weighted means for Performance and SuccessRate,
      And adds the count of models without responses and the percentage.

      :param by: By what column to summarize the models
      :type by: str, default = ModelID
      :param query: The query to supply to _apply_query
                    If a string, uses the default Pandas query function
                    Else, a dict of lists where the key is column name in the dataframe
                    and the corresponding value is a list of values to keep in the dataframe
      :type query: Union[str, dict]

      :returns: Groupby dataframe over all models
      :rtype: pd.DataFrame


   .. py:method:: pivot_df(df: pandas.DataFrame) -> pandas.DataFrame
      :staticmethod:

      Simple function to extract pivoted information


   .. py:method:: response_gain_df(df: pandas.DataFrame, by: str = 'Channel') -> pandas.DataFrame
      :staticmethod:

      Simple function to extract the response gain per model


   .. py:method:: models_by_positives_df(df: pandas.DataFrame, by: str = 'Channel') -> pandas.DataFrame
      :staticmethod:


   .. py:method:: get_model_stats(self, last: bool = True) -> dict


   .. py:method:: describe_models(self, **kwargs) -> NoReturn

      Convenience method to quickly summarize the models



.. py:function:: readDSExport(filename: Union[pandas.DataFrame, str], path: str = '.', verbose: bool = True, force_pandas: bool = False, **kwargs) -> pandas.DataFrame

   Read a Pega dataset export file.
   Can accept either a Pandas DataFrame or one of the following formats:
   - .csv
   - .json
   - .zip (zipped json or CSV)

   It automatically infers the default file names for both model data as well as predictor data.
   If you supply either 'modelData' or 'predictorData' as the 'file' argument, it will search for them.
   If you supply the full name of the file in the 'path' directory, it will import that instead.

   :param filename: Either a Pandas DataFrame with the source data (for compatibility),
                    or a string, in which case it can either be:
                    - The name of the file (if a custom name) or
                    - Whether we want to look for 'modelData' or 'predictorData' in the path folder.
   :type filename: [pd.DataFrame, str]
   :param path: The location of the file
   :type path: str, default = '.'
   :param verbose: Whether to print out which file will be imported
   :type verbose: bool, default = True
   :param Keyword arguments: Any arguments to plug into the read csv or json function, from either PyArrow or Pandas.

   :returns: * *pd.DataFrame* -- The read data from the given file
             * *Examples* -- >>> df = readDSExport(file = 'modelData', path = './datamart')
               >>> df = readDSExport(file = 'ModelSnapshot.json', path = 'data/ADMData')

               >>> df = pd.read_csv('file.csv')
               >>> df = readDSExport(file = df)


.. py:function:: CDHSample(plotting_engine='plotly', query=None)


.. py:class:: ADMTrees(file: str)

   Functions for ADM Gradient boosting

   ADM Gradient boosting models consist of multiple trees,
   which build upon each other in a 'boosting' fashion.
   This class provides some functions to extract data from
   these trees, such as the features on which the trees split,
   important values for these features, statistics about the trees,
   or visualising each individual tree.

   :param file: The input file as a json (see notes)
   :type file: str

   .. attribute:: trees

      

      :type: Dict

   .. attribute:: properties

      

      :type: Dict

   .. attribute:: learning_rate

      

      :type: float

   .. attribute:: model

      

      :type: Dict

   .. attribute:: treeStats

      

      :type: Dict

   .. attribute:: splitsPerTree

      

      :type: Dict

   .. attribute:: gainsPerTree

      

      :type: Dict

   .. attribute:: gainsPerSplit

      

      :type: pd.DataFrame

   .. attribute:: groupedGainsPerSplit

      

      :type: Dict

   .. attribute:: predictors

      

      :type: Set

   .. attribute:: allValuesPerSplit

      

      :type: Dict

   .. rubric:: Notes

   The input file is the extracted json file of the 'save model'
   action in Prediction Studio. The Datamart column 'pyModelData'
   also contains this information, but it is compressed and
   the values for each split is encoded. Using the 'save model'
   button, only that data is decompressed and decoded.

   .. py:method:: import_model(self, file: str)

      Imports the 'regular' json file


   .. py:method:: workaround_import(self, file: str)

      Imports the 'old' txt file, with json embedded in it.


   .. py:method:: _depth(self, d: Dict) -> Dict

      Calculates the depth of the tree, used in TreeStats.


   .. py:method:: parseSplitValues(self, value) -> Tuple[str, str, str]

      Parses the raw 'split' string into its three components.

      Once the split is parsed, Python can use it to evaluate.

      :param value: The raw 'split' string
      :type value: str
      :param Returns: Tuple[str, str, str]
                      The variable on which the split is done,
                      The direction of the split (< or 'in')
                      The value on which to split


   .. py:method:: parseSplitValuesWithSpaces(value) -> Tuple[str, str, str]
      :staticmethod:


   .. py:method:: getPredictors(self) -> Dict


   .. py:method:: getGainsPerSplit(self) -> Tuple[Dict, pandas.DataFrame, dict]

      Function to compute the gains of each split in each tree.


   .. py:method:: getGroupedGainsPerSplit(self) -> pandas.DataFrame

      Function to get the gains per split, grouped by split.

      It adds some additional information, such as the possible values,
      the mean gains, and the number of times the split is performed.


   .. py:method:: getSplitsRecursively(self, tree: Dict, splits: List, gains: List) -> Tuple[List, List]

      Recursively finds splits and their gains for each node.

      By Python's mutatable list mechanic, the easiest way to achieve
      this is to explicitly supply the function with empty lists.
      Therefore, the 'splits' and 'gains' parameter expect
      empty lists when initially called.

      :param tree:
      :type tree: Dict
      :param splits:
      :type splits: List
      :param gains:
      :type gains: List

      :returns: * *Tuple[List, List]*
                * *Each split, and its corresponding gain*


   .. py:method:: plotSplitsPerVariable(self, subset: Optional[Set] = None, show=True)

      Plots the splits for each variable in the tree.

      :param subset: Optional parameter to subset the variables to plot
      :type subset: Optional[Set]
      :param show: Whether to display each plot
      :type show: bool

      :rtype: plt.figure


   .. py:method:: getTreeStats(self) -> pandas.DataFrame

      Generate a dataframe with useful stats for each tree


   .. py:method:: getAllValuesPerSplit(self) -> Dict

      Generate a dictionary with the possible values for each split


   .. py:method:: getNodesRecursively(self, tree: Dict, nodelist: Dict, counter: Dict, childs: List) -> Tuple[Dict, List]

      Recursively walks through each node, used for tree representation.

      Again, nodelist, counter and childs expects
      empty dict, dict and list parameters.

      :param tree:
      :type tree: Dict
      :param nodelist:
      :type nodelist: Dict
      :param counter:
      :type counter: Dict
      :param childs:
      :type childs: List

      :returns: * *Tuple[Dict, List]*
                * *The dictionary of nodes and the list of child nodes*


   .. py:method:: _fillChildNodeIDs(nodeinfo: Dict, childs: Dict) -> Dict
      :staticmethod:

      Utility function to add child info to nodes


   .. py:method:: getTreeRepresentation(self, tree_number: int) -> Dict

      Generates a more usable tree representation.

      In this tree representation, each node has an ID,
      and its attributes are the attributes,
      with parent and child nodes added as well.

      :param tree_number: The number of the tree, in order of the original json
      :type tree_number: int
      :param returns:
      :type returns: Dict


   .. py:method:: plotTree(self, tree_number: int, highlighted: Optional[Union[Dict, List]] = None, show=True) -> pydot.Graph

      Plots the chosen decision tree.

      :param tree_number: The number of the tree to visualise
      :type tree_number: int
      :param highlighted: Optional parameter to highlight nodes in green
                          If a dictionary, it expects an 'x': i.e., features
                          with their corresponding values.
                          If a list, expects a list of node IDs for that tree.
      :type highlighted: Optional[Dict, List]

      :rtype: pydot.Graph


   .. py:method:: getVisitedNodes(self, treeID: int, x: Dict, save_all: bool = False) -> Tuple[List, float, List]

      Finds all visited nodes for a given tree, given an x

      :param treeID: The ID of the tree
      :type treeID: int
      :param x: Features to split on, with their values
      :type x: Dict
      :param save_all: Whether to save all gains for each individual split
      :type save_all: bool, default = False

      :returns: The list of visited nodes,
                The score of the final leaf node,
                The gains for each split in the visited nodes
      :rtype: List, float, List


   .. py:method:: getAllVisitedNodes(self, x: Dict) -> pandas.DataFrame

      Loops through each tree, and records the scoring info

      :param x: Features to split on, with their values
      :type x: Dict

      :rtype: pd.DataFrame


   .. py:method:: score(self, x: Dict) -> float

      Computes the score for a given x


   .. py:method:: plotContributionPerTree(self, x: Dict, show=True)

      Plots the contribution of each tree towards the final propensity.



.. py:class:: ValueFinder(path: Optional[str] = None, df: Optional[Union[pandas.DataFrame, polars.DataFrame, pyarrow.Table]] = None, verbose: bool = True, **kwargs)

   Class to analyze Value Finder datasets.

   Relies heavily on polars for faster reading and transformations.
   See https://pola-rs.github.io/polars/py-polars/html/index.html

   Requires either df or a path to be supplied,
   If a path is supplied, the 'filename' argument is optional.
   If path is given and no filename is, it will look for the most recent.

   :param path: Path to the ValueFinder data files
   :type path: Optional[str]
   :param df: Override to supply a dataframe instead of a file.
              Supports pandas or polars dataframes or pyarrow tables
   :type df: Optional[DataFrame]
   :param verbose: Whether to print out information during importing
   :type verbose: bool

   :keyword filename: The name, or extended filepath, towards the file
   :kwtype filename: Optional[str]

   .. py:method:: getCustomerSummary(self, th: Optional[float] = None, verbose: bool = True) -> polars.DataFrame

      Computes the summary of propensities for all customers

      :param th: The threshold to consider an action 'good'.
                 If a customer has actions with propensity above this,
                 the customer is considered 'Well Served'.
                 If not given, will default to 5th quantile.
      :type th: Optional[float]
      :param verbose: Whether to print out the execution times
      :type verbose: bool, default = True


   .. py:method:: getCountsPerStage(self, customersummary: Optional[polars.DataFrame] = None, verbose: bool = True) -> polars.DataFrame

      Generates an aggregated view per stage.

      :param customersummary: Optional override of the customer summary,
                              which can be generated by getCustomerSummary().
      :type customersummary: Optional[pl.DataFrame]
      :param verbose: Whether to print execution times.
      :type verbose: bool, default = True


   .. py:method:: addCountsPerTresholdRange(self, start: float, stop: float, step: float, verbose: bool = True)

      Adds the counts per stage for a range of quantiles.

      In the background, uses numpy's arange function
      to generate the range of quantiles.
      Then, for each quantile the counts per stage are computed,
      and added to the 'countsPerThreshold' dictionary.
      As optimization, if a quantile produces a threshold that is
      already previously computed, it simply adds that computed dataframe.

      :param start: The starting quantile
      :type start: float
      :param stop: The ending quantile
      :type stop: float
      :param step: The steps to compute between start and stop
      :type step: float
      :param verbose: Whether to print out the progress of computation
      :type verbose: bool, default = True


   .. py:method:: plotPropensityDistribution(self, sampledN: int = 10000) -> plotly.graph_objects.Figure

      Plots the distribution of the different propensities.

      For optimization reasons (storage for all points in a boxplot and
      time complexity for computing the distribution plot),
      we have to sample to a reasonable amount of data points.

      :param sampledN: The number of datapoints to sample
      :type sampledN: int, default = 10_000


   .. py:method:: plotPropensityTreshold(self, sampledN=10000) -> plotly.graph_objects.Figure

      Plots the propensity threshold vs the different propensities.

      :param sampledN: The number of datapoints to sample
      :type sampledN: int, default = 10_000


   .. py:method:: plotPieCharts(self, start: Optional[float] = None, stop: Optional[float] = None, step: Optional[float] = None, verbose: bool = True)

      Plots the pie chart, split per stage.

      If start, stop and step are supplied,
      it will generate a slider to set the threshold
      dynamically throughout the given range of quantiles.
      If any of the is None, it will simply use the default
      threshold based on the 5th quantile.

      :param start: The starting quantile
      :type start: float
      :param stop: The ending quantile
      :type stop: float
      :param step: The steps to compute between start and stop
      :type step: float
      :param verbose: Whether to print out the progress of computation
      :type verbose: bool, default = True


   .. py:method:: plotDistributionPerThreshold(self, target: str = 'Quantile', **kwargs)

      Plots the distribution of customers per threshold, per stage.

      Based on the precomputed data in self.countsPerThreshold,
      this function will plot the distribution per stage.

      To add more data points between a given range,
      simply pass all three arguments to this function:
      start, stop and step. Alternatively, you may
      call the self.addCountsPerThresholdRange() function,
      with the start, stop and step arguments outside of this call.

      :param target: Determines which threshold to plot:
                     based on the quantiles or the raw propensities.
                     One of: {'Quantile', 'Propensity'}
      :type target: str, default = Quantile

      :keyword start: The starting quantile
      :kwtype start: float
      :keyword stop: The ending quantile
      :kwtype stop: float
      :keyword step: The steps to compute between start and stop
      :kwtype step: float
      :keyword verbose: Whether to print out the progress of computation
      :kwtype verbose: bool, default = True


   .. py:method:: plotFunnelChart(self, level: str = 'Actions')

      Plots the funnel of actions or issues per stage.

      :param level: Which element to plot:
                    - If 'Actions', plots the distribution of actions.
                    - If 'Issues', plots the distribution of issues
      :type level: str, default = 'Actions'



